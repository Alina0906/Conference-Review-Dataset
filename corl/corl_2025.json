[
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "4XKKUifQ9c",
        "title": "ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes",
        "abstract": "Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry- and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts.",
        "keywords": [
            "Cluttered Scene",
            "Dexterous Grasping",
            "Sim-to-Real"
        ],
        "pdf_url": "https://openreview.net/pdf/03293ba8eefdc68dc5f9cc65fb7ddb23b36d8a0a.pdf",
        "reviews": [
            {
                "id": "SoVlVIKCEE",
                "forum": "4XKKUifQ9c",
                "replyto": "4XKKUifQ9c",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068326382,
                "mdate": 1754869459635,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "4XKKUifQ9c",
                "forum": "4XKKUifQ9c",
                "content": {
                    "title": {
                        "value": "ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes"
                    },
                    "authors": {
                        "value": [
                            "Zeyuan Chen",
                            "Qiyang Yan",
                            "Yuanpei Chen",
                            "Tianhao Wu",
                            "Jiyao Zhang",
                            "Zihan Ding",
                            "Jinzhou Li",
                            "Yaodong Yang",
                            "Hao Dong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zeyuan_Chen10",
                            "~Qiyang_Yan1",
                            "~Yuanpei_Chen2",
                            "~Tianhao_Wu2",
                            "~Jiyao_Zhang1",
                            "~Zihan_Ding1",
                            "~Jinzhou_Li2",
                            "~Yaodong_Yang1",
                            "~Hao_Dong3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Cluttered Scene",
                            "Dexterous Grasping",
                            "Sim-to-Real"
                        ]
                    },
                    "abstract": {
                        "value": "Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry- and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a2a8303da3ce60838859bf4d3d853a66c93fb6b6.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/03293ba8eefdc68dc5f9cc65fb7ddb23b36d8a0a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchen2025clutterdexgrasp,\ntitle={ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes},\nauthor={Zeyuan Chen and Qiyang Yan and Yuanpei Chen and Tianhao Wu and Jiyao Zhang and Zihan Ding and Jinzhou Li and Yaodong Yang and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4XKKUifQ9c}\n}"
                    },
                    "paperhash": {
                        "value": "chen|clutterdexgrasp_a_simtoreal_system_for_general_dexterous_grasping_in_cluttered_scenes"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745494137966,
                "pdate": 1754680646371,
                "odate": 1758062792833,
                "mdate": 1758062829793,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "gD6YV5OuW3",
        "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
        "abstract": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce **ScrewSplat**, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object’s underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model.",
        "keywords": [
            "Articulated objects",
            "Gaussian splatting",
            "Screw theory"
        ],
        "pdf_url": "https://openreview.net/pdf/9243e872c9c3f6dc91ff237a97963b8cd4073173.pdf",
        "reviews": [
            {
                "id": "O9bUsZFXrc",
                "forum": "gD6YV5OuW3",
                "replyto": "gD6YV5OuW3",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068326215,
                "mdate": 1754869459558,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "gD6YV5OuW3",
                "forum": "gD6YV5OuW3",
                "content": {
                    "title": {
                        "value": "ScrewSplat: An End-to-End Method for Articulated Object Recognition"
                    },
                    "authors": {
                        "value": [
                            "Seungyeon Kim",
                            "Junsu HA",
                            "Young Hun Kim",
                            "Yonghyeon Lee",
                            "Frank C. Park"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Seungyeon_Kim2",
                            "~Junsu_HA1",
                            "~Young_Hun_Kim1",
                            "~Yonghyeon_Lee2",
                            "~Frank_C._Park1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Articulated objects",
                            "Gaussian splatting",
                            "Screw theory"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper proposes a novel framework for recognizing and manipulating articulated objects."
                    },
                    "abstract": {
                        "value": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce **ScrewSplat**, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object’s underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7bc4c2fe9782e886a3af5c9455d80588a4be04d9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9243e872c9c3f6dc91ff237a97963b8cd4073173.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkim2025screwsplat,\ntitle={ScrewSplat: An End-to-End Method for Articulated Object Recognition},\nauthor={Seungyeon Kim and Junsu HA and Young Hun Kim and Yonghyeon Lee and Frank C. Park},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=gD6YV5OuW3}\n}"
                    },
                    "paperhash": {
                        "value": "kim|screwsplat_an_endtoend_method_for_articulated_object_recognition"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745493990172,
                "pdate": 1754680646311,
                "odate": 1758062792752,
                "mdate": 1758062829742,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Bw9NHYjDqR",
        "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks",
        "abstract": "Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as “picking up an object from the ground” and “placing it in a distant bin”; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.",
        "keywords": [
            "Humanoid",
            "Whole-body Control",
            "Humanoid-Environment Interaction"
        ],
        "pdf_url": "https://openreview.net/pdf/fc21fb0b014b7944d7880b265b48d58093611d28.pdf",
        "reviews": [
            {
                "id": "gCYAhJoaAb",
                "forum": "Bw9NHYjDqR",
                "replyto": "Bw9NHYjDqR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068326023,
                "mdate": 1754869479241,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Bw9NHYjDqR",
                "forum": "Bw9NHYjDqR",
                "content": {
                    "title": {
                        "value": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks"
                    },
                    "authors": {
                        "value": [
                            "Yixuan Li",
                            "Yutang Lin",
                            "Jieming Cui",
                            "Tengyu Liu",
                            "Wei Liang",
                            "Yixin Zhu",
                            "Siyuan Huang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yixuan_Li7",
                            "~Yutang_Lin2",
                            "~Jieming_Cui1",
                            "~Tengyu_Liu1",
                            "~Wei_Liang1",
                            "~Yixin_Zhu1",
                            "~Siyuan_Huang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoid",
                            "Whole-body Control",
                            "Humanoid-Environment Interaction"
                        ]
                    },
                    "TLDR": {
                        "value": "Holistic Closed-Loop Whole-Body Teleoperation"
                    },
                    "abstract": {
                        "value": "Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as “picking up an object from the ground” and “placing it in a distant bin”; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f53530f6cd7e8ba79b96b38bb5083710a52c8210.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/fc21fb0b014b7944d7880b265b48d58093611d28.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025clone,\ntitle={{CLONE}: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks},\nauthor={Yixuan Li and Yutang Lin and Jieming Cui and Tengyu Liu and Wei Liang and Yixin Zhu and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Bw9NHYjDqR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/cabe1d6428752b58e051500b3f055fa81649df11.mp4"
                    },
                    "paperhash": {
                        "value": "li|clone_closedloop_wholebody_humanoid_teleoperation_for_longhorizon_tasks"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745493362451,
                "pdate": 1754680646090,
                "odate": 1758062792524,
                "mdate": 1758062829616,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "0ViTEgiFiQ",
        "title": "Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning",
        "abstract": "In meta-learning and its downstream tasks, many methods use implicit adaptation to represent task-specific variations. However, implicit approaches hinder interpretability and make it difficult to understand which task factors drive performance. In this work, we introduce a disentangled multi-context meta-learning framework that explicitly learns separate context vectors for different aspects that define a task. By decoupling these factors, our approach improves both robustness, through deeper task understanding, and generalization, by enabling context vector sharing across tasks with the same context. We evaluate our approach in two domains. First, on a sinusoidal regression benchmark, our model outperforms baselines on out-of-distribution tasks and generalizes to unseen sine functions by sharing context vectors associated with shared amplitudes or phase shifts. Second, in a quadruped locomotion task, we disentangle the robot-specific properties and the characteristics of the terrain in the robot dynamics model. Using these context vectors in reinforcement learning, the learned policy demonstrates improved robustness under out-of-distribution conditions, compared to a model using a single unified context. Furthermore, by effectively sharing context, our model enables successful sim-to-real policy transfer to challenging terrains with out-of-distribution robot-specific properties using only real data from flat terrain, which is not achievable with single-task adaptation.",
        "keywords": [
            "Meta-Learning",
            "Multi Task Learning",
            "Quadruped Robot Locomotion"
        ],
        "pdf_url": "https://openreview.net/pdf/21a4cdd91c37f123616e5eb48a831f932bbc3aff.pdf",
        "reviews": [
            {
                "id": "GqCnl5layj",
                "forum": "0ViTEgiFiQ",
                "replyto": "0ViTEgiFiQ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325868,
                "mdate": 1754869479263,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "0ViTEgiFiQ",
                "forum": "0ViTEgiFiQ",
                "content": {
                    "title": {
                        "value": "Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning"
                    },
                    "authors": {
                        "value": [
                            "Seonsoo Kim",
                            "Jun-Gill Kang",
                            "Taehong Kim",
                            "Seongil Hong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Seonsoo_Kim1",
                            "~Jun-Gill_Kang1",
                            "~Taehong_Kim3",
                            "~Seongil_Hong1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Meta-Learning",
                            "Multi Task Learning",
                            "Quadruped Robot Locomotion"
                        ]
                    },
                    "TLDR": {
                        "value": "Explicit and selective context modeling enables robust and generalizable robot learning"
                    },
                    "abstract": {
                        "value": "In meta-learning and its downstream tasks, many methods use implicit adaptation to represent task-specific variations. However, implicit approaches hinder interpretability and make it difficult to understand which task factors drive performance. In this work, we introduce a disentangled multi-context meta-learning framework that explicitly learns separate context vectors for different aspects that define a task. By decoupling these factors, our approach improves both robustness, through deeper task understanding, and generalization, by enabling context vector sharing across tasks with the same context. We evaluate our approach in two domains. First, on a sinusoidal regression benchmark, our model outperforms baselines on out-of-distribution tasks and generalizes to unseen sine functions by sharing context vectors associated with shared amplitudes or phase shifts. Second, in a quadruped locomotion task, we disentangle the robot-specific properties and the characteristics of the terrain in the robot dynamics model. Using these context vectors in reinforcement learning, the learned policy demonstrates improved robustness under out-of-distribution conditions, compared to a model using a single unified context. Furthermore, by effectively sharing context, our model enables successful sim-to-real policy transfer to challenging terrains with out-of-distribution robot-specific properties using only real data from flat terrain, which is not achievable with single-task adaptation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b8e495e152630f720a5029e348952e578f251e8f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/21a4cdd91c37f123616e5eb48a831f932bbc3aff.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkim2025disentangled,\ntitle={Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning},\nauthor={Seonsoo Kim and Jun-Gill Kang and Taehong Kim and Seongil Hong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=0ViTEgiFiQ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/301b63c48d5afddca8074d6ae5bf9d4ef0e4ca98.mp4"
                    },
                    "paperhash": {
                        "value": "kim|disentangled_multicontext_metalearning_unlocking_robust_and_generalized_task_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745492181311,
                "pdate": 1754680645942,
                "odate": 1758062792380,
                "mdate": 1758062829548,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1n1Liq6So4",
        "title": "Meta-Optimization and Program Search using Language Models for Task and Motion Planning",
        "abstract": "Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. This requirement is formalized in the task and motion planning (TAMP) problem, in which symbolic planning and continuous trajectory generation must be solved in a coordinated manner. Recently, foundation model-based approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level plan and low-level motion generation remains to be found: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these shortcomings by: (i) using program search over trajectory optimization problems as an interface between foundation model and robot controllers, and (ii) leveraging a zero-order method to optimize numerical values in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches.",
        "keywords": [
            "Task and Motion Planning",
            "LLMs as Optimizers",
            "Trajectory Optimization"
        ],
        "pdf_url": "https://openreview.net/pdf/6fed0a32ebdb2a2c20c766d5ef7ea57ccdcb2935.pdf",
        "reviews": [
            {
                "id": "V03dQSsdLi",
                "forum": "1n1Liq6So4",
                "replyto": "1n1Liq6So4",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325857,
                "mdate": 1754869478672,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1n1Liq6So4",
                "forum": "1n1Liq6So4",
                "content": {
                    "title": {
                        "value": "Meta-Optimization and Program Search using Language Models for Task and Motion Planning"
                    },
                    "authors": {
                        "value": [
                            "Denis Shcherba",
                            "Eckart Cobo-Briesewitz",
                            "Cornelius V. Braun",
                            "Marc Toussaint"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Denis_Shcherba1",
                            "~Eckart_Cobo-Briesewitz1",
                            "~Cornelius_V._Braun1",
                            "~Marc_Toussaint3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Task and Motion Planning",
                            "LLMs as Optimizers",
                            "Trajectory Optimization"
                        ]
                    },
                    "abstract": {
                        "value": "Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. This requirement is formalized in the task and motion planning (TAMP) problem, in which symbolic planning and continuous trajectory generation must be solved in a coordinated manner. Recently, foundation model-based approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level plan and low-level motion generation remains to be found: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these shortcomings by: (i) using program search over trajectory optimization problems as an interface between foundation model and robot controllers, and (ii) leveraging a zero-order method to optimize numerical values in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches."
                    },
                    "supplementary_material": {
                        "value": "/attachment/042046c99c92632a036e6e40fd0e122011998816.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6fed0a32ebdb2a2c20c766d5ef7ea57ccdcb2935.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nshcherba2025metaoptimization,\ntitle={Meta-Optimization and Program Search using Language Models for Task and Motion Planning},\nauthor={Denis Shcherba and Eckart Cobo-Briesewitz and Cornelius V. Braun and Marc Toussaint},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1n1Liq6So4}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ab68d4b1f7a0739b9c412a8971031fd45b49ed8f.zip"
                    },
                    "paperhash": {
                        "value": "shcherba|metaoptimization_and_program_search_using_language_models_for_task_and_motion_planning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745491881087,
                "pdate": 1754680645885,
                "odate": 1758062792357,
                "mdate": 1758062829516,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1137/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "U9zcbQVDGa",
        "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions",
        "abstract": "Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning.",
        "keywords": [
            "Tactile Sensing",
            "Reinforcement Learning",
            "Large Language Models"
        ],
        "pdf_url": "https://openreview.net/pdf/d2618ad9a45d0f279a1f2b5606bb76dbb6d3030a.pdf",
        "reviews": [
            {
                "id": "LFfCNgsZEv",
                "forum": "U9zcbQVDGa",
                "replyto": "U9zcbQVDGa",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325678,
                "mdate": 1754869478483,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "U9zcbQVDGa",
                "forum": "U9zcbQVDGa",
                "content": {
                    "title": {
                        "value": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions"
                    },
                    "authors": {
                        "value": [
                            "Harrison Field",
                            "Max Yang",
                            "Yijiong Lin",
                            "Efi Psomopoulou",
                            "David A.W. Barton",
                            "Nathan F. Lepora"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Harrison_Field1",
                            "~Max_Yang1",
                            "~Yijiong_Lin1",
                            "~Efi_Psomopoulou1",
                            "~David_A.W._Barton1",
                            "~Nathan_F._Lepora1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Sensing",
                            "Reinforcement Learning",
                            "Large Language Models"
                        ]
                    },
                    "TLDR": {
                        "value": "Text2Touch employs targeted prompt engineering to steer an LLM into automatically generating reward functions that solve a challenging tactile in-hand manipulation task on real hardware, outperforming a human-engineered baseline."
                    },
                    "abstract": {
                        "value": "Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/6cc0ee830e7ce48b2ffde13022477e3c9ee66dbf.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d2618ad9a45d0f279a1f2b5606bb76dbb6d3030a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfield2025texttouch,\ntitle={Text2Touch: Tactile In-Hand Manipulation with {LLM}-Designed Reward Functions},\nauthor={Harrison Field and Max Yang and Yijiong Lin and Efi Psomopoulou and David A.W. Barton and Nathan F. Lepora},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=U9zcbQVDGa}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/fa3461a6ac2d956274b5ed3e42dc947fa06c5eeb.mp4"
                    },
                    "paperhash": {
                        "value": "field|text2touch_tactile_inhand_manipulation_with_llmdesigned_reward_functions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745491543436,
                "pdate": 1754680645748,
                "odate": 1758062792158,
                "mdate": 1758062829377,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1131/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ypDETG94BS",
        "title": "Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion",
        "abstract": "Generalizing locomotion policies across diverse legged robots with varying morphologies is a key challenge due to differences in observation/action dimensions and system dynamics. In this work, we propose \\textit{Multi-Loco}, a novel unified framework combining a morphology-agnostic generative diffusion model with a lightweight residual policy optimized via reinforcement learning (RL). The diffusion model captures morphology-invariant locomotion patterns from diverse cross-embodiment datasets, improving generalization and robustness. The residual policy is shared across all embodiments and refines the actions generated by the diffusion model, enhancing task-aware performance and robustness for real-world deployment. We evaluated our method with a rich library of four legged robots in both simulation and real-world experiments. Compared to a standard RL framework with PPO, our approach - replacing the Gaussian policy with a diffusion model and residual term - achieves a 10.35\\% average return improvement, with gains up to 13.57\\% in wheeled-biped locomotion tasks. These results highlight the benefits of cross-embodiment data and composite generative architectures in learning robust, generalized locomotion skills.",
        "keywords": [
            "Locomotion",
            "Legged Robots",
            "Multi-Embodiment",
            "Diffusion Model",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/52497c4aabeb3414a6d6da8bdf84a792ba2d2114.pdf",
        "reviews": [
            {
                "id": "UneHYjQnks",
                "forum": "ypDETG94BS",
                "replyto": "ypDETG94BS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325531,
                "mdate": 1754869478463,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ypDETG94BS",
                "forum": "ypDETG94BS",
                "content": {
                    "title": {
                        "value": "Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion"
                    },
                    "authors": {
                        "value": [
                            "Shunpeng Yang",
                            "Zhen Fu",
                            "Zhefeng Cao",
                            "Guo Junde",
                            "Patrick Wensing",
                            "Wei Zhang",
                            "Hua Chen"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shunpeng_Yang1",
                            "~Zhen_Fu1",
                            "~Zhefeng_Cao1",
                            "~Guo_Junde1",
                            "~Patrick_Wensing1",
                            "~Wei_Zhang40",
                            "~Hua_Chen2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Locomotion",
                            "Legged Robots",
                            "Multi-Embodiment",
                            "Diffusion Model",
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Generalizing locomotion policies across diverse legged robots with varying morphologies is a key challenge due to differences in observation/action dimensions and system dynamics. In this work, we propose \\textit{Multi-Loco}, a novel unified framework combining a morphology-agnostic generative diffusion model with a lightweight residual policy optimized via reinforcement learning (RL). The diffusion model captures morphology-invariant locomotion patterns from diverse cross-embodiment datasets, improving generalization and robustness. The residual policy is shared across all embodiments and refines the actions generated by the diffusion model, enhancing task-aware performance and robustness for real-world deployment. We evaluated our method with a rich library of four legged robots in both simulation and real-world experiments. Compared to a standard RL framework with PPO, our approach - replacing the Gaussian policy with a diffusion model and residual term - achieves a 10.35\\% average return improvement, with gains up to 13.57\\% in wheeled-biped locomotion tasks. These results highlight the benefits of cross-embodiment data and composite generative architectures in learning robust, generalized locomotion skills."
                    },
                    "supplementary_material": {
                        "value": "/attachment/02a6f9328ef6355a9317c3086c96d9ef813e426d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/52497c4aabeb3414a6d6da8bdf84a792ba2d2114.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyang2025multiloco,\ntitle={Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion},\nauthor={Shunpeng Yang and Zhen Fu and Zhefeng Cao and Guo Junde and Patrick Wensing and Wei Zhang and Hua Chen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ypDETG94BS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/63a8b9727268510bf8ca5c37e107b5436ed45a34.mp4"
                    },
                    "paperhash": {
                        "value": "yang|multiloco_unifying_multiembodiment_legged_locomotion_via_reinforcement_learning_augmented_diffusion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745491427884,
                "pdate": 1754680645612,
                "odate": 1758062792004,
                "mdate": 1758062829311,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "MJmzIunyBy",
        "title": "SimShear: Sim-to-Real Shear-based Tactile Servoing",
        "abstract": "We present SimShear: a sim-to-real pipeline for tactile control that allows use of shear information without explicitly modeling shear dynamics in simulation. Shear, which arises from lateral movements across contact surfaces, are critical for tasks involving dynamic object interactions but are challenging to simulate. We introduce shPix2pix: a shear-conditioned U-Net GAN that transforms simulated tactile images absent of shear plus a vector encoding shear information into realistic equivalents that include shear deformations, and show this outperforms baseline pix2pix methods for simulating tactile images and pose/shear prediction. This is applied to two control tasks using a pair of low-cost desktop robotic arms equipped with a vision-based tactile sensor: first, a tactile tracking task, where a follower arm tracks a surface moved by the leader arm; second, a collaborative co-lift task, where both arms jointly hold an object while the leader arm moves along a prescribed trajectory. Our method maintain contact errors within 1-2 mm across varied trajectories where shear sensing is essential for task performance. This work validates the use of sim-to-real shear modeling with rigid-body simulators, opening new possibilities for simulation in tactile robotics.",
        "keywords": [
            "tactile sensing",
            "sim-to-real",
            "tactile servoing"
        ],
        "pdf_url": "https://openreview.net/pdf/43ef217a11165f83d2e6b9d575aa523027f2f6e7.pdf",
        "reviews": [
            {
                "id": "gH2Udvsj7K",
                "forum": "MJmzIunyBy",
                "replyto": "MJmzIunyBy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1128/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325464,
                "mdate": 1754869478111,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "MJmzIunyBy",
                "forum": "MJmzIunyBy",
                "content": {
                    "title": {
                        "value": "SimShear: Sim-to-Real Shear-based Tactile Servoing"
                    },
                    "authors": {
                        "value": [
                            "Kipp Freud",
                            "Yijiong Lin",
                            "Nathan F. Lepora"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kipp_Freud1",
                            "~Yijiong_Lin1",
                            "~Nathan_F._Lepora1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "tactile sensing",
                            "sim-to-real",
                            "tactile servoing"
                        ]
                    },
                    "abstract": {
                        "value": "We present SimShear: a sim-to-real pipeline for tactile control that allows use of shear information without explicitly modeling shear dynamics in simulation. Shear, which arises from lateral movements across contact surfaces, are critical for tasks involving dynamic object interactions but are challenging to simulate. We introduce shPix2pix: a shear-conditioned U-Net GAN that transforms simulated tactile images absent of shear plus a vector encoding shear information into realistic equivalents that include shear deformations, and show this outperforms baseline pix2pix methods for simulating tactile images and pose/shear prediction. This is applied to two control tasks using a pair of low-cost desktop robotic arms equipped with a vision-based tactile sensor: first, a tactile tracking task, where a follower arm tracks a surface moved by the leader arm; second, a collaborative co-lift task, where both arms jointly hold an object while the leader arm moves along a prescribed trajectory. Our method maintain contact errors within 1-2 mm across varied trajectories where shear sensing is essential for task performance. This work validates the use of sim-to-real shear modeling with rigid-body simulators, opening new possibilities for simulation in tactile robotics."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b54cefe642594c6be6694a28284cd168481ec9c4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/43ef217a11165f83d2e6b9d575aa523027f2f6e7.pdf"
                    },
                    "TLDR": {
                        "value": "We introduce SimShear that addresses some drawbacks of current tactile real-to-sim pipelines—namely, the lack of shear in rigid-body simulation and the need to translate real images into the simulated domain at every inference step."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfreud2025simshear,\ntitle={SimShear: Sim-to-Real Shear-based Tactile Servoing},\nauthor={Kipp Freud and Yijiong Lin and Nathan F. Lepora},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MJmzIunyBy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/266361b68aae096158ab30f1c3dac0826e5281cc.zip"
                    },
                    "paperhash": {
                        "value": "freud|simshear_simtoreal_shearbased_tactile_servoing"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1128/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1128/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745491417092,
                "pdate": 1754680645546,
                "odate": 1758062791908,
                "mdate": 1758062829215,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1128/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Ict1OjU9gl",
        "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
        "abstract": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens  to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
        "keywords": [
            "visual-language-action models",
            "object-centric representations",
            "robotic manipulation",
            "imitation learning"
        ],
        "pdf_url": "https://openreview.net/pdf/e3b8f5360b1bcfa1afc25af5cdf129c5abe762c0.pdf",
        "reviews": [
            {
                "id": "p7T9DsCbE4",
                "forum": "Ict1OjU9gl",
                "replyto": "Ict1OjU9gl",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325196,
                "mdate": 1754869478082,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Ict1OjU9gl",
                "forum": "Ict1OjU9gl",
                "content": {
                    "title": {
                        "value": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models"
                    },
                    "authors": {
                        "value": [
                            "Rokas Bendikas",
                            "Daniel Dijkman",
                            "Markus Peschl",
                            "Sanjay Haresh",
                            "Pietro Mazzaglia"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Rokas_Bendikas1",
                            "~Daniel_Dijkman1",
                            "~Markus_Peschl1",
                            "~Sanjay_Haresh1",
                            "~Pietro_Mazzaglia1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "visual-language-action models",
                            "object-centric representations",
                            "robotic manipulation",
                            "imitation learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We drastically reduce the number of visual tokens processed in VLAs to the objects that matter for the agent and find that our approach offers an increase in training efficiency while slightly outperforming its non-object-centric counterpart."
                    },
                    "abstract": {
                        "value": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens  to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/961a2f003a0cd5596e1343ce32096c633263ef33.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/e3b8f5360b1bcfa1afc25af5cdf129c5abe762c0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nbendikas2025focusing,\ntitle={Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models},\nauthor={Rokas Bendikas and Daniel Dijkman and Markus Peschl and Sanjay Haresh and Pietro Mazzaglia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ict1OjU9gl}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f606c29f2177b51b2896ad44a8a68f86c3466a5c.zip"
                    },
                    "paperhash": {
                        "value": "bendikas|focusing_on_what_matters_objectagentcentric_tokenization_for_vision_language_action_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745490267933,
                "pdate": 1754680645284,
                "odate": 1758062791609,
                "mdate": 1758062829114,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "xPryDEv2YH",
        "title": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit",
        "abstract": "Adaptive teaming—the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination—is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices (such as Jetson Orin Nano) and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader drone pursuit task. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \\url{https://sites.google.com/view/at-drone}.",
        "keywords": [
            "adaptive teaming",
            "multi-robot collaboration",
            "multi-drone pursuit"
        ],
        "pdf_url": "https://openreview.net/pdf/750bf92a936d3f0895c9be3f623deeb4841c87f0.pdf",
        "reviews": [
            {
                "id": "gNgWFmgfCT",
                "forum": "xPryDEv2YH",
                "replyto": "xPryDEv2YH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068325159,
                "mdate": 1754869478003,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "xPryDEv2YH",
                "forum": "xPryDEv2YH",
                "content": {
                    "title": {
                        "value": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit"
                    },
                    "authors": {
                        "value": [
                            "Yang Li",
                            "Junfan Chen",
                            "Feng Xue",
                            "Jiabin Qiu",
                            "Wenbin Li",
                            "Qingrui Zhang",
                            "Ying Wen",
                            "Wei Pan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yang_Li40",
                            "~Junfan_Chen3",
                            "~Feng_Xue8",
                            "~Jiabin_Qiu1",
                            "~Wenbin_Li5",
                            "~Qingrui_Zhang1",
                            "~Ying_Wen1",
                            "~Wei_Pan2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "adaptive teaming",
                            "multi-robot collaboration",
                            "multi-drone pursuit"
                        ]
                    },
                    "abstract": {
                        "value": "Adaptive teaming—the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination—is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices (such as Jetson Orin Nano) and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader drone pursuit task. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \\url{https://sites.google.com/view/at-drone}."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/750bf92a936d3f0895c9be3f623deeb4841c87f0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025atdrone,\ntitle={{AT}-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit},\nauthor={Yang Li and Junfan Chen and Feng Xue and Jiabin Qiu and Wenbin Li and Qingrui Zhang and Ying Wen and Wei Pan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xPryDEv2YH}\n}"
                    },
                    "paperhash": {
                        "value": "li|atdrone_benchmarking_adaptive_teaming_in_multidrone_pursuit"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745489367101,
                "pdate": 1754680645200,
                "odate": 1758062791567,
                "mdate": 1758062829084,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Cv3ihZYkZf",
        "title": "Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation",
        "abstract": "Reliable scene understanding requires not only accurate predictions but also well-calibrated confidence estimates to ensure calibrated uncertainty estimation, especially in safety-critical domains like autonomous driving. In this context, semantic segmentation of LiDAR points supports real-time 3D scene understanding, where reliable uncertainty estimates help identify potentially erroneous predictions. While most existing calibration approaches focus on modeling epistemic uncertainty, they often overlook aleatoric uncertainty arising from measurement inaccuracies, which is especially prevalent in LiDAR data and essential for real-world deployment.\nIn this work, we introduce a sampling-free approach for estimating well-calibrated confidence values by explicitly modeling aleatoric uncertainty in semantic segmentation, achieving alignment with true classification accuracy and reducing inference time compared to sampling-based methods. Evaluated on the real-world SemanticKITTI benchmark, our approach achieves 1.70\\% and 1.33\\% Adaptive Calibration Error (ACE) in semantic segmentation of LiDAR data using RangeViT and SalsaNext models, and is more than one order of magnitude faster than the comparable baseline. Furthermore, reliability diagrams reveal that our method produces underconfident rather than overconfident predictions — an advantageous property in safety-critical systems.",
        "keywords": [
            "Confidence Calibration in Deep Learning",
            "Aleatoric Uncertainty Estimation",
            "Reliable Semantic Segmentation of LiDAR Point Clouds"
        ],
        "pdf_url": "https://openreview.net/pdf/f4fffe13f2b7fefa17b7dcf30ef7a39f079af49b.pdf",
        "reviews": [
            {
                "id": "7nyfM0bKtG",
                "forum": "Cv3ihZYkZf",
                "replyto": "Cv3ihZYkZf",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1095/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068324442,
                "mdate": 1754869477860,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Cv3ihZYkZf",
                "forum": "Cv3ihZYkZf",
                "content": {
                    "title": {
                        "value": "Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation"
                    },
                    "authors": {
                        "value": [
                            "Hanieh Shojaei Miandashti",
                            "Qianqian Zou",
                            "Claus Brenner"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hanieh_Shojaei_Miandashti1",
                            "~Qianqian_Zou1",
                            "~Claus_Brenner2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Confidence Calibration in Deep Learning",
                            "Aleatoric Uncertainty Estimation",
                            "Reliable Semantic Segmentation of LiDAR Point Clouds"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a novel sampling-free method for aleatoric uncertainty estimation, which utilizes the overlap between output distributions to calibrate confidence scores in LiDAR scene semantic segmentation."
                    },
                    "abstract": {
                        "value": "Reliable scene understanding requires not only accurate predictions but also well-calibrated confidence estimates to ensure calibrated uncertainty estimation, especially in safety-critical domains like autonomous driving. In this context, semantic segmentation of LiDAR points supports real-time 3D scene understanding, where reliable uncertainty estimates help identify potentially erroneous predictions. While most existing calibration approaches focus on modeling epistemic uncertainty, they often overlook aleatoric uncertainty arising from measurement inaccuracies, which is especially prevalent in LiDAR data and essential for real-world deployment.\nIn this work, we introduce a sampling-free approach for estimating well-calibrated confidence values by explicitly modeling aleatoric uncertainty in semantic segmentation, achieving alignment with true classification accuracy and reducing inference time compared to sampling-based methods. Evaluated on the real-world SemanticKITTI benchmark, our approach achieves 1.70\\% and 1.33\\% Adaptive Calibration Error (ACE) in semantic segmentation of LiDAR data using RangeViT and SalsaNext models, and is more than one order of magnitude faster than the comparable baseline. Furthermore, reliability diagrams reveal that our method produces underconfident rather than overconfident predictions — an advantageous property in safety-critical systems."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f4fffe13f2b7fefa17b7dcf30ef7a39f079af49b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmiandashti2025uncertaintyaware,\ntitle={Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation},\nauthor={Hanieh Shojaei Miandashti and Qianqian Zou and Claus Brenner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Cv3ihZYkZf}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/e7fceac986c8a563874c0b5136c1b14f9a8eef82.zip"
                    },
                    "paperhash": {
                        "value": "miandashti|uncertaintyaware_scene_understanding_via_efficient_samplingfree_confidence_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1095/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1095/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745485489975,
                "pdate": 1754680644681,
                "odate": 1758062790931,
                "mdate": 1758062828970,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1095/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1095/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "thVTNoJ4Lx",
        "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
        "abstract": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level ``WayObject Costmap'' representation that eliminates the need for an explicit RGB input.\n We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments.",
        "keywords": [
            "visual navigation",
            "topological",
            "object",
            "learning"
        ],
        "pdf_url": "https://openreview.net/pdf/6317ef29deee37d85952ccd4418c610f7846aabd.pdf",
        "reviews": [
            {
                "id": "8DYorFMRmh",
                "forum": "thVTNoJ4Lx",
                "replyto": "thVTNoJ4Lx",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068324398,
                "mdate": 1754869477808,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "thVTNoJ4Lx",
                "forum": "thVTNoJ4Lx",
                "content": {
                    "title": {
                        "value": "ObjectReact: Learning Object-Relative Control for Visual Navigation"
                    },
                    "authors": {
                        "value": [
                            "Sourav Garg",
                            "Dustin Craggs",
                            "Vineeth Bhat",
                            "Lachlan Mares",
                            "Stefan Podgorski",
                            "Madhava Krishna",
                            "Feras Dayoub",
                            "Ian Reid"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sourav_Garg1",
                            "~Dustin_Craggs1",
                            "~Vineeth_Bhat1",
                            "~Lachlan_Mares1",
                            "stefan.podgorski@adelaide.edu.au",
                            "~Madhava_Krishna2",
                            "~Feras_Dayoub1",
                            "~Ian_Reid1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "visual navigation",
                            "topological",
                            "object",
                            "learning"
                        ]
                    },
                    "TLDR": {
                        "value": "Learning a navigation control policy conditioned on object-level global path planning costs"
                    },
                    "abstract": {
                        "value": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level ``WayObject Costmap'' representation that eliminates the need for an explicit RGB input.\n We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1d9e823ffe6cbe238fba323c3822a342edc08741.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6317ef29deee37d85952ccd4418c610f7846aabd.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngarg2025objectreact,\ntitle={ObjectReact: Learning Object-Relative Control for Visual Navigation},\nauthor={Sourav Garg and Dustin Craggs and Vineeth Bhat and Lachlan Mares and Stefan Podgorski and Madhava Krishna and Feras Dayoub and Ian Reid},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=thVTNoJ4Lx}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/75c7456bc7e522ec67dbb6757175764b6a15014b.mp4"
                    },
                    "paperhash": {
                        "value": "garg|objectreact_learning_objectrelative_control_for_visual_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745485116244,
                "pdate": 1754680644679,
                "odate": 1758062790929,
                "mdate": 1758062828881,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1092/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "IuiB5iaMxy",
        "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning",
        "abstract": "This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments:  https://github.com/anonymousCoRL/MDCM_CoRL2025",
        "keywords": [
            "aerial manipulation",
            "multi-agent reinforcement learning",
            "micro-aerial vehicles"
        ],
        "pdf_url": "https://openreview.net/pdf/111278cc41b9048e49e885348f602fb090f4ae08.pdf",
        "reviews": [
            {
                "id": "7jB9ahKJS9",
                "forum": "IuiB5iaMxy",
                "replyto": "IuiB5iaMxy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068324167,
                "mdate": 1754869477705,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "IuiB5iaMxy",
                "forum": "IuiB5iaMxy",
                "content": {
                    "title": {
                        "value": "Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Jack Zeng",
                            "Andreu Matoses Gimenez",
                            "Eugene Vinitsky",
                            "Javier Alonso-Mora",
                            "Sihao Sun"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jack_Zeng1",
                            "~Andreu_Matoses_Gimenez1",
                            "~Eugene_Vinitsky1",
                            "~Javier_Alonso-Mora1",
                            "~Sihao_Sun1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "aerial manipulation",
                            "multi-agent reinforcement learning",
                            "micro-aerial vehicles"
                        ]
                    },
                    "abstract": {
                        "value": "This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments:  https://github.com/anonymousCoRL/MDCM_CoRL2025"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/111278cc41b9048e49e885348f602fb090f4ae08.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzeng2025decentralized,\ntitle={Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning},\nauthor={Jack Zeng and Andreu Matoses Gimenez and Eugene Vinitsky and Javier Alonso-Mora and Sihao Sun},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=IuiB5iaMxy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b96538b063ae50399e130d767982d578666f543f.zip"
                    },
                    "paperhash": {
                        "value": "zeng|decentralized_aerial_manipulation_of_a_cablesuspended_load_using_multiagent_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745483727530,
                "pdate": 1754680644486,
                "odate": 1758062790628,
                "mdate": 1758062828826,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1086/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "RqNpzq17kw",
        "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving",
        "abstract": "Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases.",
        "keywords": [
            "Multimodal LLM",
            "Closed-Loop Evaluation",
            "Autonomous Driving"
        ],
        "pdf_url": "https://openreview.net/pdf/5a7a839f5ce149913fdcf46b7dd5847263c0c341.pdf",
        "reviews": [
            {
                "id": "HrgWbnzy9o",
                "forum": "RqNpzq17kw",
                "replyto": "RqNpzq17kw",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068324024,
                "mdate": 1754869477607,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "RqNpzq17kw",
                "forum": "RqNpzq17kw",
                "content": {
                    "title": {
                        "value": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving"
                    },
                    "authors": {
                        "value": [
                            "Xueyi Liu",
                            "Zuodong Zhong",
                            "Qichao Zhang",
                            "Yuxin Guo",
                            "Yupeng Zheng",
                            "Junli Wang",
                            "Dongbin Zhao",
                            "Yun-Fu Liu",
                            "Zhiguo Su",
                            "Yinfeng Gao",
                            "Qiao Lin",
                            "Chen Huiyong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xueyi_Liu3",
                            "~Zuodong_Zhong1",
                            "~Qichao_Zhang3",
                            "~Yuxin_Guo2",
                            "~Yupeng_Zheng1",
                            "~Junli_Wang4",
                            "~Dongbin_Zhao1",
                            "~Yun-Fu_Liu1",
                            "~Zhiguo_Su1",
                            "~Yinfeng_Gao1",
                            "~Qiao_Lin2",
                            "~Chen_Huiyong1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multimodal LLM",
                            "Closed-Loop Evaluation",
                            "Autonomous Driving"
                        ]
                    },
                    "abstract": {
                        "value": "Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process."
                    },
                    "pdf": {
                        "value": "/pdf/5a7a839f5ce149913fdcf46b7dd5847263c0c341.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025reasonplan,\ntitle={ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving},\nauthor={Xueyi Liu and Zuodong Zhong and Qichao Zhang and Yuxin Guo and Yupeng Zheng and Junli Wang and Dongbin Zhao and Yun-Fu Liu and Zhiguo Su and Yinfeng Gao and Qiao Lin and Chen Huiyong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RqNpzq17kw}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b8cf9ac474be90857bc2b81425ee632795334915.zip"
                    },
                    "paperhash": {
                        "value": "liu|reasonplan_unified_scene_prediction_and_decision_reasoning_for_closedloop_autonomous_driving"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745483693634,
                "pdate": 1754680644412,
                "odate": 1758062790628,
                "mdate": 1758062828734,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JibqR9sEdW",
        "title": "Embrace Contacts: humanoid shadowing with full body ground contacts",
        "abstract": "Previous humanoid robot research works treat the robot as a bipedal mobile manipulation platform, where only the feet and hands contact the environment. However, we humans use all body parts to interact with the world, e.g., we sit in chairs, get up from the ground, or roll on the floor. Contacting the environment using body parts other than feet and hands brings significant challenges in both model-predictive control and reinforcement learning-based methods: an unpredictable contact sequence makes it almost impossible for model-predictive control to plan ahead in real time; the success of sim-to-real reinforcement learning for humanoids heavily depends on the acceleration of the rigid-body physical simulator and the simplification of collision detection. On the other hand, lacking extreme torso movement of humanoid data makes all other components non-trivial to design, such as dataset distribution, motion commands, and task rewards. To address these challenges, we propose a general humanoid motion framework that takes discrete motion commands and controls the robot’s motor actions in real time. Using a GPU-accelerated simulator, we train a humanoid whole-body control policy that follows the high-level motion command in the real world in real time, even with stochastic contacts and extremely large robot base rotation and not-so-feasible motion commands.",
        "keywords": [
            "Sim-to-Real",
            "Deep Reinforcement Learning",
            "Humanoid",
            "Whole-body Control"
        ],
        "pdf_url": "https://openreview.net/pdf/fe24ba5cc928580062abe842f438c25fcdeb8fc5.pdf",
        "reviews": [
            {
                "id": "x0PMPIwaOi",
                "forum": "JibqR9sEdW",
                "replyto": "JibqR9sEdW",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068324022,
                "mdate": 1754869477554,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JibqR9sEdW",
                "forum": "JibqR9sEdW",
                "content": {
                    "title": {
                        "value": "Embrace Contacts: humanoid shadowing with full body ground contacts"
                    },
                    "authors": {
                        "value": [
                            "Ziwen Zhuang",
                            "Hang Zhao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ziwen_Zhuang1",
                            "~Hang_Zhao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Sim-to-Real",
                            "Deep Reinforcement Learning",
                            "Humanoid",
                            "Whole-body Control"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a unified humanoid motion interface and a zero-shot sim-to-real reinforcement learning framework, so that humanoid robots can successfully perform extreme contact-agnostic motion in the real world."
                    },
                    "abstract": {
                        "value": "Previous humanoid robot research works treat the robot as a bipedal mobile manipulation platform, where only the feet and hands contact the environment. However, we humans use all body parts to interact with the world, e.g., we sit in chairs, get up from the ground, or roll on the floor. Contacting the environment using body parts other than feet and hands brings significant challenges in both model-predictive control and reinforcement learning-based methods: an unpredictable contact sequence makes it almost impossible for model-predictive control to plan ahead in real time; the success of sim-to-real reinforcement learning for humanoids heavily depends on the acceleration of the rigid-body physical simulator and the simplification of collision detection. On the other hand, lacking extreme torso movement of humanoid data makes all other components non-trivial to design, such as dataset distribution, motion commands, and task rewards. To address these challenges, we propose a general humanoid motion framework that takes discrete motion commands and controls the robot’s motor actions in real time. Using a GPU-accelerated simulator, we train a humanoid whole-body control policy that follows the high-level motion command in the real world in real time, even with stochastic contacts and extremely large robot base rotation and not-so-feasible motion commands."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ed7d013b412c417a382de938b8cb8d0d600d06c5.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/fe24ba5cc928580062abe842f438c25fcdeb8fc5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhuang2025embrace,\ntitle={Embrace Contacts: humanoid shadowing with full body ground contacts},\nauthor={Ziwen Zhuang and Hang Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JibqR9sEdW}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1ee38c5cc4763d0cbd1785ea39263dd4423bca5a.zip"
                    },
                    "paperhash": {
                        "value": "zhuang|embrace_contacts_humanoid_shadowing_with_full_body_ground_contacts"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745483540744,
                "pdate": 1754680644343,
                "odate": 1758062790573,
                "mdate": 1758062828544,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "udH3b2Lsx5",
        "title": "$\\texttt{SPIN}$: distilling $\\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation",
        "abstract": "Current robots struggle with long-horizon manipulation tasks requiring sequences of prehensile and non-prehensile skills, contact-rich interactions, and long-term reasoning. We present $\\texttt{SPIN}$ ($\\textbf{S}$kill $\\textbf{P}$lanning to $\\textbf{IN}$ference), a framework that distills a computationally intensive planning algorithm into a policy via imitation learning.\nWe propose $\\texttt{Skill-RRT}$, an extension of RRT that incorporates skill applicability checks and intermediate object pose sampling for solving such long-horizon problems. To chain independently trained skills, we introduce $\\textit{connectors}$, goal-conditioned policies trained to minimize object disturbance during transitions. High-quality demonstrations are generated with $\\texttt{Skill-RRT}$ and distilled through noise-based replay in order to reduce online computation time. The resulting policy, trained entirely in simulation, transfers zero-shot to the real world and achieves over 80\\% success across three challenging long-horizon manipulation tasks and outperforms state-of-the-art hierarchical RL and planning methods.",
        "keywords": [
            "Robot Skill Chaining",
            "Imitation Learning",
            "Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/73b513b25253280a5ff7e8777dd2157b4e3f4077.pdf",
        "reviews": [
            {
                "id": "j2rjyNd1xY",
                "forum": "udH3b2Lsx5",
                "replyto": "udH3b2Lsx5",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068323846,
                "mdate": 1754869477416,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "udH3b2Lsx5",
                "forum": "udH3b2Lsx5",
                "content": {
                    "title": {
                        "value": "$\\texttt{SPIN}$: distilling $\\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation"
                    },
                    "authors": {
                        "value": [
                            "Haewon Jung",
                            "Donguk Lee",
                            "Haecheol Park",
                            "Kim Jun Hyeop",
                            "Beomjoon Kim"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haewon_Jung1",
                            "~Donguk_Lee1",
                            "~Haecheol_Park2",
                            "~Kim_Jun_Hyeop1",
                            "~Beomjoon_Kim2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Skill Chaining",
                            "Imitation Learning",
                            "Planning"
                        ]
                    },
                    "abstract": {
                        "value": "Current robots struggle with long-horizon manipulation tasks requiring sequences of prehensile and non-prehensile skills, contact-rich interactions, and long-term reasoning. We present $\\texttt{SPIN}$ ($\\textbf{S}$kill $\\textbf{P}$lanning to $\\textbf{IN}$ference), a framework that distills a computationally intensive planning algorithm into a policy via imitation learning.\nWe propose $\\texttt{Skill-RRT}$, an extension of RRT that incorporates skill applicability checks and intermediate object pose sampling for solving such long-horizon problems. To chain independently trained skills, we introduce $\\textit{connectors}$, goal-conditioned policies trained to minimize object disturbance during transitions. High-quality demonstrations are generated with $\\texttt{Skill-RRT}$ and distilled through noise-based replay in order to reduce online computation time. The resulting policy, trained entirely in simulation, transfers zero-shot to the real world and achieves over 80\\% success across three challenging long-horizon manipulation tasks and outperforms state-of-the-art hierarchical RL and planning methods."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/73b513b25253280a5ff7e8777dd2157b4e3f4077.pdf"
                    },
                    "TLDR": {
                        "value": "$\\texttt{SPIN}$ distills a planner into a policy for long-horizon prehensile and non-prehensile manipulation by using $\\texttt{Skill-RRT}$ and $\\textit{connectors}$, enabling fast, robust, and zero-shot execution in simulation and the real world."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njung2025textttspin,\ntitle={\\${\\textbackslash}texttt\\{{SPIN}\\}\\$: distilling \\${\\textbackslash}texttt\\{Skill-{RRT}\\}\\$ for long-horizon prehensile and non-prehensile manipulation},\nauthor={Haewon Jung and Donguk Lee and Haecheol Park and Kim Jun Hyeop and Beomjoon Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=udH3b2Lsx5}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0d019529dd3d0eb467dc1149e925564502c9ef79.zip"
                    },
                    "paperhash": {
                        "value": "jung|\\textttspin_distilling_\\textttskillrrt_for_longhorizon_prehensile_and_nonprehensile_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745482188113,
                "pdate": 1754680644119,
                "odate": 1758062790251,
                "mdate": 1758062827870,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1074/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "nryBWao01j",
        "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing",
        "abstract": "Diffusion models hold great potential in robotics due to their ability to capture complex, high-dimensional data distributions. However, their lack of constraint-awareness limits their deployment in safety-critical applications. We propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and general-purpose framework that integrates barrier functions into the denoising process, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG enables constraint satisfaction even with limited training data and generalizes across tasks. We evaluate our framework in the challenging setting of miniature autonomous racing, where real-time obstacle avoidance is essential. Real-world experiments show that CoDiG generates safe outputs efficiently under dynamic conditions, highlighting its potential for broader robotic applications.",
        "keywords": [
            "Diffusion Guidance",
            "Constraint-Aware Sampling",
            "Real-Time Obstacle Avoidance",
            "Autonomous Racing",
            "Safe Control"
        ],
        "pdf_url": "https://openreview.net/pdf/ae69dfac37aa41910b8ab49cc983ff311c41e51b.pdf",
        "reviews": [
            {
                "id": "m0imoGmnS4",
                "forum": "nryBWao01j",
                "replyto": "nryBWao01j",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068322794,
                "mdate": 1754869477427,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "nryBWao01j",
                "forum": "nryBWao01j",
                "content": {
                    "title": {
                        "value": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing"
                    },
                    "authors": {
                        "value": [
                            "Hao Ma",
                            "Sabrina Bodmer",
                            "Andrea Carron",
                            "Melanie Zeilinger",
                            "Michael Muehlebach"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hao_Ma6",
                            "~Sabrina_Bodmer1",
                            "~Andrea_Carron1",
                            "~Melanie_Zeilinger1",
                            "~Michael_Muehlebach1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Diffusion Guidance",
                            "Constraint-Aware Sampling",
                            "Real-Time Obstacle Avoidance",
                            "Autonomous Racing",
                            "Safe Control"
                        ]
                    },
                    "abstract": {
                        "value": "Diffusion models hold great potential in robotics due to their ability to capture complex, high-dimensional data distributions. However, their lack of constraint-awareness limits their deployment in safety-critical applications. We propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and general-purpose framework that integrates barrier functions into the denoising process, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG enables constraint satisfaction even with limited training data and generalizes across tasks. We evaluate our framework in the challenging setting of miniature autonomous racing, where real-time obstacle avoidance is essential. Real-world experiments show that CoDiG generates safe outputs efficiently under dynamic conditions, highlighting its potential for broader robotic applications."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c0de470c9df4dc17ee2de5e0a83e0459477f28dc.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/ae69dfac37aa41910b8ab49cc983ff311c41e51b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nma2025constraintaware,\ntitle={Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing},\nauthor={Hao Ma and Sabrina Bodmer and Andrea Carron and Melanie Zeilinger and Michael Muehlebach},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=nryBWao01j}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/644c893c84a455aad9b9dd1971a4dfe72971ac1a.mp4"
                    },
                    "paperhash": {
                        "value": "ma|constraintaware_diffusion_guidance_for_robotics_realtime_obstacle_avoidance_for_autonomous_racing"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745477201988,
                "pdate": 1754680643606,
                "odate": 1758062789446,
                "mdate": 1758062827561,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1051/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "dmXFboqSnX",
        "title": "Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study",
        "abstract": "Dynamic rotational maneuvers, such as front flips, inherently involve large angular momentum generation and intense impact forces, presenting major challenges for reinforcement learning and sim-to-real transfer. In this work, we propose a general framework for learning and deploying impact-rich, rotation-intensive behaviors through centroidal velocity-based rewards and actuator-aware sim-to-real techniques. We identify that conventional link-level reward formulations fail to induce true whole-body rotation and introduce a centroidal angular velocity reward that accurately captures system-wide rotational dynamics. To bridge the sim-to-real gap under extreme conditions, we model motor operating regions (MOR) and apply transmission load regularization to ensure realistic torque commands and mechanical robustness. Using the one-leg hopper front flip as a representative case study, we demonstrate the first successful hardware realization of a full front flip. Our results highlight that incorporating centroidal dynamics and actuator constraints is critical for reliably executing highly dynamic motions.",
        "keywords": [
            "Reinforcement Learning",
            "Sim-to-Real Transfer",
            "One-Leg Hopper"
        ],
        "pdf_url": "https://openreview.net/pdf/4ca434689978e455d838fb0876658a2c359130e3.pdf",
        "reviews": [
            {
                "id": "PwYmbp6kLf",
                "forum": "dmXFboqSnX",
                "replyto": "dmXFboqSnX",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068322403,
                "mdate": 1754869477327,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "dmXFboqSnX",
                "forum": "dmXFboqSnX",
                "content": {
                    "title": {
                        "value": "Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study"
                    },
                    "authors": {
                        "value": [
                            "Dongyun Kang",
                            "Gijeong Kim",
                            "JongHun Choe",
                            "Hajun Kim",
                            "Hae-Won Park"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Dongyun_Kang1",
                            "~Gijeong_Kim1",
                            "~JongHun_Choe1",
                            "~Hajun_Kim1",
                            "~Hae-Won_Park2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement Learning",
                            "Sim-to-Real Transfer",
                            "One-Leg Hopper"
                        ]
                    },
                    "abstract": {
                        "value": "Dynamic rotational maneuvers, such as front flips, inherently involve large angular momentum generation and intense impact forces, presenting major challenges for reinforcement learning and sim-to-real transfer. In this work, we propose a general framework for learning and deploying impact-rich, rotation-intensive behaviors through centroidal velocity-based rewards and actuator-aware sim-to-real techniques. We identify that conventional link-level reward formulations fail to induce true whole-body rotation and introduce a centroidal angular velocity reward that accurately captures system-wide rotational dynamics. To bridge the sim-to-real gap under extreme conditions, we model motor operating regions (MOR) and apply transmission load regularization to ensure realistic torque commands and mechanical robustness. Using the one-leg hopper front flip as a representative case study, we demonstrate the first successful hardware realization of a full front flip. Our results highlight that incorporating centroidal dynamics and actuator constraints is critical for reliably executing highly dynamic motions."
                    },
                    "supplementary_material": {
                        "value": "/attachment/45f1baa7b387af2c626a3c1a13aa359fc3f98885.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4ca434689978e455d838fb0876658a2c359130e3.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkang2025learning,\ntitle={Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study},\nauthor={Dongyun Kang and Gijeong Kim and JongHun Choe and Hajun Kim and Hae-Won Park},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dmXFboqSnX}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b2182ba35b02ce2c9ba3b2e35a811c391e7fabf5.zip"
                    },
                    "paperhash": {
                        "value": "kang|learning_impactrich_rotational_maneuvers_via_centroidal_velocity_rewards_and_simtoreal_techniques_a_oneleg_hopper_flip_case_study"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745476219774,
                "pdate": 1754680643480,
                "odate": 1758062789134,
                "mdate": 1758062828632,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "6yB6AX8aSU",
        "title": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations",
        "abstract": "Developing robotic systems capable of robustly executing long-horizon manipulation tasks with human-level dexterity is challenging, as such tasks require both physical dexterity and seamless sequencing of manipulation skills while robustly handling environment variations. While imitation learning offers a promising approach, acquiring comprehensive datasets is resource-intensive. In this work, we propose a learning framework and system LodeStar that automatically decomposes task demonstrations into semantically meaningful skills using off-the-shelf foundation models, and generates diverse synthetic demonstration datasets from a few human demos through reinforcement learning. These sim-augmented datasets enable robust skill training, with a Skill Routing Transformer (SRT) policy effectively chaining the learned skills together to execute complex long-horizon manipulation tasks. Experimental evaluations on three challenging real-world long-horizon dexterous manipulation tasks demonstrate that our approach significantly improves task performance and robustness compared to previous baselines. Videos are available at lodestar-robot.github.io.",
        "keywords": [
            "Dexterous Manipulation",
            "Imitation Learning",
            "Sim-to-Real"
        ],
        "pdf_url": "https://openreview.net/pdf/58cb887ad909f57c866aad08dae8f76273b0f36c.pdf",
        "reviews": [
            {
                "id": "qTM1jRGtnk",
                "forum": "6yB6AX8aSU",
                "replyto": "6yB6AX8aSU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321977,
                "mdate": 1754869477212,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "6yB6AX8aSU",
                "forum": "6yB6AX8aSU",
                "content": {
                    "title": {
                        "value": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations"
                    },
                    "authors": {
                        "value": [
                            "Weikang Wan",
                            "Jiawei Fu",
                            "Xiaodi Yuan",
                            "Yifeng Zhu",
                            "Hao Su"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Weikang_Wan1",
                            "~Jiawei_Fu1",
                            "~Xiaodi_Yuan1",
                            "~Yifeng_Zhu2",
                            "~Hao_Su1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Manipulation",
                            "Imitation Learning",
                            "Sim-to-Real"
                        ]
                    },
                    "abstract": {
                        "value": "Developing robotic systems capable of robustly executing long-horizon manipulation tasks with human-level dexterity is challenging, as such tasks require both physical dexterity and seamless sequencing of manipulation skills while robustly handling environment variations. While imitation learning offers a promising approach, acquiring comprehensive datasets is resource-intensive. In this work, we propose a learning framework and system LodeStar that automatically decomposes task demonstrations into semantically meaningful skills using off-the-shelf foundation models, and generates diverse synthetic demonstration datasets from a few human demos through reinforcement learning. These sim-augmented datasets enable robust skill training, with a Skill Routing Transformer (SRT) policy effectively chaining the learned skills together to execute complex long-horizon manipulation tasks. Experimental evaluations on three challenging real-world long-horizon dexterous manipulation tasks demonstrate that our approach significantly improves task performance and robustness compared to previous baselines. Videos are available at lodestar-robot.github.io."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/58cb887ad909f57c866aad08dae8f76273b0f36c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwan2025lodestar,\ntitle={LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations},\nauthor={Weikang Wan and Jiawei Fu and Xiaodi Yuan and Yifeng Zhu and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=6yB6AX8aSU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5d82eea2794667d72f965ec6b8d49411b19246e4.zip"
                    },
                    "paperhash": {
                        "value": "wan|lodestar_longhorizon_dexterity_via_synthetic_data_augmentation_from_human_demonstrations"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745474592371,
                "pdate": 1754680643068,
                "odate": 1758062788015,
                "mdate": 1758062827281,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Af2RMaWRjm",
        "title": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics",
        "abstract": "In the field of robot learning, it is becoming possible to coordinate robot action through language instructions. On the other hand, it is still a difficult task to adjust the action based on human instructions because human instructions are often qualitative, and there are cases where there is no one-to-one correspondence between the behavior and the instructions. In this paper, we propose a motion generation model that can adjust actions in response to qualitative human instructions during task execution. The core of the proposed method is a learning architecture that maps qualitative human instructions to actions. Specifically, the demonstration is divided into short action sequences, and labels reflecting human qualitative senses are assigned to these sequences to realize learning that links human qualitative instructions and robot actions. In evaluation experiments, we verified the effectiveness of the method in two tasks: a pick-and-place task and a wiping task. Experimental results showed that the proposed method is able to generate motions in response to human qualitative instructions during task execution, whereas the conventional method generates trajectories all at once, making it impossible to adjust motions during task execution.",
        "keywords": [
            "Imitation learning",
            "Disentangled representation learning"
        ],
        "pdf_url": "https://openreview.net/pdf/46efe522c13a31a30b128dfe2d777e76b6bf1acb.pdf",
        "reviews": [
            {
                "id": "E79WdYHBAz",
                "forum": "Af2RMaWRjm",
                "replyto": "Af2RMaWRjm",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321792,
                "mdate": 1754869477215,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Af2RMaWRjm",
                "forum": "Af2RMaWRjm",
                "content": {
                    "title": {
                        "value": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics"
                    },
                    "authors": {
                        "value": [
                            "Ryoga Oishi",
                            "Sho Sakaino",
                            "Toshiaki Tsuji"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ryoga_Oishi1",
                            "sakaino@iit.tsukuba.ac.jp",
                            "~Toshiaki_Tsuji1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation learning",
                            "Disentangled representation learning"
                        ]
                    },
                    "abstract": {
                        "value": "In the field of robot learning, it is becoming possible to coordinate robot action through language instructions. On the other hand, it is still a difficult task to adjust the action based on human instructions because human instructions are often qualitative, and there are cases where there is no one-to-one correspondence between the behavior and the instructions. In this paper, we propose a motion generation model that can adjust actions in response to qualitative human instructions during task execution. The core of the proposed method is a learning architecture that maps qualitative human instructions to actions. Specifically, the demonstration is divided into short action sequences, and labels reflecting human qualitative senses are assigned to these sequences to realize learning that links human qualitative instructions and robot actions. In evaluation experiments, we verified the effectiveness of the method in two tasks: a pick-and-place task and a wiping task. Experimental results showed that the proposed method is able to generate motions in response to human qualitative instructions during task execution, whereas the conventional method generates trajectories all at once, making it impossible to adjust motions during task execution."
                    },
                    "supplementary_material": {
                        "value": "/attachment/37aa45869b773b06c1ec8b9c5fc0c1962661a6bc.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/46efe522c13a31a30b128dfe2d777e76b6bf1acb.pdf"
                    },
                    "TLDR": {
                        "value": "We propose a motion generation model that adapts to human modifier directives in real time during task execution."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\noishi2025imitation,\ntitle={Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics},\nauthor={Ryoga Oishi and Sho Sakaino and Toshiaki Tsuji},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Af2RMaWRjm}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/616f4b2cb45b144d6af0dccc2d6ab45317bc9d7b.mp4"
                    },
                    "paperhash": {
                        "value": "oishi|imitation_learning_based_on_disentangled_representation_learning_of_behavioral_characteristics"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745474244071,
                "pdate": 1754680642918,
                "odate": 1758062787776,
                "mdate": 1758062827044,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "C6VxzSpjrv",
        "title": "Visual Imitation Enables Contextual Humanoid Control",
        "abstract": "How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably the simplest way is to _just show them_—casually capture a human motion video and feed it to humanoids. We introduce **VideoMimic**, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills all from a single policy, conditioned on the environment and global root commands. We hope our data and approach help enable a scalable path towards teaching humanoids to operate in diverse real-world environments.",
        "keywords": [
            "Visual Imitation",
            "Humanoids",
            "Reinforcement Learning",
            "Reconstruction",
            "Real2Sim2Real"
        ],
        "pdf_url": "https://openreview.net/pdf/c4302c60608fddf06157a7f3c495a645621af6d7.pdf",
        "reviews": [
            {
                "id": "7kDCpWvaWe",
                "forum": "C6VxzSpjrv",
                "replyto": "C6VxzSpjrv",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321663,
                "mdate": 1754869459477,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "C6VxzSpjrv",
                "forum": "C6VxzSpjrv",
                "content": {
                    "title": {
                        "value": "Visual Imitation Enables Contextual Humanoid Control"
                    },
                    "authors": {
                        "value": [
                            "Arthur Allshire",
                            "Hongsuk Choi",
                            "Junyi Zhang",
                            "David McAllister",
                            "Anthony Zhang",
                            "Chung Min Kim",
                            "Trevor Darrell",
                            "Pieter Abbeel",
                            "Jitendra Malik",
                            "Angjoo Kanazawa"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Arthur_Allshire1",
                            "~Hongsuk_Choi1",
                            "~Junyi_Zhang3",
                            "~David_McAllister2",
                            "~Anthony_Zhang2",
                            "~Chung_Min_Kim1",
                            "~Trevor_Darrell2",
                            "~Pieter_Abbeel2",
                            "~Jitendra_Malik2",
                            "~Angjoo_Kanazawa1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Visual Imitation",
                            "Humanoids",
                            "Reinforcement Learning",
                            "Reconstruction",
                            "Real2Sim2Real"
                        ]
                    },
                    "TLDR": {
                        "value": "Reconstruct human + env from video, train a policy to control humanoid to do all those skills in real."
                    },
                    "abstract": {
                        "value": "How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably the simplest way is to _just show them_—casually capture a human motion video and feed it to humanoids. We introduce **VideoMimic**, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills all from a single policy, conditioned on the environment and global root commands. We hope our data and approach help enable a scalable path towards teaching humanoids to operate in diverse real-world environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f779d0a0e2faf429a711b1b31d22140a170ab009.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c4302c60608fddf06157a7f3c495a645621af6d7.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nallshire2025visual,\ntitle={Visual Imitation Enables Contextual Humanoid Control},\nauthor={Arthur Allshire and Hongsuk Choi and Junyi Zhang and David McAllister and Anthony Zhang and Chung Min Kim and Trevor Darrell and Pieter Abbeel and Jitendra Malik and Angjoo Kanazawa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=C6VxzSpjrv}\n}"
                    },
                    "paperhash": {
                        "value": "allshire|visual_imitation_enables_contextual_humanoid_control"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745473498657,
                "pdate": 1754680642667,
                "odate": 1758062787677,
                "mdate": 1758062827052,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1024/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JXBm4Xfrvj",
        "title": "Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility",
        "abstract": "Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. \nMotion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. \nWe address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors.\nA subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains.\nSimulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors.\nFurthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. \nReal-world experiments with an ANYmal-D quadruped robot confirm our policy’s capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.",
        "keywords": [
            "Motion Priors",
            "Reinforcement Learning",
            "Legged Locomotion",
            "Local Navigation"
        ],
        "pdf_url": "https://openreview.net/pdf/8d99b118b740cc53999de39f2acd3da01e296529.pdf",
        "reviews": [
            {
                "id": "QfwqLCIkvZ",
                "forum": "JXBm4Xfrvj",
                "replyto": "JXBm4Xfrvj",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321612,
                "mdate": 1754869477132,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JXBm4Xfrvj",
                "forum": "JXBm4Xfrvj",
                "content": {
                    "title": {
                        "value": "Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility"
                    },
                    "authors": {
                        "value": [
                            "Zewei Zhang",
                            "Chenhao Li",
                            "Takahiro Miki",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zewei_Zhang3",
                            "~Chenhao_Li3",
                            "~Takahiro_Miki1",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Motion Priors",
                            "Reinforcement Learning",
                            "Legged Locomotion",
                            "Local Navigation"
                        ]
                    },
                    "abstract": {
                        "value": "Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. \nMotion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. \nWe address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors.\nA subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains.\nSimulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors.\nFurthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. \nReal-world experiments with an ANYmal-D quadruped robot confirm our policy’s capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/8d99b118b740cc53999de39f2acd3da01e296529.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025motion,\ntitle={Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility},\nauthor={Zewei Zhang and Chenhao Li and Takahiro Miki and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JXBm4Xfrvj}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1ad24970e81190dbf1b7622e5cb00ccfbf981644.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|motion_priors_reimagined_adapting_flatterrain_skills_for_complex_quadruped_mobility"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745473263715,
                "pdate": 1754680642544,
                "odate": 1758062787553,
                "mdate": 1758062827633,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "UTPBM4dEUS",
        "title": "Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning",
        "abstract": "Sim-to-real discrepancies hinder learning-based policies from achieving high-precision tasks in the real world. While Domain Randomization (DR) is commonly used to bridge this gap, it often relies on heuristics and can lead to overly conservative policies with degrading performance when not properly tuned. System Identification (Sys-ID) offers a targeted approach, but standard techniques rely on differentiable dynamics and/or direct torque measurement, assumptions that rarely hold for contact-rich legged systems. To this end, we present SPI-Active (Sampling-based Parameter Identification with Active Exploration), a two-stage framework that estimates physical parameters of legged robots to minimize the sim-to-real gap. SPI-Active robustly identifies key physical parameters through massive parallel sampling, minimizing state prediction errors between simulated and real-world trajectories. To further improve the informativeness of collected data, we introduce an active exploration strategy that maximizes the Fisher Information of the collected real-world trajectories via optimizing the input commands of an exploration policy. This targeted exploration leads to accurate identification and better generalization across diverse tasks. Experimental results demonstrate that SPI-Active enables precise sim-to-real transfer of learned policies to the real world, outperforming baselines by 42-63% in various locomotion tasks. Videos at the anonymous website https://anonymous-spi-active.github.io/",
        "keywords": [
            "System Identification",
            "Sim2Real",
            "Legged Robots"
        ],
        "pdf_url": "https://openreview.net/pdf/e5927f067ebfe16f62864c4e2bf3027de9f9cdcc.pdf",
        "reviews": [
            {
                "id": "pUm7HhkCHm",
                "forum": "UTPBM4dEUS",
                "replyto": "UTPBM4dEUS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321469,
                "mdate": 1754869459331,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "UTPBM4dEUS",
                "forum": "UTPBM4dEUS",
                "content": {
                    "title": {
                        "value": "Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning"
                    },
                    "authors": {
                        "value": [
                            "Nikhil Sobanbabu",
                            "Guanqi He",
                            "Tairan He",
                            "Yuxiang Yang",
                            "Guanya Shi"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Nikhil_Sobanbabu1",
                            "~Guanqi_He1",
                            "~Tairan_He1",
                            "~Yuxiang_Yang2",
                            "~Guanya_Shi1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "System Identification",
                            "Sim2Real",
                            "Legged Robots"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a two-stage Sampling-based System Identification framework with active exploration for data collection enabling high precision in several sim2real locomotion tasks in both quadruped and humanoid systems."
                    },
                    "abstract": {
                        "value": "Sim-to-real discrepancies hinder learning-based policies from achieving high-precision tasks in the real world. While Domain Randomization (DR) is commonly used to bridge this gap, it often relies on heuristics and can lead to overly conservative policies with degrading performance when not properly tuned. System Identification (Sys-ID) offers a targeted approach, but standard techniques rely on differentiable dynamics and/or direct torque measurement, assumptions that rarely hold for contact-rich legged systems. To this end, we present SPI-Active (Sampling-based Parameter Identification with Active Exploration), a two-stage framework that estimates physical parameters of legged robots to minimize the sim-to-real gap. SPI-Active robustly identifies key physical parameters through massive parallel sampling, minimizing state prediction errors between simulated and real-world trajectories. To further improve the informativeness of collected data, we introduce an active exploration strategy that maximizes the Fisher Information of the collected real-world trajectories via optimizing the input commands of an exploration policy. This targeted exploration leads to accurate identification and better generalization across diverse tasks. Experimental results demonstrate that SPI-Active enables precise sim-to-real transfer of learned policies to the real world, outperforming baselines by 42-63% in various locomotion tasks. Videos at the anonymous website https://anonymous-spi-active.github.io/"
                    },
                    "supplementary_material": {
                        "value": "/attachment/bbd53fb6c30e4cc4a64747f6d75f07da0cc9b379.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/e5927f067ebfe16f62864c4e2bf3027de9f9cdcc.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsobanbabu2025samplingbased,\ntitle={Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning},\nauthor={Nikhil Sobanbabu and Guanqi He and Tairan He and Yuxiang Yang and Guanya Shi},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=UTPBM4dEUS}\n}"
                    },
                    "paperhash": {
                        "value": "sobanbabu|samplingbased_system_identification_with_active_exploration_for_legged_sim2real_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745472973445,
                "pdate": 1754680642492,
                "odate": 1758062787505,
                "mdate": 1758062826810,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1020/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Tl7girqoLi",
        "title": "DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration",
        "abstract": "Imitation learning has shown great promise in robotic manipulation, but the policy’s execution is often unsatisfactorily slow due to commonly tardy demonstrations collected by human operators. In this work, we present DemoSpeedup, a self-supervised method to accelerate visuomotor policy execution via entropy-guided demonstration acceleration. DemoSpeedup starts from training an arbitrary generative policy (e.g., ACT or Diffusion Policy) on normal-speed demonstrations, which serves as a per-frame action entropy estimator. The key insight is that frames with lower action entropy estimates call for more consistent policy behaviors, which often indicate the demands for higher-precision operations. In contrast, frames with higher entropy estimates correspond to more casual sections, and therefore can be more safely accelerated. Thus, we segment the original demonstrations according to the estimated entropy, and accelerate them by down-sampling at rates that increase with the entropy values. Trained with the speedup demonstrations, the resulting policies execute up to 3 times faster while maintaining the task completion performance. Interestingly, these policies could even achieve higher success rates than those trained with normal-speed demonstrations, due to the benefits of reduced decision-making horizons.",
        "keywords": [
            "Imitation learning",
            "Manipulation",
            "Demonstration Acceleration"
        ],
        "pdf_url": "https://openreview.net/pdf/1204842e7202a6c742867cb051162923664807a7.pdf",
        "reviews": [
            {
                "id": "6DWFFlcd3X",
                "forum": "Tl7girqoLi",
                "replyto": "Tl7girqoLi",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321142,
                "mdate": 1754869458994,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Tl7girqoLi",
                "forum": "Tl7girqoLi",
                "content": {
                    "title": {
                        "value": "DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration"
                    },
                    "authors": {
                        "value": [
                            "Lingxiao Guo",
                            "Zhengrong Xue",
                            "Zijing Xu",
                            "Huazhe Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Lingxiao_Guo1",
                            "~Zhengrong_Xue1",
                            "~Zijing_Xu1",
                            "~Huazhe_Xu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation learning",
                            "Manipulation",
                            "Demonstration Acceleration"
                        ]
                    },
                    "TLDR": {
                        "value": "DemoSpeedup is a self-supervised method that accelerates visuomotor policy execution via entropy-guided demonstration acceleration."
                    },
                    "abstract": {
                        "value": "Imitation learning has shown great promise in robotic manipulation, but the policy’s execution is often unsatisfactorily slow due to commonly tardy demonstrations collected by human operators. In this work, we present DemoSpeedup, a self-supervised method to accelerate visuomotor policy execution via entropy-guided demonstration acceleration. DemoSpeedup starts from training an arbitrary generative policy (e.g., ACT or Diffusion Policy) on normal-speed demonstrations, which serves as a per-frame action entropy estimator. The key insight is that frames with lower action entropy estimates call for more consistent policy behaviors, which often indicate the demands for higher-precision operations. In contrast, frames with higher entropy estimates correspond to more casual sections, and therefore can be more safely accelerated. Thus, we segment the original demonstrations according to the estimated entropy, and accelerate them by down-sampling at rates that increase with the entropy values. Trained with the speedup demonstrations, the resulting policies execute up to 3 times faster while maintaining the task completion performance. Interestingly, these policies could even achieve higher success rates than those trained with normal-speed demonstrations, due to the benefits of reduced decision-making horizons."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7b140f63582349fcd24dcdef6566280f4ea650ea.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/1204842e7202a6c742867cb051162923664807a7.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nguo2025demospeedup,\ntitle={DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration},\nauthor={Lingxiao Guo and Zhengrong Xue and Zijing Xu and Huazhe Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Tl7girqoLi}\n}"
                    },
                    "paperhash": {
                        "value": "guo|demospeedup_accelerating_visuomotor_policies_via_entropyguided_demonstration_acceleration"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745471372752,
                "pdate": 1754680642179,
                "odate": 1758062787269,
                "mdate": 1758062826791,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "cpmwi3Xwcr",
        "title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies",
        "abstract": "Comprehensive, unbiased, and comparable evaluation of modern generalist policies is uniquely challenging: existing approaches for robot benchmarking typically rely on heavy standardization, either by specifying fixed evaluation tasks and environments, or by hosting centralized \"robot challenges\", and do not readily scale to evaluating generalist policies across a broad range of tasks and environments. In this work, we propose RoboArena, a new approach for scalable evaluation of generalist robot policies in the real world. Instead of standardizing evaluations around fixed tasks, environments, or locations, we propose to crowd-source evaluations across a distributed network of evaluators. Importantly, evaluators can freely choose the tasks and environments they evaluate on, enabling easy scaling of diversity, but they are required to perform double-blind evaluations over pairs of policies. Then, by aggregating preference feedback from pairwise comparisons across diverse tasks and environments, we can derive a ranking of policies. We instantiate our approach across a network of evaluators at seven academic institutions using the DROID robot platform. Through more than 600 pairwise real-robot evaluation episodes across seven generalist policies, we demonstrate that our crowd-sourced approach can more accurately rank the performance of existing generalist policies than conventional, centralized evaluation approaches, while being more scalable, resilient, and trustworthy. We open our evaluation network to the community and hope that it can enable more accessible comparisons of generalist robot policies.",
        "keywords": [
            "Generalist Robot Policy Evaluation",
            "Crowd-Sourced Evaluation",
            "Robot Foundation Models"
        ],
        "pdf_url": "https://openreview.net/pdf/a471fe13a7dbbcf87d41a18be32b45e80671853d.pdf",
        "reviews": [
            {
                "id": "XgiDqYIoUN",
                "forum": "cpmwi3Xwcr",
                "replyto": "cpmwi3Xwcr",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068321055,
                "mdate": 1754869458662,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "cpmwi3Xwcr",
                "forum": "cpmwi3Xwcr",
                "content": {
                    "title": {
                        "value": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies"
                    },
                    "authors": {
                        "value": [
                            "Pranav Atreya",
                            "Karl Pertsch",
                            "Tony Lee",
                            "Moo Jin Kim",
                            "Arhan Jain",
                            "Artur Kuramshin",
                            "Cyrus Neary",
                            "Edward S. Hu",
                            "Kanav Arora",
                            "Kirsty Ellis",
                            "Luca Macesanu",
                            "Matthew Leonard",
                            "Meedeum Cho",
                            "Ozgur Aslan",
                            "Shivin Dass",
                            "Tony Wang",
                            "Xingfang Yuan",
                            "Abhishek Gupta",
                            "Dinesh Jayaraman",
                            "Glen Berseth",
                            "Kostas Daniilidis",
                            "Roberto Martín-Martín",
                            "Youngwoon Lee",
                            "Percy Liang",
                            "Chelsea Finn",
                            "Sergey Levine"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Pranav_Atreya1",
                            "~Karl_Pertsch1",
                            "~Tony_Lee1",
                            "~Moo_Jin_Kim1",
                            "~Arhan_Jain1",
                            "~Artur_Kuramshin1",
                            "~Cyrus_Neary1",
                            "~Edward_S._Hu1",
                            "~Kanav_Arora1",
                            "kirsty.ellis@mila.quebec",
                            "~Luca_Macesanu1",
                            "~Matthew_Leonard1",
                            "~Meedeum_Cho1",
                            "~Ozgur_Aslan1",
                            "~Shivin_Dass2",
                            "tonyw3@seas.upenn.edu",
                            "xingfy@seas.upenn.edu",
                            "~Abhishek_Gupta1",
                            "~Dinesh_Jayaraman2",
                            "~Glen_Berseth1",
                            "~Kostas_Daniilidis1",
                            "~Roberto_Martín-Martín1",
                            "~Youngwoon_Lee1",
                            "~Percy_Liang1",
                            "~Chelsea_Finn1",
                            "~Sergey_Levine1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Generalist Robot Policy Evaluation",
                            "Crowd-Sourced Evaluation",
                            "Robot Foundation Models"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose RoboArena, a distributed real-world evaluation framework for generalist robot policies based on crowd-sourced, pairwise policy comparisons."
                    },
                    "abstract": {
                        "value": "Comprehensive, unbiased, and comparable evaluation of modern generalist policies is uniquely challenging: existing approaches for robot benchmarking typically rely on heavy standardization, either by specifying fixed evaluation tasks and environments, or by hosting centralized \"robot challenges\", and do not readily scale to evaluating generalist policies across a broad range of tasks and environments. In this work, we propose RoboArena, a new approach for scalable evaluation of generalist robot policies in the real world. Instead of standardizing evaluations around fixed tasks, environments, or locations, we propose to crowd-source evaluations across a distributed network of evaluators. Importantly, evaluators can freely choose the tasks and environments they evaluate on, enabling easy scaling of diversity, but they are required to perform double-blind evaluations over pairs of policies. Then, by aggregating preference feedback from pairwise comparisons across diverse tasks and environments, we can derive a ranking of policies. We instantiate our approach across a network of evaluators at seven academic institutions using the DROID robot platform. Through more than 600 pairwise real-robot evaluation episodes across seven generalist policies, we demonstrate that our crowd-sourced approach can more accurately rank the performance of existing generalist policies than conventional, centralized evaluation approaches, while being more scalable, resilient, and trustworthy. We open our evaluation network to the community and hope that it can enable more accessible comparisons of generalist robot policies."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/a471fe13a7dbbcf87d41a18be32b45e80671853d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\natreya2025roboarena,\ntitle={RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies},\nauthor={Pranav Atreya and Karl Pertsch and Tony Lee and Moo Jin Kim and Arhan Jain and Artur Kuramshin and Cyrus Neary and Edward S. Hu and Kanav Arora and Kirsty Ellis and Luca Macesanu and Matthew Leonard and Meedeum Cho and Ozgur Aslan and Shivin Dass and Tony Wang and Xingfang Yuan and Abhishek Gupta and Dinesh Jayaraman and Glen Berseth and Kostas Daniilidis and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n and Youngwoon Lee and Percy Liang and Chelsea Finn and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=cpmwi3Xwcr}\n}"
                    },
                    "paperhash": {
                        "value": "atreya|roboarena_distributed_realworld_evaluation_of_generalist_robot_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745471185434,
                "pdate": 1754680642094,
                "odate": 1758062787214,
                "mdate": 1758062826651,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "b24y5SENo5",
        "title": "Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation",
        "abstract": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation. Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication. Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses. We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to \"theory of mind\" in latent space. Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold. Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings. To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism. Execution can be fully distributed, requiring no explicit communication between policies. In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene. Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian. While we tested our method using two manipulators, our approach can naturally be extended to an arbitrary number of agents. In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation. Additionally, we show that LatentToM is naturally robust to temporary robot failure or delays, while a centralized policy may fail.",
        "keywords": [
            "Cooperative Manipulation",
            "Diffusion Policy",
            "Consensus Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/3425b50e6087fb05595f0f2b0f136bf21d8b5ecd.pdf",
        "reviews": [
            {
                "id": "sj7qINGK9n",
                "forum": "b24y5SENo5",
                "replyto": "b24y5SENo5",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070537898,
                "mdate": 1754869458478,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "b24y5SENo5",
                "forum": "b24y5SENo5",
                "content": {
                    "title": {
                        "value": "Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Chengyang He",
                            "Gadiel Mark Sznaier Camps",
                            "Xu Liu",
                            "Mac Schwager",
                            "Guillaume Adrien Sartoretti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chengyang_He1",
                            "~Gadiel_Mark_Sznaier_Camps1",
                            "~Xu_Liu11",
                            "~Mac_Schwager1",
                            "~Guillaume_Adrien_Sartoretti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Cooperative Manipulation",
                            "Diffusion Policy",
                            "Consensus Learning"
                        ]
                    },
                    "abstract": {
                        "value": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation. Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication. Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses. We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to \"theory of mind\" in latent space. Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold. Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings. To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism. Execution can be fully distributed, requiring no explicit communication between policies. In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene. Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian. While we tested our method using two manipulators, our approach can naturally be extended to an arbitrary number of agents. In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation. Additionally, we show that LatentToM is naturally robust to temporary robot failure or delays, while a centralized policy may fail."
                    },
                    "supplementary_material": {
                        "value": "/attachment/003c865b3604836b70f5520fddf86dd9cc07e8d7.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3425b50e6087fb05595f0f2b0f136bf21d8b5ecd.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhe2025latent,\ntitle={Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation},\nauthor={Chengyang He and Gadiel Mark Sznaier Camps and Xu Liu and Mac Schwager and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b24y5SENo5}\n}"
                    },
                    "paperhash": {
                        "value": "he|latent_theory_of_mind_a_decentralized_diffusion_architecture_for_cooperative_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745470826610,
                "pdate": 1754680641887,
                "odate": 1758062787077,
                "mdate": 1758062826541,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1007/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "rbMoMEK4m2",
        "title": "Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis",
        "abstract": "Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths.",
        "keywords": [
            "Trajectory Planning",
            "Imitation Learning",
            "Robustness Analysis",
            "Aerial Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/9e60c197f3c5ba3e4926c98e8478c3dca930b7d4.pdf",
        "reviews": [
            {
                "id": "KIVkZ0EzUt",
                "forum": "rbMoMEK4m2",
                "replyto": "rbMoMEK4m2",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070538363,
                "mdate": 1754869476930,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "rbMoMEK4m2",
                "forum": "rbMoMEK4m2",
                "content": {
                    "title": {
                        "value": "Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis"
                    },
                    "authors": {
                        "value": [
                            "Katherine Mao",
                            "Hongzhan Yu",
                            "Ruipeng Zhang",
                            "Igor Spasojevic",
                            "Sicun Gao",
                            "Vijay Kumar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Katherine_Mao1",
                            "~Hongzhan_Yu1",
                            "~Ruipeng_Zhang2",
                            "~Igor_Spasojevic1",
                            "~Sicun_Gao1",
                            "~Vijay_Kumar2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Trajectory Planning",
                            "Imitation Learning",
                            "Robustness Analysis",
                            "Aerial Robotics"
                        ]
                    },
                    "abstract": {
                        "value": "Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths."
                    },
                    "supplementary_material": {
                        "value": "/attachment/595fa27bfe4cb1d077a014d4c69e381cb6e0c720.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9e60c197f3c5ba3e4926c98e8478c3dca930b7d4.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmao2025sequence,\ntitle={Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis},\nauthor={Katherine Mao and Hongzhan Yu and Ruipeng Zhang and Igor Spasojevic and Sicun Gao and Vijay Kumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=rbMoMEK4m2}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/881debef4e914564dabde25c46deb55cf1c9f93b.mp4"
                    },
                    "paperhash": {
                        "value": "mao|sequence_modeling_for_timeoptimal_quadrotor_trajectory_optimization_with_samplingbased_robustness_analysis"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745470772401,
                "pdate": 1754680641834,
                "odate": 1758062787031,
                "mdate": 1758062826529,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission1006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "PMKwnV6Azi",
        "title": "HALO : Human Preference Aligned Offline Reward Learning for Robot Navigation",
        "abstract": "In this paper, we introduce HALO, a novel Offline Reward Learning algorithm that quantifies human intuition in navigation into a vision-based reward function for robot navigation. HALO learns a reward model from offline data, leveraging expert trajectories collected from mobile robots. During training, actions are randomly sampled from the action space around the expert action and ranked using a Boltzmann probability distribution that combines their distance to the expert action with human preference scores derived from intuitive navigation queries based on the corresponding egocentric camera feed. These scores establish preference rankings, enabling the training of a novel reward model based on Plackett-Luce loss, which allows for preference-driven navigation. To demonstrate the effectiveness of HALO, we deploy its reward model in two downstream applications: (i) an offline learned policy trained directly on the HALO-derived rewards, and (ii) a model-predictive-control (MPC) based planner that incorporates the HALO reward as an additional cost term. This showcases the versatility of HALO across both learning-based and classical navigation frameworks. Our real-world deployments on a Clearpath Husky across multiple scenarios demonstrate that policies trained with HALO achieve improved performance over state-of-the-art methods in terms of success rate and normalized trajectory length while maintaining lower Fréchet distance with the human expert trajectories.",
        "keywords": [
            "Reward Modelling",
            "Preference Alignment",
            "Vision-based Navigation"
        ],
        "pdf_url": "https://openreview.net/pdf/ef341c0c000543775dab9bc25b601b6c5e3701cb.pdf",
        "reviews": [
            {
                "id": "tM5ACO34MT",
                "forum": "PMKwnV6Azi",
                "replyto": "PMKwnV6Azi",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission997/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068320773,
                "mdate": 1754869476855,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "PMKwnV6Azi",
                "forum": "PMKwnV6Azi",
                "content": {
                    "title": {
                        "value": "HALO : Human Preference Aligned Offline Reward Learning for Robot Navigation"
                    },
                    "authors": {
                        "value": [
                            "Gershom Seneviratne",
                            "Jianyu An",
                            "Sahire Ellahy",
                            "Kasun Weerakoon",
                            "Mohamed Bashir Elnoor",
                            "Jonathan Deepak Kannan",
                            "Amogha Thalihalla Sunil",
                            "Dinesh Manocha"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Gershom_Seneviratne1",
                            "~Jianyu_An1",
                            "~Sahire_Ellahy1",
                            "~Kasun_Weerakoon1",
                            "~Mohamed_Bashir_Elnoor1",
                            "~Jonathan_Deepak_Kannan1",
                            "~Amogha_Thalihalla_Sunil1",
                            "~Dinesh_Manocha3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reward Modelling",
                            "Preference Alignment",
                            "Vision-based Navigation"
                        ]
                    },
                    "TLDR": {
                        "value": "HALO learns a vision-based reward model from offline human preferences, enabling vision based navigation that generalizes across varying scenarios and outperforms state of the art methods."
                    },
                    "abstract": {
                        "value": "In this paper, we introduce HALO, a novel Offline Reward Learning algorithm that quantifies human intuition in navigation into a vision-based reward function for robot navigation. HALO learns a reward model from offline data, leveraging expert trajectories collected from mobile robots. During training, actions are randomly sampled from the action space around the expert action and ranked using a Boltzmann probability distribution that combines their distance to the expert action with human preference scores derived from intuitive navigation queries based on the corresponding egocentric camera feed. These scores establish preference rankings, enabling the training of a novel reward model based on Plackett-Luce loss, which allows for preference-driven navigation. To demonstrate the effectiveness of HALO, we deploy its reward model in two downstream applications: (i) an offline learned policy trained directly on the HALO-derived rewards, and (ii) a model-predictive-control (MPC) based planner that incorporates the HALO reward as an additional cost term. This showcases the versatility of HALO across both learning-based and classical navigation frameworks. Our real-world deployments on a Clearpath Husky across multiple scenarios demonstrate that policies trained with HALO achieve improved performance over state-of-the-art methods in terms of success rate and normalized trajectory length while maintaining lower Fréchet distance with the human expert trajectories."
                    },
                    "supplementary_material": {
                        "value": "/attachment/39805ec99e50afe82d5a50fa67811f41fc9119bc.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/ef341c0c000543775dab9bc25b601b6c5e3701cb.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nseneviratne2025halo,\ntitle={{HALO} : Human Preference Aligned Offline Reward Learning for Robot Navigation},\nauthor={Gershom Seneviratne and Jianyu An and Sahire Ellahy and Kasun Weerakoon and Mohamed Bashir Elnoor and Jonathan Deepak Kannan and Amogha Thalihalla Sunil and Dinesh Manocha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PMKwnV6Azi}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/825d728a0d4834ec89e08bc6be88dbb7a2e8e885.mp4"
                    },
                    "paperhash": {
                        "value": "seneviratne|halo_human_preference_aligned_offline_reward_learning_for_robot_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission997/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission997/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission997/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745469789760,
                "pdate": 1754680641539,
                "odate": 1758062786820,
                "mdate": 1758062826383,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission997/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "kUA2ec94LI",
        "title": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control",
        "abstract": "Efficient robot locomotion often requires balancing task performance with energy expenditure. A common approach in reinforcement learning (RL) is to penalize energy use directly in the reward function. This requires carefully weighting the reward terms to avoid undesirable trade-offs where energy minimization harms task success or vice versa. In this work, we propose a hyperparameter-free gradient optimization method to minimize energy without conflicting with task performance. Inspired by recent works in multitask learning, our method applies policy gradient projection between task and energy objectives to promote non-conflicting updates. We evaluate this technique on standard locomotion benchmarks of DM-Control and HumanoidBench and demonstrate a reduction of $64$% energy usage while maintaining comparable task performance. Further, we conduct experiments on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient policies. Our method is easy to implement in standard RL pipelines with minimal code changes, and offers a principled alternative to reward shaping for energy efficient control policies.",
        "keywords": [
            "Energy efficient locomotion",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/96b9bd4f71dd310b10190b93b28b7066a7c89a25.pdf",
        "reviews": [
            {
                "id": "l2OZlFueEY",
                "forum": "kUA2ec94LI",
                "replyto": "kUA2ec94LI",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission995/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068320700,
                "mdate": 1754869458319,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "kUA2ec94LI",
                "forum": "kUA2ec94LI",
                "content": {
                    "title": {
                        "value": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control"
                    },
                    "authors": {
                        "value": [
                            "Skand Peri",
                            "Akhil Perincherry",
                            "Bikram Pandit",
                            "Stefan Lee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Skand_Peri1",
                            "~Akhil_Perincherry1",
                            "~Bikram_Pandit1",
                            "~Stefan_Lee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Energy efficient locomotion",
                            "Reinforcement Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a simple, hyperparameter-free gradient optimization methods that minimizes energy while maintaining task performance for locomotion."
                    },
                    "abstract": {
                        "value": "Efficient robot locomotion often requires balancing task performance with energy expenditure. A common approach in reinforcement learning (RL) is to penalize energy use directly in the reward function. This requires carefully weighting the reward terms to avoid undesirable trade-offs where energy minimization harms task success or vice versa. In this work, we propose a hyperparameter-free gradient optimization method to minimize energy without conflicting with task performance. Inspired by recent works in multitask learning, our method applies policy gradient projection between task and energy objectives to promote non-conflicting updates. We evaluate this technique on standard locomotion benchmarks of DM-Control and HumanoidBench and demonstrate a reduction of $64$% energy usage while maintaining comparable task performance. Further, we conduct experiments on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient policies. Our method is easy to implement in standard RL pipelines with minimal code changes, and offers a principled alternative to reward shaping for energy efficient control policies."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/96b9bd4f71dd310b10190b93b28b7066a7c89a25.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nperi2025nonconflicting,\ntitle={Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control},\nauthor={Skand Peri and Akhil Perincherry and Bikram Pandit and Stefan Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kUA2ec94LI}\n}"
                    },
                    "paperhash": {
                        "value": "peri|nonconflicting_energy_minimization_in_reinforcement_learning_based_robot_control"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission995/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission995/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission995/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745469759878,
                "pdate": 1754680641466,
                "odate": 1758062786811,
                "mdate": 1758062826304,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission995/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission995/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "QPTZfaxATs",
        "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models, pre-trained on large-scale imitation learning datasets, have demonstrated remarkable capabilities in visuomotor control. However, these models exhibit diverse failure modes in unstructured real-world environments, limiting the widespread adoption of VLAs in robotics. Efforts to enhance the robustness and generalization of VLAs have gradually shifted from the pre-training to the post-training phase. Yet, the potential of scaling test-time compute remains underexplored. In this paper, we investigate test-time scaling for robotics through the lens of sampling and verification. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on this insight, we propose a synthetic data generation pipeline for training a Vision-Language Model (VLM)-based action verifier, and show that scaling the synthetic dataset consistently improves verification and downstream accuracy. We then introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbations and majority voting to construct an action proposal distribution, and then uses the VLM-based verifier to select the optimal action. Through extensive evaluations across simulated and real-world environments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25\\% absolute improvement on out-of-distribution tasks and 8\\% higher average success rate on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7\\% performance increase compared to fine-tuning VLAs alone.",
        "keywords": [
            "Vision-Language-Action Models",
            "Test-Time Scaling",
            "Reward Learning",
            "Imitation Learning",
            "Generalist Policies",
            "Visuomotor Control"
        ],
        "pdf_url": "https://openreview.net/pdf/c758ae4f652eb5754995c86eecbef5f5666a9432.pdf",
        "reviews": [
            {
                "id": "0KclANzFri",
                "forum": "QPTZfaxATs",
                "replyto": "QPTZfaxATs",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission986/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068320375,
                "mdate": 1754869476697,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "QPTZfaxATs",
                "forum": "QPTZfaxATs",
                "content": {
                    "title": {
                        "value": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models"
                    },
                    "authors": {
                        "value": [
                            "Jacky Kwok",
                            "Christopher Agia",
                            "Rohan Sinha",
                            "Matt Foutter",
                            "Shulu Li",
                            "Ion Stoica",
                            "Azalia Mirhoseini",
                            "Marco Pavone"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jacky_Kwok2",
                            "~Christopher_Agia1",
                            "~Rohan_Sinha1",
                            "~Matt_Foutter1",
                            "~Shulu_Li1",
                            "~Ion_Stoica1",
                            "~Azalia_Mirhoseini3",
                            "~Marco_Pavone1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action Models",
                            "Test-Time Scaling",
                            "Reward Learning",
                            "Imitation Learning",
                            "Generalist Policies",
                            "Visuomotor Control"
                        ]
                    },
                    "abstract": {
                        "value": "Vision-Language-Action (VLA) models, pre-trained on large-scale imitation learning datasets, have demonstrated remarkable capabilities in visuomotor control. However, these models exhibit diverse failure modes in unstructured real-world environments, limiting the widespread adoption of VLAs in robotics. Efforts to enhance the robustness and generalization of VLAs have gradually shifted from the pre-training to the post-training phase. Yet, the potential of scaling test-time compute remains underexplored. In this paper, we investigate test-time scaling for robotics through the lens of sampling and verification. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on this insight, we propose a synthetic data generation pipeline for training a Vision-Language Model (VLM)-based action verifier, and show that scaling the synthetic dataset consistently improves verification and downstream accuracy. We then introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbations and majority voting to construct an action proposal distribution, and then uses the VLM-based verifier to select the optimal action. Through extensive evaluations across simulated and real-world environments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25\\% absolute improvement on out-of-distribution tasks and 8\\% higher average success rate on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7\\% performance increase compared to fine-tuning VLAs alone."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c758ae4f652eb5754995c86eecbef5f5666a9432.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkwok2025robomonkey,\ntitle={RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models},\nauthor={Jacky Kwok and Christopher Agia and Rohan Sinha and Matt Foutter and Shulu Li and Ion Stoica and Azalia Mirhoseini and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=QPTZfaxATs}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1c59961b3a6a112d2526fa54dc37a1dd16985a18.mp4"
                    },
                    "paperhash": {
                        "value": "kwok|robomonkey_scaling_testtime_sampling_and_verification_for_visionlanguageaction_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission986/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission986/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission986/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745468608245,
                "pdate": 1754680641261,
                "odate": 1758062786549,
                "mdate": 1758062826177,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission986/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission986/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "t2asRJv2SD",
        "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing",
        "abstract": "Robots equipped with rich sensor suites can localize reliably in partially-observable environments---but powering every sensor continuously is wasteful and often infeasible. Belief-space planners address this by propagating pose-belief covariance through analytic models and switching sensors heuristically--a brittle, runtime expensive approach. Data-driven approaches--including diffusion models--learn multi-modal trajectories from demonstrations, but presuppose an accurate, always-on state estimate. We address the largely open problem: for a given task in a  mapped environment, which *minimal sensor subset* must be active at each location to maintain state uncertainty *just low enough* to complete the task? Our key insight is that when a diffusion planner is explicitly conditioned on a pose-belief raster and a sensor mask, the spread of its denoising trajectories yields a calibrated, differentiable proxy for the expected localization error. Building on this insight, we present Belief-Conditioned One-Step Diffusion (B-COD), the first planner that, in a 10 ms forward pass, returns a short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for localization error--eliminating external covariance rollouts. We show that this single proxy suffices for a soft-actor–critic to choose sensors online, optimising energy while bounding pose-covariance growth. We deploy B-COD in real-time marine trials on an unmanned surface vehicle and show that it reduces sensing energy consumption while matching the goal-reach performance of an always-on baseline.",
        "keywords": [
            "Navigation",
            "Planning Under Uncertainty",
            "Multi-Modal Sensing"
        ],
        "pdf_url": "https://openreview.net/pdf/2b0fc040d2c0c4d8b4aa483bc42b87bfe1e11317.pdf",
        "reviews": [
            {
                "id": "QLeRKX86KY",
                "forum": "t2asRJv2SD",
                "replyto": "t2asRJv2SD",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission976/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070537658,
                "mdate": 1754869458237,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "t2asRJv2SD",
                "forum": "t2asRJv2SD",
                "content": {
                    "title": {
                        "value": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing"
                    },
                    "authors": {
                        "value": [
                            "Gokul Puthumanaillam",
                            "Aditya Penumarti",
                            "Manav Vora",
                            "Paulo Padrao",
                            "Jose Fuentes",
                            "Leonardo Bobadilla",
                            "Jane Shin",
                            "Melkior Ornik"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Gokul_Puthumanaillam1",
                            "~Aditya_Penumarti1",
                            "~Manav_Vora1",
                            "~Paulo_Padrao1",
                            "~Jose_Fuentes2",
                            "~Leonardo_Bobadilla1",
                            "~Jane_Shin1",
                            "~Melkior_Ornik1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Navigation",
                            "Planning Under Uncertainty",
                            "Multi-Modal Sensing"
                        ]
                    },
                    "TLDR": {
                        "value": "A belief-conditioned one-step diffusion planner outputs short-horizon motions and a calibrated risk scalar; a lightweight SAC uses this signal to toggle just the needed sensors, saving energy while matching always-on success."
                    },
                    "abstract": {
                        "value": "Robots equipped with rich sensor suites can localize reliably in partially-observable environments---but powering every sensor continuously is wasteful and often infeasible. Belief-space planners address this by propagating pose-belief covariance through analytic models and switching sensors heuristically--a brittle, runtime expensive approach. Data-driven approaches--including diffusion models--learn multi-modal trajectories from demonstrations, but presuppose an accurate, always-on state estimate. We address the largely open problem: for a given task in a  mapped environment, which *minimal sensor subset* must be active at each location to maintain state uncertainty *just low enough* to complete the task? Our key insight is that when a diffusion planner is explicitly conditioned on a pose-belief raster and a sensor mask, the spread of its denoising trajectories yields a calibrated, differentiable proxy for the expected localization error. Building on this insight, we present Belief-Conditioned One-Step Diffusion (B-COD), the first planner that, in a 10 ms forward pass, returns a short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for localization error--eliminating external covariance rollouts. We show that this single proxy suffices for a soft-actor–critic to choose sensors online, optimising energy while bounding pose-covariance growth. We deploy B-COD in real-time marine trials on an unmanned surface vehicle and show that it reduces sensing energy consumption while matching the goal-reach performance of an always-on baseline."
                    },
                    "supplementary_material": {
                        "value": "/attachment/57b498e54dd7685f62cc9c0c623c51aba6eadf56.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2b0fc040d2c0c4d8b4aa483bc42b87bfe1e11317.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nputhumanaillam2025beliefconditioned,\ntitle={Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing},\nauthor={Gokul Puthumanaillam and Aditya Penumarti and Manav Vora and Paulo Padrao and Jose Fuentes and Leonardo Bobadilla and Jane Shin and Melkior Ornik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=t2asRJv2SD}\n}"
                    },
                    "paperhash": {
                        "value": "puthumanaillam|beliefconditioned_onestep_diffusion_realtime_trajectory_planning_with_justenough_sensing"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission976/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission976/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission976/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745467331560,
                "pdate": 1754680640953,
                "odate": 1758062786244,
                "mdate": 1758062826139,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission976/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "KSKzA1mwKs",
        "title": "Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization",
        "abstract": "Large-scale demonstration data has powered key breakthroughs in robot manipulation, but collecting that data remains costly and time-consuming. To this end, we present Constraint-Preserving Data Generation (CP-Gen), a method that uses a single expert trajectory to generate robot demonstrations containing novel object geometries and poses. These generated demonstrations are used to train closed-loop visuomotor policies that transfer zero-shot to the real world. Similar to prior data-generation work focused on pose variations, CP-Gen first decomposes expert demonstrations into free-space motions and robot skills. Unlike prior work, we achieve geometry-aware data generation by formulating robot skills as keypoint-trajectory constraints: keypoints on the robot or grasped object must track a reference trajectory defined relative to a task-relevant object. To generate a new demonstration, CP-Gen samples pose and geometry transforms for each task-relevant object, then applies these transforms to the object and its associated keypoints or keypoint trajectories. We optimize robot joint configurations so that the keypoints on the robot or grasped object track the transformed keypoint trajectory, and then motion plan a collision-free path to the first optimized joint configuration. Using demonstrations generated by CP-Gen, we train visuomotor policies that generalize across variations in object geometries and poses. Experiments on 16 simulation tasks and four real-world tasks, featuring multi-stage, non-prehensile and tight-tolerance manipulation, show that policies trained using our method achieve an average success rate of 77%, outperforming the best baseline which achieves an average success rate of 50\\%.",
        "keywords": [
            "Imitation Learning",
            "Data Generation",
            "Robot Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/c9f99244c031d111461a7e2359cc06277141547d.pdf",
        "reviews": [
            {
                "id": "N8FyQ1tyh9",
                "forum": "KSKzA1mwKs",
                "replyto": "KSKzA1mwKs",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission975/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068319577,
                "mdate": 1754869476635,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "KSKzA1mwKs",
                "forum": "KSKzA1mwKs",
                "content": {
                    "title": {
                        "value": "Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization"
                    },
                    "authors": {
                        "value": [
                            "Kevin Lin",
                            "Varun Ragunath",
                            "Andrew McAlinden",
                            "Aaditya Prasad",
                            "Jimmy Wu",
                            "Yuke Zhu",
                            "Jeannette Bohg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kevin_Lin6",
                            "varun.ragunath@utexas.edu",
                            "andrewmca@utexas.edu",
                            "~Aaditya_Prasad2",
                            "~Jimmy_Wu1",
                            "~Yuke_Zhu1",
                            "~Jeannette_Bohg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Data Generation",
                            "Robot Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "CP-Gen generates demonstrations by preserving keypoint-trajectory constraints to enable visuomotor policy generalization to novel object geometries and poses."
                    },
                    "abstract": {
                        "value": "Large-scale demonstration data has powered key breakthroughs in robot manipulation, but collecting that data remains costly and time-consuming. To this end, we present Constraint-Preserving Data Generation (CP-Gen), a method that uses a single expert trajectory to generate robot demonstrations containing novel object geometries and poses. These generated demonstrations are used to train closed-loop visuomotor policies that transfer zero-shot to the real world. Similar to prior data-generation work focused on pose variations, CP-Gen first decomposes expert demonstrations into free-space motions and robot skills. Unlike prior work, we achieve geometry-aware data generation by formulating robot skills as keypoint-trajectory constraints: keypoints on the robot or grasped object must track a reference trajectory defined relative to a task-relevant object. To generate a new demonstration, CP-Gen samples pose and geometry transforms for each task-relevant object, then applies these transforms to the object and its associated keypoints or keypoint trajectories. We optimize robot joint configurations so that the keypoints on the robot or grasped object track the transformed keypoint trajectory, and then motion plan a collision-free path to the first optimized joint configuration. Using demonstrations generated by CP-Gen, we train visuomotor policies that generalize across variations in object geometries and poses. Experiments on 16 simulation tasks and four real-world tasks, featuring multi-stage, non-prehensile and tight-tolerance manipulation, show that policies trained using our method achieve an average success rate of 77%, outperforming the best baseline which achieves an average success rate of 50\\%."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f450cedb83b5e3222428d5f7134cb5465e438654.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c9f99244c031d111461a7e2359cc06277141547d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlin2025constraintpreserving,\ntitle={Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization},\nauthor={Kevin Lin and Varun Ragunath and Andrew McAlinden and Aaditya Prasad and Jimmy Wu and Yuke Zhu and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KSKzA1mwKs}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/bd4cc3242b7752879fc2d158dcbea4c831cd5cd4.mp4"
                    },
                    "paperhash": {
                        "value": "lin|constraintpreserving_data_generation_for_oneshot_visuomotor_policy_generalization"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission975/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission975/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission975/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745467308590,
                "pdate": 1754680640900,
                "odate": 1758062786242,
                "mdate": 1758062826070,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission975/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission975/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "H0EgeP3feg",
        "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching",
        "abstract": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.",
        "keywords": [
            "Learn from Human Data",
            "Humanoid; Whole-Body Control"
        ],
        "pdf_url": "https://openreview.net/pdf/02fded37b8fd3d18d5599fa82ed7b76e55f76537.pdf",
        "reviews": [
            {
                "id": "ThlFUhqfW5",
                "forum": "H0EgeP3feg",
                "replyto": "H0EgeP3feg",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission967/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068317064,
                "mdate": 1754869476572,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "H0EgeP3feg",
                "forum": "H0EgeP3feg",
                "content": {
                    "title": {
                        "value": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching"
                    },
                    "authors": {
                        "value": [
                            "Sirui Chen",
                            "Yufei Ye",
                            "Zi-ang Cao",
                            "Pei Xu",
                            "Jennifer Lew",
                            "Karen Liu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sirui_Chen1",
                            "~Yufei_Ye1",
                            "~Zi-ang_Cao1",
                            "~Pei_Xu1",
                            "~Jennifer_Lew1",
                            "~Karen_Liu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learn from Human Data",
                            "Humanoid; Whole-Body Control"
                        ]
                    },
                    "TLDR": {
                        "value": "We introduce HEAD, a framework that enables humanoids to navigate, locomote, and reach in human environments by learning from a combination of motion capture and AR-glasses human data via a 3-point tracking interface."
                    },
                    "abstract": {
                        "value": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans."
                    },
                    "supplementary_material": {
                        "value": "/attachment/4774d1a2494615e593c011ad04d63eeb637f3e3b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/02fded37b8fd3d18d5599fa82ed7b76e55f76537.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchen2025handeye,\ntitle={Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching},\nauthor={Sirui Chen and Yufei Ye and Zi-ang Cao and Pei Xu and Jennifer Lew and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=H0EgeP3feg}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/edeb7b53f63ba88963cc86fffd4c4dff233571fe.mp4"
                    },
                    "paperhash": {
                        "value": "chen|handeye_autonomous_delivery_learning_humanoid_navigation_locomotion_and_reaching"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission967/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission967/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission967/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745466498062,
                "pdate": 1754680640767,
                "odate": 1758062786052,
                "mdate": 1758062825939,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission967/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission967/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "oGcC8nMOit",
        "title": "Cross-Sensor Touch Generation",
        "abstract": "Today's visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as cup stacking and tool insertion, where models originally designed for one sensor are successfully transferred to another using in-hand pose estimation.",
        "keywords": [
            "Tactile Sensing",
            "Cross-Modal Generation",
            "Manipulation",
            "Representation Learning",
            "Soft Bubbles",
            "GelSlims",
            "Digits"
        ],
        "pdf_url": "https://openreview.net/pdf/3731c937aecdfb6abe54ed993d95fd5f0d5ee880.pdf",
        "reviews": [
            {
                "id": "VqLxsCZy0F",
                "forum": "oGcC8nMOit",
                "replyto": "oGcC8nMOit",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission957/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068311166,
                "mdate": 1754869458203,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "oGcC8nMOit",
                "forum": "oGcC8nMOit",
                "content": {
                    "title": {
                        "value": "Cross-Sensor Touch Generation"
                    },
                    "authors": {
                        "value": [
                            "Samanta Rodriguez",
                            "Yiming Dou",
                            "Miquel Oller",
                            "Andrew Owens",
                            "Nima Fazeli"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Samanta_Rodriguez1",
                            "~Yiming_Dou1",
                            "~Miquel_Oller1",
                            "~Andrew_Owens1",
                            "~Nima_Fazeli1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Sensing",
                            "Cross-Modal Generation",
                            "Manipulation",
                            "Representation Learning",
                            "Soft Bubbles",
                            "GelSlims",
                            "Digits"
                        ]
                    },
                    "TLDR": {
                        "value": "We present Touch2Touch and T2D2, two generative models for cross-modal tactile image generation that enable sensor-to-sensor transfer with and without paired data."
                    },
                    "abstract": {
                        "value": "Today's visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as cup stacking and tool insertion, where models originally designed for one sensor are successfully transferred to another using in-hand pose estimation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2db9b95fb609689b26616a35566d5d9131e967e4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3731c937aecdfb6abe54ed993d95fd5f0d5ee880.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nrodriguez2025crosssensor,\ntitle={Cross-Sensor Touch Generation},\nauthor={Samanta Rodriguez and Yiming Dou and Miquel Oller and Andrew Owens and Nima Fazeli},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oGcC8nMOit}\n}"
                    },
                    "paperhash": {
                        "value": "rodriguez|crosssensor_touch_generation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission957/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission957/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission957/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745465180059,
                "pdate": 1754680640306,
                "odate": 1758062785741,
                "mdate": 1758062825895,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission957/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission957/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HXJ6pUSn1L",
        "title": "FLARE: Robot Learning with Implicit World Modeling",
        "abstract": "We introduce **F**uture **LA**tent **R**presentation Alignm**E**nt (**FLARE**), a novel framework that integrates predictive world modeling into robot policy learning.\nBy aligning features from a diffusion transformer with latent embeddings of future observations, **FLARE** enables a diffusion transformer policy to anticipate latent representations of future observations, allowing it to reason about long-term consequences while generating actions.\nRemarkably lightweight, **FLARE** requires only minimal architectural modifications---adding a few tokens to standard vision-language-action (VLA) models---yet delivers substantial performance gains.\nAcross two challenging multitask simulation imitation learning benchmarks spanning single-arm and humanoid tabletop manipulation, **FLARE** achieves state-of-the-art performance, outperforming prior policy learning baselines by up to 26\\%.\nMoreover, **FLARE** unlocks the ability to co-train with human egocentric video demonstrations lacking action labels, significantly boosting policy generalization to a novel object with unseen geometry with as few as 1 robot demonstration.\nOur results establish **FLARE** as a general and scalable approach for combining implicit world modeling with high-frequency robotic control.",
        "keywords": [
            "World Model",
            "VLA",
            "Humanoid Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/6d70df0c6e41ac412a5526ea8234c59656ae1f27.pdf",
        "reviews": [
            {
                "id": "prA6EX8Sbs",
                "forum": "HXJ6pUSn1L",
                "replyto": "HXJ6pUSn1L",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission951/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068311108,
                "mdate": 1754869476437,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HXJ6pUSn1L",
                "forum": "HXJ6pUSn1L",
                "content": {
                    "title": {
                        "value": "FLARE: Robot Learning with Implicit World Modeling"
                    },
                    "authors": {
                        "value": [
                            "Ruijie Zheng",
                            "Jing Wang",
                            "Scott Reed",
                            "Johan Bjorck",
                            "Yu Fang",
                            "Fengyuan Hu",
                            "Joel Jang",
                            "Kaushil Kundalia",
                            "Zongyu Lin",
                            "Loïc Magne",
                            "Avnish Narayan",
                            "You Liang Tan",
                            "Guanzhi Wang",
                            "Qi Wang",
                            "Jiannan Xiang",
                            "Yinzhen Xu",
                            "Seonghyeon Ye",
                            "Jan Kautz",
                            "Furong Huang",
                            "Yuke Zhu",
                            "Linxi Fan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ruijie_Zheng1",
                            "~Jing_Wang79",
                            "~Scott_Reed1",
                            "jbjorck@nvidia.com",
                            "~Yu_Fang2",
                            "~Fengyuan_Hu2",
                            "~Joel_Jang1",
                            "~Kaushil_Kundalia1",
                            "~Zongyu_Lin1",
                            "~Loïc_Magne1",
                            "~Avnish_Narayan1",
                            "~You_Liang_Tan1",
                            "~Guanzhi_Wang1",
                            "qiwang@nvidia.com",
                            "~Jiannan_Xiang1",
                            "~Yinzhen_Xu1",
                            "~Seonghyeon_Ye1",
                            "~Jan_Kautz1",
                            "~Furong_Huang1",
                            "~Yuke_Zhu1",
                            "~Linxi_Fan2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "World Model",
                            "VLA",
                            "Humanoid Robotics"
                        ]
                    },
                    "abstract": {
                        "value": "We introduce **F**uture **LA**tent **R**presentation Alignm**E**nt (**FLARE**), a novel framework that integrates predictive world modeling into robot policy learning.\nBy aligning features from a diffusion transformer with latent embeddings of future observations, **FLARE** enables a diffusion transformer policy to anticipate latent representations of future observations, allowing it to reason about long-term consequences while generating actions.\nRemarkably lightweight, **FLARE** requires only minimal architectural modifications---adding a few tokens to standard vision-language-action (VLA) models---yet delivers substantial performance gains.\nAcross two challenging multitask simulation imitation learning benchmarks spanning single-arm and humanoid tabletop manipulation, **FLARE** achieves state-of-the-art performance, outperforming prior policy learning baselines by up to 26\\%.\nMoreover, **FLARE** unlocks the ability to co-train with human egocentric video demonstrations lacking action labels, significantly boosting policy generalization to a novel object with unseen geometry with as few as 1 robot demonstration.\nOur results establish **FLARE** as a general and scalable approach for combining implicit world modeling with high-frequency robotic control."
                    },
                    "supplementary_material": {
                        "value": "/attachment/fab99d56793ab121fe588f2bafcb5a667fe10e0b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We propose FLARE, a conceptually simple and lightweight framework for joint robot policy learning and latent world modeling."
                    },
                    "pdf": {
                        "value": "/pdf/6d70df0c6e41ac412a5526ea8234c59656ae1f27.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzheng2025flare,\ntitle={{FLARE}: Robot Learning with Implicit World Modeling},\nauthor={Ruijie Zheng and Jing Wang and Scott Reed and Johan Bjorck and Yu Fang and Fengyuan Hu and Joel Jang and Kaushil Kundalia and Zongyu Lin and Lo{\\\"\\i}c Magne and Avnish Narayan and You Liang Tan and Guanzhi Wang and Qi Wang and Jiannan Xiang and Yinzhen Xu and Seonghyeon Ye and Jan Kautz and Furong Huang and Yuke Zhu and Linxi Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HXJ6pUSn1L}\n}"
                    },
                    "paperhash": {
                        "value": "zheng|flare_robot_learning_with_implicit_world_modeling"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission951/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission951/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745464576422,
                "pdate": 1754680640150,
                "odate": 1758062785533,
                "mdate": 1758062785614,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission951/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "BO7qo66YJ2",
        "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
        "abstract": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30\\% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection, and (3) generalizes to new camera viewpoints and test-time changes.",
        "keywords": [
            "Learning from Human Videos",
            "Sim-to-Real",
            "Representation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/f5b46886b9dcf738033ced83da58f27b6be6d2d6.pdf",
        "reviews": [
            {
                "id": "PptgQ7Cu99",
                "forum": "BO7qo66YJ2",
                "replyto": "BO7qo66YJ2",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission947/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310968,
                "mdate": 1754869458084,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "BO7qo66YJ2",
                "forum": "BO7qo66YJ2",
                "content": {
                    "title": {
                        "value": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real"
                    },
                    "authors": {
                        "value": [
                            "Prithwish Dan",
                            "Kushal Kedia",
                            "Angela Chao",
                            "Edward Duan",
                            "Maximus Adrian Pace",
                            "Wei-Chiu Ma",
                            "Sanjiban Choudhury"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Prithwish_Dan1",
                            "~Kushal_Kedia1",
                            "~Angela_Chao1",
                            "~Edward_Duan1",
                            "~Maximus_Adrian_Pace1",
                            "~Wei-Chiu_Ma1",
                            "~Sanjiban_Choudhury3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learning from Human Videos",
                            "Sim-to-Real",
                            "Representation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30\\% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection, and (3) generalizes to new camera viewpoints and test-time changes."
                    },
                    "supplementary_material": {
                        "value": "/attachment/657abd2e638eebca34701098844a3757b77ab236.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f5b46886b9dcf738033ced83da58f27b6be6d2d6.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndan2025xsim,\ntitle={X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real},\nauthor={Prithwish Dan and Kushal Kedia and Angela Chao and Edward Duan and Maximus Adrian Pace and Wei-Chiu Ma and Sanjiban Choudhury},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BO7qo66YJ2}\n}"
                    },
                    "paperhash": {
                        "value": "dan|xsim_crossembodiment_learning_via_realtosimtoreal"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission947/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission947/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission947/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745463752235,
                "pdate": 1754680640017,
                "odate": 1758062785470,
                "mdate": 1758062825842,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission947/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission947/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1cA6OYsfoJ",
        "title": "From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning",
        "abstract": "Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. \nWe propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into logic-based world models, facilitating efficient zero-shot generalization to significantly more complex tasks. Empirical results demonstrate that our approach achieves performance comparable to hand-crafted models, successfully scaling execution horizons and handling up to 18 times more objects than seen in training, providing the first autonomous framework for learning transferable symbolic abstractions from raw robot trajectories.",
        "keywords": [
            "Learnng symbolic abstractions",
            "Symbolic world model learning",
            "Learning for task and motion planning",
            "learning for planning"
        ],
        "pdf_url": "https://openreview.net/pdf/6c2a06682808cbe42b0b544251fd250d51019439.pdf",
        "reviews": [
            {
                "id": "bQxFr8iLCh",
                "forum": "1cA6OYsfoJ",
                "replyto": "1cA6OYsfoJ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission943/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310929,
                "mdate": 1754869476386,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1cA6OYsfoJ",
                "forum": "1cA6OYsfoJ",
                "content": {
                    "title": {
                        "value": "From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning"
                    },
                    "authors": {
                        "value": [
                            "Naman Shah",
                            "Jayesh Nagpal",
                            "Siddharth Srivastava"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Naman_Shah1",
                            "~Jayesh_Nagpal1",
                            "~Siddharth_Srivastava2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learnng symbolic abstractions",
                            "Symbolic world model learning",
                            "Learning for task and motion planning",
                            "learning for planning"
                        ]
                    },
                    "TLDR": {
                        "value": "We present the first known approach for learning generalizable and transferrable symbolic representations from unsegmented and unlabeled raw tracjtories."
                    },
                    "abstract": {
                        "value": "Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. \nWe propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into logic-based world models, facilitating efficient zero-shot generalization to significantly more complex tasks. Empirical results demonstrate that our approach achieves performance comparable to hand-crafted models, successfully scaling execution horizons and handling up to 18 times more objects than seen in training, providing the first autonomous framework for learning transferable symbolic abstractions from raw robot trajectories."
                    },
                    "supplementary_material": {
                        "value": "/attachment/62ee16d18189ce71d6a12147bffd728dc50d44e2.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6c2a06682808cbe42b0b544251fd250d51019439.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nshah2025from,\ntitle={From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning},\nauthor={Naman Shah and Jayesh Nagpal and Siddharth Srivastava},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1cA6OYsfoJ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ee38f44d603918c31eb98090e2b6e1077b38daa1.mp4"
                    },
                    "paperhash": {
                        "value": "shah|from_real_world_to_logic_and_back_learning_generalizable_relational_concepts_for_long_horizon_robot_planning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission943/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission943/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission943/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745463256124,
                "pdate": 1754680639990,
                "odate": 1758062785407,
                "mdate": 1759844583774,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission943/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission943/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "2xvxn3Hm3n",
        "title": "NeuralSVCD for Efficient Swept Volume Collision Detection",
        "abstract": "Robot manipulation in unstructured environments requires efficient and reliable Swept Volume Collision Detection (SVCD) for safe motion planning. Traditional discrete methods potentially miss collisions between these points, whereas SVCD continuously checks for collisions along the entire trajectory. Existing SVCD methods typically face a trade-off between efficiency and accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a novel neural encoder-decoder architecture tailored to overcome this trade-off. Our approach leverages shape locality and temporal locality through distributed geometric representations and temporal optimization. This enhances computational efficiency without sacrificing accuracy. Comprehensive experiments show that NeuralSVCD consistently outperforms existing state-of-the-art SVCD methods in terms of both collision detection accuracy and computational efficiency, demonstrating its robust applicability across diverse robotic manipulation scenarios. Code and videos are available at https://neuralsvcd.github.io/.",
        "keywords": [
            "Neural swept-volume collision detection",
            "Motion planning"
        ],
        "pdf_url": "https://openreview.net/pdf/bd4815450dfcb83ab5b83fc97fd2a8bebafaa066.pdf",
        "reviews": [
            {
                "id": "2nldGwUhF7",
                "forum": "2xvxn3Hm3n",
                "replyto": "2xvxn3Hm3n",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission938/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310771,
                "mdate": 1754869476279,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "2xvxn3Hm3n",
                "forum": "2xvxn3Hm3n",
                "content": {
                    "title": {
                        "value": "NeuralSVCD for Efficient Swept Volume Collision Detection"
                    },
                    "authors": {
                        "value": [
                            "Hojin Jung",
                            "Dongwon Son",
                            "Beomjoon Kim"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hojin_Jung1",
                            "~Dongwon_Son1",
                            "~Beomjoon_Kim2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Neural swept-volume collision detection",
                            "Motion planning"
                        ]
                    },
                    "abstract": {
                        "value": "Robot manipulation in unstructured environments requires efficient and reliable Swept Volume Collision Detection (SVCD) for safe motion planning. Traditional discrete methods potentially miss collisions between these points, whereas SVCD continuously checks for collisions along the entire trajectory. Existing SVCD methods typically face a trade-off between efficiency and accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a novel neural encoder-decoder architecture tailored to overcome this trade-off. Our approach leverages shape locality and temporal locality through distributed geometric representations and temporal optimization. This enhances computational efficiency without sacrificing accuracy. Comprehensive experiments show that NeuralSVCD consistently outperforms existing state-of-the-art SVCD methods in terms of both collision detection accuracy and computational efficiency, demonstrating its robust applicability across diverse robotic manipulation scenarios. Code and videos are available at https://neuralsvcd.github.io/."
                    },
                    "supplementary_material": {
                        "value": "/attachment/dba1ecb4cb05c6f9a74d297ed549563fd3c4b709.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/bd4815450dfcb83ab5b83fc97fd2a8bebafaa066.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njung2025neuralsvcd,\ntitle={Neural{SVCD} for Efficient Swept Volume Collision Detection},\nauthor={Hojin Jung and Dongwon Son and Beomjoon Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2xvxn3Hm3n}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f0aadacd6e3519ffcf7a62daa36b7869f4917575.mp4"
                    },
                    "paperhash": {
                        "value": "jung|neuralsvcd_for_efficient_swept_volume_collision_detection"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission938/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission938/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission938/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745462986887,
                "pdate": 1754680639878,
                "odate": 1758062785405,
                "mdate": 1758062825561,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission938/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission938/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "iQQy1BKlGv",
        "title": "MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention",
        "abstract": "Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce Maximum-Entropy Residual-Q Inverse Reinforcement Learning, designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention compared to other baselines.",
        "keywords": [
            "Interactive imitation learning",
            "Learning from human feedback",
            "Inverse reinforcement learning"
        ],
        "pdf_url": "https://openreview.net/pdf/7dcba4e258afa6a048ea01c53bf71e9705752847.pdf",
        "reviews": [
            {
                "id": "DeNPIfnYWy",
                "forum": "iQQy1BKlGv",
                "replyto": "iQQy1BKlGv",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission931/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310407,
                "mdate": 1754869476236,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "iQQy1BKlGv",
                "forum": "iQQy1BKlGv",
                "content": {
                    "title": {
                        "value": "MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention"
                    },
                    "authors": {
                        "value": [
                            "Yuxin Chen",
                            "Chen Tang",
                            "Jianglan Wei",
                            "Chenran Li",
                            "Thomas Tian",
                            "Xiang Zhang",
                            "Wei Zhan",
                            "Peter Stone",
                            "Masayoshi Tomizuka"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuxin_Chen7",
                            "~Chen_Tang2",
                            "~Jianglan_Wei1",
                            "~Chenran_Li1",
                            "~Thomas_Tian1",
                            "~Xiang_Zhang20",
                            "~Wei_Zhan2",
                            "~Peter_Stone1",
                            "~Masayoshi_Tomizuka2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Interactive imitation learning",
                            "Learning from human feedback",
                            "Inverse reinforcement learning"
                        ]
                    },
                    "abstract": {
                        "value": "Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce Maximum-Entropy Residual-Q Inverse Reinforcement Learning, designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention compared to other baselines."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ab1cb8c73e94f3aa417d7ea43fe03f1cdb363c9b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7dcba4e258afa6a048ea01c53bf71e9705752847.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchen2025mereq,\ntitle={{MER}eQ: Max-Ent Residual-Q Inverse {RL} for Sample-Efficient Alignment from Intervention},\nauthor={Yuxin Chen and Chen Tang and Jianglan Wei and Chenran Li and Thomas Tian and Xiang Zhang and Wei Zhan and Peter Stone and Masayoshi Tomizuka},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iQQy1BKlGv}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ddca4b39bb7b090bc4582eb46e2a7ebd100bd7b3.mp4"
                    },
                    "paperhash": {
                        "value": "chen|mereq_maxent_residualq_inverse_rl_for_sampleefficient_alignment_from_intervention"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission931/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission931/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission931/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745462325662,
                "pdate": 1754680639497,
                "odate": 1758062785154,
                "mdate": 1758062825569,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission931/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "wnWYoetLhC",
        "title": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning",
        "abstract": "While large-scale robot datasets have propelled recent progress in imitation learning, learning from smaller task specific datasets remains critical for deployment in new environments and unseen tasks. One such approach to few-shot imitation learning is retrieval-based imitation learning, which extracts relevant samples from large, widely available prior datasets to augment a limited demonstration dataset. To determine the relevant data from prior datasets, retrieval-based approaches most commonly calculate a prior data point's minimum distance to a point in the target dataset in latent space. While retrieval-based methods have shown success using this metric for data selection, we demonstrate its equivalence to the limit of a Gaussian kernel density (KDE) estimate of the target data distribution. This reveals two shortcomings of the retrieval rule used in prior work. First, it relies on high-variance nearest neighbor estimates that are susceptible to noise. Second, it does not account for the distribution of prior data when retrieving data. To address these issues, we introduce Importance Weighted Retrieval (IWR), which estimates importance weights, or the ratio between the target and prior data distributions for retrieval, using Gaussian KDEs. By considering the probability ratio, IWR overcomes the bias of previous selection rules, and by using reasonable modeling parameters, IWR effectively smooths estimates using all data points.  Across both simulation environments and real-world evaluations on the Bridge dataset we find that our method, IWR, consistently improves performance of existing retrieval-based methods, despite only requiring minor modifications.",
        "keywords": [
            "Few-shot imitation learning",
            "Retrieval",
            "Data selection"
        ],
        "pdf_url": "https://openreview.net/pdf/10de019e4cee5f7f9ef71903e696bf42906ca575.pdf",
        "reviews": [
            {
                "id": "oBa9EA7R4q",
                "forum": "wnWYoetLhC",
                "replyto": "wnWYoetLhC",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission929/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310401,
                "mdate": 1754869458107,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "wnWYoetLhC",
                "forum": "wnWYoetLhC",
                "content": {
                    "title": {
                        "value": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning"
                    },
                    "authors": {
                        "value": [
                            "Amber Xie",
                            "Rahul Chand",
                            "Dorsa Sadigh",
                            "Joey Hejna"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Amber_Xie1",
                            "~Rahul_Chand1",
                            "~Dorsa_Sadigh1",
                            "~Joey_Hejna1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Few-shot imitation learning",
                            "Retrieval",
                            "Data selection"
                        ]
                    },
                    "TLDR": {
                        "value": "Uses importance weights to retrieve data for few-shot imitation learning."
                    },
                    "abstract": {
                        "value": "While large-scale robot datasets have propelled recent progress in imitation learning, learning from smaller task specific datasets remains critical for deployment in new environments and unseen tasks. One such approach to few-shot imitation learning is retrieval-based imitation learning, which extracts relevant samples from large, widely available prior datasets to augment a limited demonstration dataset. To determine the relevant data from prior datasets, retrieval-based approaches most commonly calculate a prior data point's minimum distance to a point in the target dataset in latent space. While retrieval-based methods have shown success using this metric for data selection, we demonstrate its equivalence to the limit of a Gaussian kernel density (KDE) estimate of the target data distribution. This reveals two shortcomings of the retrieval rule used in prior work. First, it relies on high-variance nearest neighbor estimates that are susceptible to noise. Second, it does not account for the distribution of prior data when retrieving data. To address these issues, we introduce Importance Weighted Retrieval (IWR), which estimates importance weights, or the ratio between the target and prior data distributions for retrieval, using Gaussian KDEs. By considering the probability ratio, IWR overcomes the bias of previous selection rules, and by using reasonable modeling parameters, IWR effectively smooths estimates using all data points.  Across both simulation environments and real-world evaluations on the Bridge dataset we find that our method, IWR, consistently improves performance of existing retrieval-based methods, despite only requiring minor modifications."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/10de019e4cee5f7f9ef71903e696bf42906ca575.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxie2025data,\ntitle={Data Retrieval with Importance Weights for Few-Shot Imitation Learning},\nauthor={Amber Xie and Rahul Chand and Dorsa Sadigh and Joey Hejna},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=wnWYoetLhC}\n}"
                    },
                    "paperhash": {
                        "value": "xie|data_retrieval_with_importance_weights_for_fewshot_imitation_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission929/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission929/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission929/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745462235733,
                "pdate": 1754680639494,
                "odate": 1758062785094,
                "mdate": 1758062825314,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission929/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission929/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "E9t1ekt6W9",
        "title": "Joint Model-based Model-free Diffusion for Planning with Constraints",
        "abstract": "Model-free diffusion planners have shown great promise for robot motion planning, but practical robotic systems often require combining them with model-based optimization modules to enforce constraints, such as safety. Na\\\"ively integrating these modules presents compatibility challenges when diffusion's multi-modal outputs behave adversarially to optimization-based modules. To address this, we introduce Joint Model-based Model-free Diffusion (JM2D), a novel generative modeling framework. JM2D formulates module integration as a joint sampling problem to maximize compatibility via an interaction potential, without additional training. Using importance sampling, JM2D guides modules outputs based only on evaluations of the interaction potential, thus handling non-differentiable objectives commonly arising from non-convex optimization modules. We evaluate JM2D via application to aligning diffusion planners with safety modules on offline RL and robot manipulation. JM2D significantly improves task performance compared to conventional safety filters without sacrificing safety. Further, we show that conditional generation is a special case of JM2D and elucidate key design choices by comparing with SOTA gradient-based and projection-based diffusion planners. More details at: \\url{https://sites.google.com/view/joint-mbmf-diffusion}",
        "keywords": [
            "Diffusion Model",
            "Safety",
            "Constrained Generation"
        ],
        "pdf_url": "https://openreview.net/pdf/19ae6558e1e298cd718ff2ae626aca2ac74949d6.pdf",
        "reviews": [
            {
                "id": "86NVLbRb5h",
                "forum": "E9t1ekt6W9",
                "replyto": "E9t1ekt6W9",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission928/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310228,
                "mdate": 1754869476193,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "E9t1ekt6W9",
                "forum": "E9t1ekt6W9",
                "content": {
                    "title": {
                        "value": "Joint Model-based Model-free Diffusion for Planning with Constraints"
                    },
                    "authors": {
                        "value": [
                            "Wonsuhk Jung",
                            "Utkarsh Aashu Mishra",
                            "Nadun Ranawaka Arachchige",
                            "Yongxin Chen",
                            "Danfei Xu",
                            "Shreyas Kousik"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Wonsuhk_Jung1",
                            "~Utkarsh_Aashu_Mishra2",
                            "~Nadun_Ranawaka_Arachchige1",
                            "~Yongxin_Chen1",
                            "~Danfei_Xu1",
                            "~Shreyas_Kousik1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Diffusion Model",
                            "Safety",
                            "Constrained Generation"
                        ]
                    },
                    "abstract": {
                        "value": "Model-free diffusion planners have shown great promise for robot motion planning, but practical robotic systems often require combining them with model-based optimization modules to enforce constraints, such as safety. Na\\\"ively integrating these modules presents compatibility challenges when diffusion's multi-modal outputs behave adversarially to optimization-based modules. To address this, we introduce Joint Model-based Model-free Diffusion (JM2D), a novel generative modeling framework. JM2D formulates module integration as a joint sampling problem to maximize compatibility via an interaction potential, without additional training. Using importance sampling, JM2D guides modules outputs based only on evaluations of the interaction potential, thus handling non-differentiable objectives commonly arising from non-convex optimization modules. We evaluate JM2D via application to aligning diffusion planners with safety modules on offline RL and robot manipulation. JM2D significantly improves task performance compared to conventional safety filters without sacrificing safety. Further, we show that conditional generation is a special case of JM2D and elucidate key design choices by comparing with SOTA gradient-based and projection-based diffusion planners. More details at: \\url{https://sites.google.com/view/joint-mbmf-diffusion}"
                    },
                    "supplementary_material": {
                        "value": "/attachment/ef82bdb695dc85be2bb4c1deed71b82896e0b96b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/19ae6558e1e298cd718ff2ae626aca2ac74949d6.pdf"
                    },
                    "TLDR": {
                        "value": "We propose a method to align diffusion planners with model‐based optimization via importance‐sampled joint sampling, yielding safer, high‐fidelity robot motion plans without extra training."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njung2025joint,\ntitle={Joint Model-based Model-free Diffusion for Planning with Constraints},\nauthor={Wonsuhk Jung and Utkarsh Aashu Mishra and Nadun Ranawaka Arachchige and Yongxin Chen and Danfei Xu and Shreyas Kousik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=E9t1ekt6W9}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6b2a32120e9445cd030275bfd10b873131eba452.zip"
                    },
                    "paperhash": {
                        "value": "jung|joint_modelbased_modelfree_diffusion_for_planning_with_constraints"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission928/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission928/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission928/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745462169458,
                "pdate": 1754680639437,
                "odate": 1758062785042,
                "mdate": 1758062825283,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission928/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission928/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Spg25qkV81",
        "title": "Versatile Loco-Manipulation through Flexible Interlimb Coordination",
        "abstract": "The ability to flexibly leverage limbs for loco-manipulation is essential for enabling autonomous robots to operate in unstructured environments. Yet, prior work on loco-manipulation is often constrained to specific tasks or predetermined limb configurations. In this work, we present einforcement Learning for Interlimb Coordination (ReLIC), an approach that enables versatile loco-manipulation through flexible interlimb coordination. The key to our approach is an adaptive controller that seamlessly bridges the execution of manipulation motions and the generation of stable gaits based on task demands. Through the interplay between two controller modules, ReLIC dynamically assigns each limb for manipulation or locomotion and robustly coordinates them to achieve the task success. Using efficient reinforcement learning in simulation, ReLIC learns to perform stable gaits in accordance with the manipulation goals in the real world. To solve diverse and complex tasks, we further propose to interface the learned controller with different types of task specifications, including target trajectories, contact points, and natural language instructions. Evaluated on 12 real-world tasks that require diverse and complex coordination patterns, ReLIC demonstrates its versatility and robustness by achieving a success rate of 78.9% on average.",
        "keywords": [
            "Loco-Manipulation",
            "Whole-Body Control",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/caa5865216574b05b6732071117d0bf5597d1203.pdf",
        "reviews": [
            {
                "id": "7fw4yl2AZc",
                "forum": "Spg25qkV81",
                "replyto": "Spg25qkV81",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission925/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068310207,
                "mdate": 1754869457885,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Spg25qkV81",
                "forum": "Spg25qkV81",
                "content": {
                    "title": {
                        "value": "Versatile Loco-Manipulation through Flexible Interlimb Coordination"
                    },
                    "authors": {
                        "value": [
                            "Xinghao Zhu",
                            "Yuxin Chen",
                            "Lingfeng Sun",
                            "Farzad Niroui",
                            "Simon Le Cleac'h",
                            "Jiuguang Wang",
                            "Kuan Fang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xinghao_Zhu1",
                            "~Yuxin_Chen7",
                            "~Lingfeng_Sun1",
                            "~Farzad_Niroui1",
                            "~Simon_Le_Cleac'h1",
                            "~Jiuguang_Wang1",
                            "~Kuan_Fang3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Loco-Manipulation",
                            "Whole-Body Control",
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "The ability to flexibly leverage limbs for loco-manipulation is essential for enabling autonomous robots to operate in unstructured environments. Yet, prior work on loco-manipulation is often constrained to specific tasks or predetermined limb configurations. In this work, we present einforcement Learning for Interlimb Coordination (ReLIC), an approach that enables versatile loco-manipulation through flexible interlimb coordination. The key to our approach is an adaptive controller that seamlessly bridges the execution of manipulation motions and the generation of stable gaits based on task demands. Through the interplay between two controller modules, ReLIC dynamically assigns each limb for manipulation or locomotion and robustly coordinates them to achieve the task success. Using efficient reinforcement learning in simulation, ReLIC learns to perform stable gaits in accordance with the manipulation goals in the real world. To solve diverse and complex tasks, we further propose to interface the learned controller with different types of task specifications, including target trajectories, contact points, and natural language instructions. Evaluated on 12 real-world tasks that require diverse and complex coordination patterns, ReLIC demonstrates its versatility and robustness by achieving a success rate of 78.9% on average."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b28876c861b4a4f908bed465e74580bb3974871a.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/caa5865216574b05b6732071117d0bf5597d1203.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhu2025versatile,\ntitle={Versatile Loco-Manipulation through Flexible Interlimb Coordination},\nauthor={Xinghao Zhu and Yuxin Chen and Lingfeng Sun and Farzad Niroui and Simon Le Cleac'h and Jiuguang Wang and Kuan Fang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Spg25qkV81}\n}"
                    },
                    "paperhash": {
                        "value": "zhu|versatile_locomanipulation_through_flexible_interlimb_coordination"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission925/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission925/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission925/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745461978393,
                "pdate": 1754680639367,
                "odate": 1758062785039,
                "mdate": 1758062825310,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission925/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zQXurgHUVX",
        "title": "DEQ-MPC : Deep Equilibrium Model Predictive Control",
        "abstract": "Incorporating task-specific priors within a policy or network architecture is crucial for enhancing safety and improving representation and generalization in robotic control problems. Differentiable Model Predictive Control (MPC) layers have proven effective for embedding these priors, such as constraints and cost functions, directly within the architecture, enabling end-to-end training. However, current methods often treat the solver and the neural network as separate, independent entities, leading to suboptimal integration. In this work, we propose a novel approach that co-develops the solver and architecture unifying the optimization solver and network inference problems. Specifically, we formulate this as a \\textit{joint fixed-point problem} over the coupled network outputs and necessary conditions of the optimization problem. We solve this problem in an iterative manner where we alternate between network forward passes and optimization iterations. Through extensive ablations in various robotic control tasks, we demonstrate that our approach results in richer representations and more stable training, while naturally accommodating warm starting, a key requirement for MPC.",
        "keywords": [
            "MPC",
            "Model Predictive Control",
            "Optimization",
            "Differentiable Optimization",
            "Control"
        ],
        "pdf_url": "https://openreview.net/pdf/df9e38de64ce997534ebd49a770700ccdf11f158.pdf",
        "reviews": [
            {
                "id": "O4kNpqpJaD",
                "forum": "zQXurgHUVX",
                "replyto": "zQXurgHUVX",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission918/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070538252,
                "mdate": 1754869476117,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zQXurgHUVX",
                "forum": "zQXurgHUVX",
                "content": {
                    "title": {
                        "value": "DEQ-MPC : Deep Equilibrium Model Predictive Control"
                    },
                    "authors": {
                        "value": [
                            "Swaminathan Gurumurthy",
                            "Khai Nguyen",
                            "Arun L Bishop",
                            "J Zico Kolter",
                            "Zachary Manchester"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Swaminathan_Gurumurthy1",
                            "~Khai_Nguyen2",
                            "~Arun_L_Bishop1",
                            "~J_Zico_Kolter1",
                            "~Zachary_Manchester1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "MPC",
                            "Model Predictive Control",
                            "Optimization",
                            "Differentiable Optimization",
                            "Control"
                        ]
                    },
                    "TLDR": {
                        "value": "Integrating differentiable MPC layers within NNets by posing it as a joint equilibrium finding problem over the network outputs and solver iterates, resulting in improved representation, gradients and warm-starting for robotic control tasks."
                    },
                    "abstract": {
                        "value": "Incorporating task-specific priors within a policy or network architecture is crucial for enhancing safety and improving representation and generalization in robotic control problems. Differentiable Model Predictive Control (MPC) layers have proven effective for embedding these priors, such as constraints and cost functions, directly within the architecture, enabling end-to-end training. However, current methods often treat the solver and the neural network as separate, independent entities, leading to suboptimal integration. In this work, we propose a novel approach that co-develops the solver and architecture unifying the optimization solver and network inference problems. Specifically, we formulate this as a \\textit{joint fixed-point problem} over the coupled network outputs and necessary conditions of the optimization problem. We solve this problem in an iterative manner where we alternate between network forward passes and optimization iterations. Through extensive ablations in various robotic control tasks, we demonstrate that our approach results in richer representations and more stable training, while naturally accommodating warm starting, a key requirement for MPC."
                    },
                    "supplementary_material": {
                        "value": "/attachment/8cd07a75ac836efec859c45f0efc937406a888b5.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/df9e38de64ce997534ebd49a770700ccdf11f158.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngurumurthy2025deqmpc,\ntitle={{DEQ}-{MPC} : Deep Equilibrium Model Predictive Control},\nauthor={Swaminathan Gurumurthy and Khai Nguyen and Arun L Bishop and J Zico Kolter and Zachary Manchester},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zQXurgHUVX}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d86a06be7b896c67990bb0670e0cab0d91930bc6.mp4"
                    },
                    "paperhash": {
                        "value": "gurumurthy|deqmpc_deep_equilibrium_model_predictive_control"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission918/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission918/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission918/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745461666771,
                "pdate": 1754680639249,
                "odate": 1758062784846,
                "mdate": 1758062824966,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission918/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "TqevdDMqrK",
        "title": "CUPID: Curating Data your Robot Loves with Influence Functions",
        "abstract": "In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes—such as closed-loop task success or failure—remains a persistent challenge. Inspired by the theory of influence functions, we propose CUPID. Given a set of evaluation rollouts, CUPID estimates the influence of a training demonstration on the policy’s expected return. This enables ranking and selection of demonstrations according to their impact on the policy’s closed-loop performance. We use our estimator to curate data by 1) filtering out training demonstrations that harmed the policy’s performance and 2) subselecting newly collected trajectories that will most help improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can result in state-of-the-art diffusion policies on the simulated Robomimic benchmark, and we observe similar improvements in hardware experiments. Furthermore, our hardware experiments show that our influence-based estimator can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance post-training of generalist policies.",
        "keywords": [
            "Imitation Learning",
            "Data Curation",
            "Influence Functions"
        ],
        "pdf_url": "https://openreview.net/pdf/e632b12e34fc4df22c1e323c8551ddf3ec84ce3f.pdf",
        "reviews": [
            {
                "id": "X7g8sOpiLs",
                "forum": "TqevdDMqrK",
                "replyto": "TqevdDMqrK",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission910/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068309542,
                "mdate": 1754869475983,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "TqevdDMqrK",
                "forum": "TqevdDMqrK",
                "content": {
                    "title": {
                        "value": "CUPID: Curating Data your Robot Loves with Influence Functions"
                    },
                    "authors": {
                        "value": [
                            "Christopher Agia",
                            "Rohan Sinha",
                            "Jingyun Yang",
                            "Rika Antonova",
                            "Marco Pavone",
                            "Haruki Nishimura",
                            "Masha Itkina",
                            "Jeannette Bohg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Christopher_Agia1",
                            "~Rohan_Sinha1",
                            "~Jingyun_Yang1",
                            "~Rika_Antonova1",
                            "~Marco_Pavone1",
                            "~Haruki_Nishimura2",
                            "~Masha_Itkina1",
                            "~Jeannette_Bohg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Data Curation",
                            "Influence Functions"
                        ]
                    },
                    "abstract": {
                        "value": "In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes—such as closed-loop task success or failure—remains a persistent challenge. Inspired by the theory of influence functions, we propose CUPID. Given a set of evaluation rollouts, CUPID estimates the influence of a training demonstration on the policy’s expected return. This enables ranking and selection of demonstrations according to their impact on the policy’s closed-loop performance. We use our estimator to curate data by 1) filtering out training demonstrations that harmed the policy’s performance and 2) subselecting newly collected trajectories that will most help improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can result in state-of-the-art diffusion policies on the simulated Robomimic benchmark, and we observe similar improvements in hardware experiments. Furthermore, our hardware experiments show that our influence-based estimator can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance post-training of generalist policies."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1c20fa190e2867c4cf720e366fa896fad05612c1.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We propose a data curation method for robot imitation learning that uses influence functions to measure the causal impact of a demonstration on the policy's closed-loop performance."
                    },
                    "pdf": {
                        "value": "/pdf/e632b12e34fc4df22c1e323c8551ddf3ec84ce3f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nagia2025cupid,\ntitle={{CUPID}: Curating Data your Robot Loves with Influence Functions},\nauthor={Christopher Agia and Rohan Sinha and Jingyun Yang and Rika Antonova and Marco Pavone and Haruki Nishimura and Masha Itkina and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=TqevdDMqrK}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/45bfd90ccc179d9d43f73a53206e6e00c14a1046.mp4"
                    },
                    "paperhash": {
                        "value": "agia|cupid_curating_data_your_robot_loves_with_influence_functions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission910/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission910/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission910/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745461014913,
                "pdate": 1754680638864,
                "odate": 1758062784561,
                "mdate": 1758062824928,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission910/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission910/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "KKpdjQK6nT",
        "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop",
        "abstract": "Humans do not passively observe the visual world---we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by introducing a BC-RL loop trained using teleoperated demonstrations recorded with a 360 camera. The resulting video enables a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing reinforcement learning of gaze behavior. The hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct actions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. We evaluate EyeRobot on five large workspace manipulation tasks and compare performance to two common camera setups: wrist and external cameras. Our experiments suggest EyeRobot exhibits hand-eye coordination which effectively facilitates action such as visual search or target switching, which enable manipulation across large workspaces.",
        "keywords": [
            "Active Vision",
            "Reinforcement Learning",
            "Behavior Cloning",
            "Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/79bbd1d0f2b7d897798c8dc2cdf2ce9993c1f555.pdf",
        "reviews": [
            {
                "id": "SVUw6d8sVJ",
                "forum": "KKpdjQK6nT",
                "replyto": "KKpdjQK6nT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission909/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068309459,
                "mdate": 1754869475891,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "KKpdjQK6nT",
                "forum": "KKpdjQK6nT",
                "content": {
                    "title": {
                        "value": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop"
                    },
                    "authors": {
                        "value": [
                            "Justin Kerr",
                            "Kush Hari",
                            "Ethan Weber",
                            "Chung Min Kim",
                            "Brent Yi",
                            "tyler bonnen",
                            "Ken Goldberg",
                            "Angjoo Kanazawa"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Justin_Kerr1",
                            "~Kush_Hari1",
                            "~Ethan_Weber1",
                            "~Chung_Min_Kim1",
                            "~Brent_Yi1",
                            "~tyler_bonnen1",
                            "~Ken_Goldberg1",
                            "~Angjoo_Kanazawa1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Active Vision",
                            "Reinforcement Learning",
                            "Behavior Cloning",
                            "Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "Training an active mechanical eyeball policy with RL to look around to facilitate behavior cloning over a large workspace"
                    },
                    "abstract": {
                        "value": "Humans do not passively observe the visual world---we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by introducing a BC-RL loop trained using teleoperated demonstrations recorded with a 360 camera. The resulting video enables a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing reinforcement learning of gaze behavior. The hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct actions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. We evaluate EyeRobot on five large workspace manipulation tasks and compare performance to two common camera setups: wrist and external cameras. Our experiments suggest EyeRobot exhibits hand-eye coordination which effectively facilitates action such as visual search or target switching, which enable manipulation across large workspaces."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b9930586a682cdced121a3c9c482d7db2da99dc9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/79bbd1d0f2b7d897798c8dc2cdf2ce9993c1f555.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkerr2025eye,\ntitle={Eye, Robot: Learning to Look to Act with a {BC}-{RL} Perception-Action Loop},\nauthor={Justin Kerr and Kush Hari and Ethan Weber and Chung Min Kim and Brent Yi and tyler bonnen and Ken Goldberg and Angjoo Kanazawa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KKpdjQK6nT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d9e956bfdfcbe6f15f9d97618620b7ef36d4c369.mp4"
                    },
                    "paperhash": {
                        "value": "kerr|eye_robot_learning_to_look_to_act_with_a_bcrl_perceptionaction_loop"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission909/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission909/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission909/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745460912474,
                "pdate": 1754680638739,
                "odate": 1758062784465,
                "mdate": 1758062824948,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission909/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9AHjtHLlIe",
        "title": "Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models",
        "abstract": "Exploration is key for general-purpose robotic learning, particularly in open-ended environments where explicit guidance or task-specific feedback is limited. Vision-language models (VLMs), which can reason about object semantics, spatial relations, and potential outcomes, offer a promising foundation for guiding exploratory behavior by generating high-level goals or transitions. However, their outputs lack grounding, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity.  Human exploration often emerges from the drive to discover novel scene configurations and to understand the environment.\nInspired by this, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE produces more diverse and meaningful exploration than RL baselines. The collected data facilitates learning downstream tasks that closely match those of policies trained on human-collected demonstrations.",
        "keywords": [
            "Exploration",
            "Agentic System",
            "Vision-Language Model"
        ],
        "pdf_url": "https://openreview.net/pdf/0f9e94d73a351bf55403039f2ba701ad6271f332.pdf",
        "reviews": [
            {
                "id": "tzCycbxDdu",
                "forum": "9AHjtHLlIe",
                "replyto": "9AHjtHLlIe",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission907/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068309210,
                "mdate": 1754869475820,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9AHjtHLlIe",
                "forum": "9AHjtHLlIe",
                "content": {
                    "title": {
                        "value": "Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models"
                    },
                    "authors": {
                        "value": [
                            "Seungjae Lee",
                            "Daniel Ekpo",
                            "Haowen Liu",
                            "Furong Huang",
                            "Abhinav Shrivastava",
                            "Jia-Bin Huang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Seungjae_Lee2",
                            "~Daniel_Ekpo1",
                            "~Haowen_Liu2",
                            "~Furong_Huang1",
                            "~Abhinav_Shrivastava2",
                            "~Jia-Bin_Huang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Exploration",
                            "Agentic System",
                            "Vision-Language Model"
                        ]
                    },
                    "abstract": {
                        "value": "Exploration is key for general-purpose robotic learning, particularly in open-ended environments where explicit guidance or task-specific feedback is limited. Vision-language models (VLMs), which can reason about object semantics, spatial relations, and potential outcomes, offer a promising foundation for guiding exploratory behavior by generating high-level goals or transitions. However, their outputs lack grounding, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity.  Human exploration often emerges from the drive to discover novel scene configurations and to understand the environment.\nInspired by this, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE produces more diverse and meaningful exploration than RL baselines. The collected data facilitates learning downstream tasks that closely match those of policies trained on human-collected demonstrations."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2b73bb234439bf8433d2cde76593db8660ba9815.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0f9e94d73a351bf55403039f2ba701ad6271f332.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlee2025imagine,\ntitle={Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models},\nauthor={Seungjae Lee and Daniel Ekpo and Haowen Liu and Furong Huang and Abhinav Shrivastava and Jia-Bin Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9AHjtHLlIe}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/00c602a96f204a2488b92280d9786dc0f49417b1.mp4"
                    },
                    "paperhash": {
                        "value": "lee|imagine_verify_execute_memoryguided_agentic_exploration_with_visionlanguage_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission907/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission907/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission907/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745460479001,
                "pdate": 1754680638668,
                "odate": 1758062784393,
                "mdate": 1758062824699,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission907/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission907/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "8RdxHk9hpr",
        "title": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation",
        "abstract": "Nominal payload ratings for articulated robots are typically derived from worst-case configurations, resulting in uniform payload constraints across the entire workspace. This conservative approach severely underutilizes the robot's inherent capabilities---our analysis demonstrates that manipulators can safely handle payloads well above nominal capacity across broad regions of their workspace while staying within joint angle, velocity, acceleration, and torque limits. To address this gap between assumed and actual capability, we propose a novel trajectory generation approach using denoising diffusion models that explicitly incorporates payload constraints into the planning process. Unlike traditional sampling-based methods that rely on inefficient trial-and-error, optimization-based methods that are prohibitively slow, or kinodynamic planners that struggle with problem dimensionality, our approach generates dynamically feasible joint-space trajectories in constant time that can be directly executed on physical hardware without post-processing. Experimental validation on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the workspace remains accessible even with payloads exceeding 3 times the nominal capacity. This expanded operational envelope highlights the importance of a more nuanced consideration of payload dynamics in motion planning algorithms.",
        "keywords": [
            "Robot Planning",
            "Grasping & Manipulation",
            "Robot Modeling & Simulation",
            "diffusion models",
            "dynamics-constrained planning",
            "payload transport"
        ],
        "pdf_url": "https://openreview.net/pdf/644cd8cb8bef0d4a1a3952caae9a2c30b05574a9.pdf",
        "reviews": [
            {
                "id": "1xpBGm3VZi",
                "forum": "8RdxHk9hpr",
                "replyto": "8RdxHk9hpr",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission905/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068309168,
                "mdate": 1754869475629,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "8RdxHk9hpr",
                "forum": "8RdxHk9hpr",
                "content": {
                    "title": {
                        "value": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Anuj Pasricha",
                            "Joewie J. Koh",
                            "Jay Vakil",
                            "Alessandro Roncone"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Anuj_Pasricha1",
                            "~Joewie_J._Koh1",
                            "~Jay_Vakil1",
                            "~Alessandro_Roncone1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Planning",
                            "Grasping & Manipulation",
                            "Robot Modeling & Simulation",
                            "diffusion models",
                            "dynamics-constrained planning",
                            "payload transport"
                        ]
                    },
                    "abstract": {
                        "value": "Nominal payload ratings for articulated robots are typically derived from worst-case configurations, resulting in uniform payload constraints across the entire workspace. This conservative approach severely underutilizes the robot's inherent capabilities---our analysis demonstrates that manipulators can safely handle payloads well above nominal capacity across broad regions of their workspace while staying within joint angle, velocity, acceleration, and torque limits. To address this gap between assumed and actual capability, we propose a novel trajectory generation approach using denoising diffusion models that explicitly incorporates payload constraints into the planning process. Unlike traditional sampling-based methods that rely on inefficient trial-and-error, optimization-based methods that are prohibitively slow, or kinodynamic planners that struggle with problem dimensionality, our approach generates dynamically feasible joint-space trajectories in constant time that can be directly executed on physical hardware without post-processing. Experimental validation on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the workspace remains accessible even with payloads exceeding 3 times the nominal capacity. This expanded operational envelope highlights the importance of a more nuanced consideration of payload dynamics in motion planning algorithms."
                    },
                    "supplementary_material": {
                        "value": "/attachment/5756274f5d29dd872e481b80ec8c1dc34973b778.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/644cd8cb8bef0d4a1a3952caae9a2c30b05574a9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\npasricha2025dynamicscompliant,\ntitle={Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation},\nauthor={Anuj Pasricha and Joewie J. Koh and Jay Vakil and Alessandro Roncone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8RdxHk9hpr}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/7fd2111ab5e00a735b4ebe62e6a79a36a6294ef4.mp4"
                    },
                    "paperhash": {
                        "value": "pasricha|dynamicscompliant_trajectory_diffusion_for_supernominal_payload_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission905/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission905/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission905/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745460200692,
                "pdate": 1754680638600,
                "odate": 1758062784313,
                "mdate": 1758062824682,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission905/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission905/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "dBaSaa7qi4",
        "title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction",
        "abstract": "Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed.",
        "keywords": [
            "Robot intent generation",
            "Human-robot communication",
            "Assistive robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/48d09d45f69b4486868c7cb5f466d7548eb7458c.pdf",
        "reviews": [
            {
                "id": "QSr65n2zFK",
                "forum": "dBaSaa7qi4",
                "replyto": "dBaSaa7qi4",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission893/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068308804,
                "mdate": 1754869475582,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "dBaSaa7qi4",
                "forum": "dBaSaa7qi4",
                "content": {
                    "title": {
                        "value": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction"
                    },
                    "authors": {
                        "value": [
                            "Junxiang Wang",
                            "Emek Barış Küçüktabak",
                            "Rana Soltani Zarrin",
                            "Zackory Erickson"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Junxiang_Wang5",
                            "baris_kucuktabak@honda-ri.com",
                            "~Rana_Soltani_Zarrin1",
                            "~Zackory_Erickson1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot intent generation",
                            "Human-robot communication",
                            "Assistive robotics"
                        ]
                    },
                    "abstract": {
                        "value": "Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/48d09d45f69b4486868c7cb5f466d7548eb7458c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025cori,\ntitle={Co{RI}: Communication of Robot Intent for Physical Human-Robot Interaction},\nauthor={Junxiang Wang and Emek Bar{\\i}{\\c{s}} K{\\\"u}{\\c{c}}{\\\"u}ktabak and Rana Soltani Zarrin and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dBaSaa7qi4}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/109d026cdd33d85ce435f8d2a918e223e7a913f3.mp4"
                    },
                    "paperhash": {
                        "value": "wang|cori_communication_of_robot_intent_for_physical_humanrobot_interaction"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission893/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission893/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission893/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745458638330,
                "pdate": 1754680638369,
                "odate": 1758062784063,
                "mdate": 1758062824650,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission893/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission893/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "hg9YtHV8MJ",
        "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning",
        "abstract": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. \nDespite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals, a requirement when learning from demonstrations (LfD). \nWe present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when the robot is placed away from the desired trajectory, and tracks the trajectory until the end point.\nTo demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset, including spectral analysis.\nWe also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment.\nOur approach is highly sample efficient in both space and time, requiring only 3\\% of the LASA dataset to generate dense motion plans.\nAdditionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy.",
        "keywords": [
            "motion planning;koopman operator theory",
            "dynamical systems",
            "learning from demonstrations"
        ],
        "pdf_url": "https://openreview.net/pdf/b1b89cb36b40c30954760135788b2b6350e69984.pdf",
        "reviews": [
            {
                "id": "7TwJk3njbd",
                "forum": "hg9YtHV8MJ",
                "replyto": "hg9YtHV8MJ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission885/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068308660,
                "mdate": 1754869475627,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "hg9YtHV8MJ",
                "forum": "hg9YtHV8MJ",
                "content": {
                    "title": {
                        "value": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning"
                    },
                    "authors": {
                        "value": [
                            "Alice Kate Li",
                            "Thales C. Silva",
                            "Victoria Edwards",
                            "Vijay Kumar",
                            "M. Ani Hsieh"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Alice_Kate_Li1",
                            "s.c.thales@gmail.com",
                            "~Victoria_Edwards1",
                            "~Vijay_Kumar2",
                            "~M._Ani_Hsieh1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "motion planning;koopman operator theory",
                            "dynamical systems",
                            "learning from demonstrations"
                        ]
                    },
                    "abstract": {
                        "value": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. \nDespite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals, a requirement when learning from demonstrations (LfD). \nWe present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when the robot is placed away from the desired trajectory, and tracks the trajectory until the end point.\nTo demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset, including spectral analysis.\nWe also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment.\nOur approach is highly sample efficient in both space and time, requiring only 3\\% of the LASA dataset to generate dense motion plans.\nAdditionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy."
                    },
                    "supplementary_material": {
                        "value": "/attachment/74204fed6790d35a00894e091b65bf0de5dd54ed.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "learning almost divergence free koopman dynamical systems for motion planning"
                    },
                    "pdf": {
                        "value": "/pdf/b1b89cb36b40c30954760135788b2b6350e69984.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025koopmotion,\ntitle={KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning},\nauthor={Alice Kate Li and Thales C. Silva and Victoria Edwards and Vijay Kumar and M. Ani Hsieh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=hg9YtHV8MJ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6866d31d0c00e522003858258944553fc65dbbb1.mp4"
                    },
                    "paperhash": {
                        "value": "li|koopmotion_learning_almost_divergence_free_koopman_flow_fields_for_motion_planning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission885/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission885/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission885/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745457418301,
                "pdate": 1754680638125,
                "odate": 1758062783808,
                "mdate": 1758062824438,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission885/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "oYC10hiFua",
        "title": "Learning Smooth State-Dependent Traversability from Dense Point Clouds",
        "abstract": "A key open challenge in off-road autonomy is that the traversability of terrain often depends on the vehicle's state. In particular, some obstacles are only traversable from some orientations. However, learning this interaction by encoding the angle of approach as a model input demands a large and diverse training dataset and is computationally inefficient during planning due to repeated model inference. To address these challenges, we present SPARTA, a method for estimating approach angle conditioned traversability from point clouds. Specifically, we impose geometric structure into our network by outputting a smooth analytical function over the 1-Sphere that predicts risk distribution for any angle of approach with minimal overhead and can be reused for subsequent queries. The function is composed of Fourier basis functions, which has important advantages for generalization due to their periodic nature and smoothness. We demonstrate SPARTA both in a high-fidelity simulation platform, where our model achieves a 91% success rate crossing a 40m boulder field (compared to 73\\% for the baseline), and on hardware, illustrating the generalization ability of the model to real-world settings.",
        "keywords": [
            "Off-road Autonomy",
            "Traversability",
            "Geometric Deep Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/6e14b6283bfb8a011d59d3ebc763dd51f0d8499c.pdf",
        "reviews": [
            {
                "id": "ojMtSsYHIK",
                "forum": "oYC10hiFua",
                "replyto": "oYC10hiFua",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission883/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068308339,
                "mdate": 1754869475205,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "oYC10hiFua",
                "forum": "oYC10hiFua",
                "content": {
                    "title": {
                        "value": "Learning Smooth State-Dependent Traversability from Dense Point Clouds"
                    },
                    "authors": {
                        "value": [
                            "Zihao Dong",
                            "Alan Papalia",
                            "Leonard Jung",
                            "Alenna Spiro",
                            "Philip R Osteen",
                            "Christa S. Robison",
                            "Michael Everett"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zihao_Dong2",
                            "~Alan_Papalia2",
                            "~Leonard_Jung1",
                            "~Alenna_Spiro1",
                            "~Philip_R_Osteen1",
                            "~Christa_S._Robison1",
                            "~Michael_Everett1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Off-road Autonomy",
                            "Traversability",
                            "Geometric Deep Learning"
                        ]
                    },
                    "abstract": {
                        "value": "A key open challenge in off-road autonomy is that the traversability of terrain often depends on the vehicle's state. In particular, some obstacles are only traversable from some orientations. However, learning this interaction by encoding the angle of approach as a model input demands a large and diverse training dataset and is computationally inefficient during planning due to repeated model inference. To address these challenges, we present SPARTA, a method for estimating approach angle conditioned traversability from point clouds. Specifically, we impose geometric structure into our network by outputting a smooth analytical function over the 1-Sphere that predicts risk distribution for any angle of approach with minimal overhead and can be reused for subsequent queries. The function is composed of Fourier basis functions, which has important advantages for generalization due to their periodic nature and smoothness. We demonstrate SPARTA both in a high-fidelity simulation platform, where our model achieves a 91% success rate crossing a 40m boulder field (compared to 73\\% for the baseline), and on hardware, illustrating the generalization ability of the model to real-world settings."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e889a373d614a37edbb2ba129adae0dd0f47b43c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A General system that predicts angle-dependent risk variable distribution for off-road autonomy"
                    },
                    "pdf": {
                        "value": "/pdf/6e14b6283bfb8a011d59d3ebc763dd51f0d8499c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndong2025learning,\ntitle={Learning Smooth State-Dependent Traversability from Dense Point Clouds},\nauthor={Zihao Dong and Alan Papalia and Leonard Jung and Alenna Spiro and Philip R Osteen and Christa S. Robison and Michael Everett},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oYC10hiFua}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5d8c957478072ef7c53c5dc9852cdec925687e20.mp4"
                    },
                    "paperhash": {
                        "value": "dong|learning_smooth_statedependent_traversability_from_dense_point_clouds"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission883/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission883/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission883/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745457194185,
                "pdate": 1754680638056,
                "odate": 1758062783732,
                "mdate": 1758062824413,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission883/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission883/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ejV8YHTpHR",
        "title": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation",
        "abstract": "Learning-based robotic systems demand rigorous validation to assure reliable performance, but extensive real‐world testing is often prohibitively expensive and if conducted may still yield insufficient data for high-confidence guarantees. In this work, we introduce a general estimation framework that leverages *paired* data across test platforms, e.g., paired simulation and real‐world observations, to achieve better estimates of real-world metrics via the method of control variates. By incorporating cheap and abundant auxiliary measurements (for example, simulator outputs) as control variates for costly real‐world samples, our method provably reduces the variance of Monte Carlo estimates and thus requires significantly fewer real‐world samples to attain a specified confidence bound on the mean performance. We provide theoretical analysis characterizing the variance and sample-efficiency improvement, and demonstrate empirically in autonomous driving and quadruped robotics settings that our approach achieves high‐probability bounds with markedly reduced sample complexity. Our technique can lower the real‐world testing burden for validating the performance of the stack, thereby enabling more efficient and cost‐effective experimental evaluation of robotic systems.",
        "keywords": [
            "metric estimation",
            "sample efficiency",
            "control variates"
        ],
        "pdf_url": "https://openreview.net/pdf/ea6a75c33dbfbc2752530e9e46406f9831e3a77b.pdf",
        "reviews": [
            {
                "id": "9sB0X1m6JD",
                "forum": "ejV8YHTpHR",
                "replyto": "ejV8YHTpHR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission874/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068308071,
                "mdate": 1754869475124,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ejV8YHTpHR",
                "forum": "ejV8YHTpHR",
                "content": {
                    "title": {
                        "value": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation"
                    },
                    "authors": {
                        "value": [
                            "Rachel Luo",
                            "Heng Yang",
                            "Michael Watson",
                            "Apoorva Sharma",
                            "Sushant Veer",
                            "Edward Schmerling",
                            "Marco Pavone"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Rachel_Luo1",
                            "~Heng_Yang4",
                            "mwatson@nvidia.com",
                            "~Apoorva_Sharma1",
                            "~Sushant_Veer1",
                            "~Edward_Schmerling1",
                            "~Marco_Pavone1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "metric estimation",
                            "sample efficiency",
                            "control variates"
                        ]
                    },
                    "abstract": {
                        "value": "Learning-based robotic systems demand rigorous validation to assure reliable performance, but extensive real‐world testing is often prohibitively expensive and if conducted may still yield insufficient data for high-confidence guarantees. In this work, we introduce a general estimation framework that leverages *paired* data across test platforms, e.g., paired simulation and real‐world observations, to achieve better estimates of real-world metrics via the method of control variates. By incorporating cheap and abundant auxiliary measurements (for example, simulator outputs) as control variates for costly real‐world samples, our method provably reduces the variance of Monte Carlo estimates and thus requires significantly fewer real‐world samples to attain a specified confidence bound on the mean performance. We provide theoretical analysis characterizing the variance and sample-efficiency improvement, and demonstrate empirically in autonomous driving and quadruped robotics settings that our approach achieves high‐probability bounds with markedly reduced sample complexity. Our technique can lower the real‐world testing burden for validating the performance of the stack, thereby enabling more efficient and cost‐effective experimental evaluation of robotic systems."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/ea6a75c33dbfbc2752530e9e46406f9831e3a77b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nluo2025leveraging,\ntitle={Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation},\nauthor={Rachel Luo and Heng Yang and Michael Watson and Apoorva Sharma and Sushant Veer and Edward Schmerling and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ejV8YHTpHR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/cc2adb2277c6e5a2e3ab2052f8ccf3a1799f4dc3.mp4"
                    },
                    "paperhash": {
                        "value": "luo|leveraging_correlation_across_test_platforms_for_variancereduced_metric_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission874/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission874/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission874/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745455549429,
                "pdate": 1754680637896,
                "odate": 1758062783494,
                "mdate": 1758062824369,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission874/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "sXoaNAECCK",
        "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance",
        "abstract": "Manipulating clothing is challenging due to their complex, variable configurations and frequent self-occlusion. While prior systems often rely on flattening garments, humans routinely identify keypoints in highly crumpled and suspended states. We present a novel, task-agnostic, visuotactile framework that operates directly on crumpled clothing—including in-air configurations that have not been addressed before. Our approach combines global visual perception with local tactile feedback to enable robust, reactive manipulation. We train dense visual descriptors on a custom simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that dynamically selects between folding strategies based on perceptual uncertainty. In parallel, we train a visuotactile grasp affordance network using high-resolution tactile feedback to supervise grasp success. The same tactile classifier is used during execution for real-time grasp validation. Together, these components enable a reactive, task-agnostic framework for in-air garment manipulation, including folding and hanging tasks. Moreover, our dense descriptors serve as a versatile intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.",
        "keywords": [
            "Deformable Object Manipulation",
            "Dense Correspondence Learning",
            "Confidence-Aware Planning",
            "Visuotactile Perception"
        ],
        "pdf_url": "https://openreview.net/pdf/601890f5232b9a5faaf890a1523e35b3fd8e792d.pdf",
        "reviews": [
            {
                "id": "oKPsB8q3ya",
                "forum": "sXoaNAECCK",
                "replyto": "sXoaNAECCK",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission871/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068307803,
                "mdate": 1754869457824,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "sXoaNAECCK",
                "forum": "sXoaNAECCK",
                "content": {
                    "title": {
                        "value": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance"
                    },
                    "authors": {
                        "value": [
                            "Neha Sunil",
                            "Megha Tippur",
                            "Arnau Saumell Portillo",
                            "Edward H Adelson",
                            "Alberto Rodriguez Garcia"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Neha_Sunil1",
                            "~Megha_Tippur1",
                            "~Arnau_Saumell_Portillo1",
                            "~Edward_H_Adelson1",
                            "~Alberto_Rodriguez_Garcia1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Deformable Object Manipulation",
                            "Dense Correspondence Learning",
                            "Confidence-Aware Planning",
                            "Visuotactile Perception"
                        ]
                    },
                    "TLDR": {
                        "value": "We develop a visuotactile system capable of folding and hanging in-air using dense visual representations and tactilely-supervised visual affordance networks."
                    },
                    "abstract": {
                        "value": "Manipulating clothing is challenging due to their complex, variable configurations and frequent self-occlusion. While prior systems often rely on flattening garments, humans routinely identify keypoints in highly crumpled and suspended states. We present a novel, task-agnostic, visuotactile framework that operates directly on crumpled clothing—including in-air configurations that have not been addressed before. Our approach combines global visual perception with local tactile feedback to enable robust, reactive manipulation. We train dense visual descriptors on a custom simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that dynamically selects between folding strategies based on perceptual uncertainty. In parallel, we train a visuotactile grasp affordance network using high-resolution tactile feedback to supervise grasp success. The same tactile classifier is used during execution for real-time grasp validation. Together, these components enable a reactive, task-agnostic framework for in-air garment manipulation, including folding and hanging tasks. Moreover, our dense descriptors serve as a versatile intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b0b109db77e2a0718aac47e91e804c1e9ff6fb3a.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/601890f5232b9a5faaf890a1523e35b3fd8e792d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsunil2025reactive,\ntitle={Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance},\nauthor={Neha Sunil and Megha Tippur and Arnau Saumell Portillo and Edward H Adelson and Alberto Rodriguez Garcia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sXoaNAECCK}\n}"
                    },
                    "paperhash": {
                        "value": "sunil|reactive_inair_clothing_manipulation_with_confidenceaware_dense_correspondence_and_visuotactile_affordance"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission871/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission871/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission871/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745454774776,
                "pdate": 1754680637789,
                "odate": 1758062783484,
                "mdate": 1758062824210,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission871/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission871/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "K7KLc4FexO",
        "title": "Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning",
        "abstract": "Autonomous surgical robots are a promising solution to the increasing demand for surgery amid a shortage of surgeons. Recent work has proposed learning-based approaches for the autonomous manipulation of soft tissue. However, due to variability in aspects such as tissue geometries and stiffnesses, these methods do not always perform well, especially in out-of-distribution settings. To address this challenge, we propose a novel second-order metric for uncertainty quantification, agreement volatility, that enables successful and efficient collaborative handoffs between a human operator and a robot during soft-tissue manipulation by allowing the robot to know when to cede control to human operators and when to resume autonomous operation. We validate our approach using the daVinci Research Kit (dVRK) surgical robot to perform risk-aware physical soft-tissue manipulation. Our experimental results demonstrate that our proposed agreement volatility metric improves system success rates and leads to a 10\\% lower reliance on human interventions compared to a variance-only baseline.  We further demonstrate the usefulness of our agreement volatility metric as a spatial uncertainty map over geometric point cloud data, enabling uncertainty attribution which provides insight into regions of the input causing uncertainty.",
        "keywords": [
            "Uncertainty Quantification",
            "Uncertainty Attribution",
            "Surgical Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/64c973938d53bff0d9e3bf3a877f3cc9f7f84aff.pdf",
        "reviews": [
            {
                "id": "qSlx3Jp5ya",
                "forum": "K7KLc4FexO",
                "replyto": "K7KLc4FexO",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission870/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068307839,
                "mdate": 1754869474929,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "K7KLc4FexO",
                "forum": "K7KLc4FexO",
                "content": {
                    "title": {
                        "value": "Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning"
                    },
                    "authors": {
                        "value": [
                            "Jordan Thompson",
                            "Britton Jordan",
                            "Daniel S. Brown",
                            "Alan Kuntz"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jordan_Thompson1",
                            "u1064972@utah.edu",
                            "~Daniel_S._Brown1",
                            "~Alan_Kuntz1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Uncertainty Quantification",
                            "Uncertainty Attribution",
                            "Surgical Robotics"
                        ]
                    },
                    "abstract": {
                        "value": "Autonomous surgical robots are a promising solution to the increasing demand for surgery amid a shortage of surgeons. Recent work has proposed learning-based approaches for the autonomous manipulation of soft tissue. However, due to variability in aspects such as tissue geometries and stiffnesses, these methods do not always perform well, especially in out-of-distribution settings. To address this challenge, we propose a novel second-order metric for uncertainty quantification, agreement volatility, that enables successful and efficient collaborative handoffs between a human operator and a robot during soft-tissue manipulation by allowing the robot to know when to cede control to human operators and when to resume autonomous operation. We validate our approach using the daVinci Research Kit (dVRK) surgical robot to perform risk-aware physical soft-tissue manipulation. Our experimental results demonstrate that our proposed agreement volatility metric improves system success rates and leads to a 10\\% lower reliance on human interventions compared to a variance-only baseline.  We further demonstrate the usefulness of our agreement volatility metric as a spatial uncertainty map over geometric point cloud data, enabling uncertainty attribution which provides insight into regions of the input causing uncertainty."
                    },
                    "supplementary_material": {
                        "value": "/attachment/65d86d56d9f0616ccd23a6c6f44425266b7f30bd.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/64c973938d53bff0d9e3bf3a877f3cc9f7f84aff.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nthompson2025agreement,\ntitle={Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning},\nauthor={Jordan Thompson and Britton Jordan and Daniel S. Brown and Alan Kuntz},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=K7KLc4FexO}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ac73ece138884e15c8f4b026e7c72a42017c3674.mp4"
                    },
                    "paperhash": {
                        "value": "thompson|agreement_volatility_a_secondorder_metric_for_uncertainty_quantification_in_surgical_robot_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission870/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission870/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission870/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745454749624,
                "pdate": 1754680637788,
                "odate": 1758062783448,
                "mdate": 1758062824149,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission870/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jr1Gjpjmqc",
        "title": "Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates",
        "abstract": "Data-driven control methods need to be sample-efficient and lightweight, especially when data acquisition and computational resources are limited---such as during learning on hardware. Most modern data-driven methods require large datasets and struggle with real-time updates of models, limiting their performance in dynamic environments. Koopman theory formally represents nonlinear systems as linear models over observables, and Koopman representations can be determined from data in an optimization-friendly setting with potentially rapid model updates. In this paper, we present a highly sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning (RKL). We identify sufficient conditions for model convergence and provide formal algorithmic analysis supporting our claim that RKL is lightweight and fast, with complexity independent of dataset size. We validate our method on a simulated planar two-link arm and a hybrid nonlinear hardware system with soft actuators, showing that real-time recursive Koopman model updates improve the sample efficiency and stability of data-driven controller synthesis---requiring only <10% of the data compared to benchmarks. The high-performance C++ codebase will be open-sourced.",
        "keywords": [
            "Koopman Operator; Control"
        ],
        "pdf_url": "https://openreview.net/pdf/004e75861f05ed648951f0b323661355a369bb0c.pdf",
        "reviews": [
            {
                "id": "k1tWHR8JTH",
                "forum": "jr1Gjpjmqc",
                "replyto": "jr1Gjpjmqc",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission860/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068306997,
                "mdate": 1754869474950,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jr1Gjpjmqc",
                "forum": "jr1Gjpjmqc",
                "content": {
                    "title": {
                        "value": "Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates"
                    },
                    "authors": {
                        "value": [
                            "Zixin Zhang",
                            "James Avtges",
                            "Todd Murphey"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zixin_Zhang7",
                            "~James_Avtges1",
                            "~Todd_Murphey1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Koopman Operator; Control"
                        ]
                    },
                    "abstract": {
                        "value": "Data-driven control methods need to be sample-efficient and lightweight, especially when data acquisition and computational resources are limited---such as during learning on hardware. Most modern data-driven methods require large datasets and struggle with real-time updates of models, limiting their performance in dynamic environments. Koopman theory formally represents nonlinear systems as linear models over observables, and Koopman representations can be determined from data in an optimization-friendly setting with potentially rapid model updates. In this paper, we present a highly sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning (RKL). We identify sufficient conditions for model convergence and provide formal algorithmic analysis supporting our claim that RKL is lightweight and fast, with complexity independent of dataset size. We validate our method on a simulated planar two-link arm and a hybrid nonlinear hardware system with soft actuators, showing that real-time recursive Koopman model updates improve the sample efficiency and stability of data-driven controller synthesis---requiring only <10% of the data compared to benchmarks. The high-performance C++ codebase will be open-sourced."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/004e75861f05ed648951f0b323661355a369bb0c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025sampleefficient,\ntitle={Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates},\nauthor={Zixin Zhang and James Avtges and Todd Murphey},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jr1Gjpjmqc}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2a284abd44ee3e5553a19a7eec5310049827d5d4.zip"
                    },
                    "paperhash": {
                        "value": "zhang|sampleefficient_online_control_policy_learning_with_realtime_recursive_model_updates"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission860/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission860/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission860/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745452797337,
                "pdate": 1754680637515,
                "odate": 1758062783006,
                "mdate": 1758062824112,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission860/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission860/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "3p7rTnLJM8",
        "title": "Lucid-XR: An Extended-Reality Data Engine for Robotic Manipulation",
        "abstract": "We introduce Lucid-XR, a generative data engine for creating diverse and realistic-looking data to train real-world robot systems. At the core of Lucid-XR is vuer, a web-based physics simulation environment that runs directly on the XR headset, enabling internet-scale access to immersive, latency-free virtual interactions without requiring specialized equipment. The complete system integrates on-device physics simulation with on-device human-to-robot pose retargeting, that are further amplified by a physics-guided video generation pipeline commandable with natural language specifications. We demonstrate zero-shot sim-to-real transfer of robot visual policies, trained entirely on Lucid-XR's synthetic data, across bimanual and dexterous manipulation tasks that involve flexible materials, adhesive interaction between particles, and rigid body contact.",
        "keywords": [
            "mixed reality",
            "extended reality",
            "robot manipulation",
            "simulation",
            "mujoco"
        ],
        "pdf_url": "https://openreview.net/pdf/89ed344226a9d88b30581809990fed4b419d516f.pdf",
        "reviews": [
            {
                "id": "v5T1ohmv6q",
                "forum": "3p7rTnLJM8",
                "replyto": "3p7rTnLJM8",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission858/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068306860,
                "mdate": 1754869474687,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "3p7rTnLJM8",
                "forum": "3p7rTnLJM8",
                "content": {
                    "title": {
                        "value": "Lucid-XR: An Extended-Reality Data Engine for Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yajvan Ravan",
                            "Adam Rashid",
                            "Alan Yu",
                            "Kai McClennen",
                            "Gio Huh",
                            "Kevin Yang",
                            "Zhutian Yang",
                            "Qinxi Yu",
                            "Xiaolong Wang",
                            "Phillip Isola",
                            "Ge Yang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yajvan_Ravan1",
                            "~Adam_Rashid1",
                            "~Alan_Yu2",
                            "~Kai_McClennen1",
                            "~Gio_Huh1",
                            "~Kevin_Yang3",
                            "~Zhutian_Yang1",
                            "~Qinxi_Yu1",
                            "~Xiaolong_Wang3",
                            "~Phillip_Isola1",
                            "~Ge_Yang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "mixed reality",
                            "extended reality",
                            "robot manipulation",
                            "simulation",
                            "mujoco"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a scalable recipe for creating synthetic robot training data via generative AI models and web-based physics simulation in extended reality."
                    },
                    "abstract": {
                        "value": "We introduce Lucid-XR, a generative data engine for creating diverse and realistic-looking data to train real-world robot systems. At the core of Lucid-XR is vuer, a web-based physics simulation environment that runs directly on the XR headset, enabling internet-scale access to immersive, latency-free virtual interactions without requiring specialized equipment. The complete system integrates on-device physics simulation with on-device human-to-robot pose retargeting, that are further amplified by a physics-guided video generation pipeline commandable with natural language specifications. We demonstrate zero-shot sim-to-real transfer of robot visual policies, trained entirely on Lucid-XR's synthetic data, across bimanual and dexterous manipulation tasks that involve flexible materials, adhesive interaction between particles, and rigid body contact."
                    },
                    "supplementary_material": {
                        "value": "/attachment/cebefde973292eb1d726628514efbb0f91e0d5ce.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/89ed344226a9d88b30581809990fed4b419d516f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nravan2025lucidxr,\ntitle={Lucid-{XR}: An Extended-Reality Data Engine for Robotic Manipulation},\nauthor={Yajvan Ravan and Adam Rashid and Alan Yu and Kai McClennen and Gio Huh and Kevin Yang and Zhutian Yang and Qinxi Yu and Xiaolong Wang and Phillip Isola and Ge Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=3p7rTnLJM8}\n}"
                    },
                    "paperhash": {
                        "value": "ravan|lucidxr_an_extendedreality_data_engine_for_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission858/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission858/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745452042248,
                "pdate": 1754680637465,
                "odate": 1758062783004,
                "mdate": 1758062783078,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission858/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission858/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zgVaMD0QjZ",
        "title": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception",
        "abstract": "Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance—properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (< $200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation.",
        "keywords": [
            "Multimodal haptic perception",
            "Robot manipulation",
            "Datasets",
            "Data-acquisition device"
        ],
        "pdf_url": "https://openreview.net/pdf/4961e8ce465a42afdcc50b993b558af101ea2c55.pdf",
        "reviews": [
            {
                "id": "I6XdARGUWs",
                "forum": "zgVaMD0QjZ",
                "replyto": "zgVaMD0QjZ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission848/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068306555,
                "mdate": 1754869474581,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zgVaMD0QjZ",
                "forum": "zgVaMD0QjZ",
                "content": {
                    "title": {
                        "value": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception"
                    },
                    "authors": {
                        "value": [
                            "Pranav N. Thakkar",
                            "Shubhangi Sinha",
                            "Karan Baijal",
                            "Yuhan (Anjelica) Bian",
                            "Leah Lackey",
                            "Ben Dodson",
                            "Heisen Kong",
                            "Jueun Kwon",
                            "Amber Li",
                            "Yifei Hu",
                            "alexios rekoutis",
                            "Tom Silver",
                            "Tapomayukh Bhattacharjee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Pranav_N._Thakkar1",
                            "~Shubhangi_Sinha1",
                            "~Karan_Baijal1",
                            "yb265@cornell.edu",
                            "lml276@cornell.edu",
                            "bzd4@cornell.edu",
                            "hk593@cornell.edu",
                            "~Jueun_Kwon1",
                            "adl94@cornell.edu",
                            "~Yifei_Hu4",
                            "~alexios_rekoutis1",
                            "~Tom_Silver1",
                            "~Tapomayukh_Bhattacharjee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multimodal haptic perception",
                            "Robot manipulation",
                            "Datasets",
                            "Data-acquisition device"
                        ]
                    },
                    "abstract": {
                        "value": "Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance—properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (< $200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/bdc4bd15dbb77155e1c386153da8387244247f2e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4961e8ce465a42afdcc50b993b558af101ea2c55.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nthakkar2025clamp,\ntitle={{CLAMP}: Crowdsourcing a {LA}rge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception},\nauthor={Pranav N. Thakkar and Shubhangi Sinha and Karan Baijal and Yuhan (Anjelica) Bian and Leah Lackey and Ben Dodson and Heisen Kong and Jueun Kwon and Amber Li and Yifei Hu and alexios rekoutis and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zgVaMD0QjZ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/34f96cc8216759fb7a540cde6b5bdfcb84ab3b80.mp4"
                    },
                    "paperhash": {
                        "value": "thakkar|clamp_crowdsourcing_a_largescale_inthewild_haptic_dataset_with_an_opensource_device_for_multimodal_robot_perception"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission848/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission848/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission848/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745450386361,
                "pdate": 1754680637095,
                "odate": 1758062782709,
                "mdate": 1758062823901,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission848/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission848/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "M1e2PEMLp2",
        "title": "Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning",
        "abstract": "In multi-robot systems, fleets are often deployed to collect data that improves the performance of machine learning models for downstream perception and planning. However, real-world robotic deployments generate vast amounts of data across diverse conditions, while only a small portion can be transmitted or labeled due to limited bandwidth, constrained onboard storage, and high annotation costs. To address these challenges, we propose Distributed Upload and Active Labeling (DUAL), a decentralized, two-stage data collection framework for resource-constrained robotic fleets. In the first stage, each robot independently selects a subset of its local observations to upload under storage and communication constraints. In the second stage, the cloud selects a subset of uploaded data to label, subject to a global annotation budget. We evaluate DUAL on classification tasks spanning multiple sensing modalities, as well as on RoadNet—a real-world dataset we collected from vehicle-mounted cameras for time and weather classification. We further validate our approach in a physical experiment using a Franka Emika Panda robot arm, where it learns to move a red cube to a green bowl. Finally, we test DUAL on trajectory prediction using the nuScenes autonomous driving dataset to assess generalization to complex prediction tasks. Across all settings, DUAL consistently outperforms state-of-the-art baselines, achieving up to 31.1\\% gain in classification accuracy and a 13\\% improvement in real-world robotics task completion rates.",
        "keywords": [
            "Fleet Learning",
            "Distributed Data Collection",
            "Submodular Maximization"
        ],
        "pdf_url": "https://openreview.net/pdf/e94bfc7fafbb86a2ace178388c3d571221c3ef12.pdf",
        "reviews": [
            {
                "id": "Z8AEFrv2VG",
                "forum": "M1e2PEMLp2",
                "replyto": "M1e2PEMLp2",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission847/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068306472,
                "mdate": 1754869474574,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "M1e2PEMLp2",
                "forum": "M1e2PEMLp2",
                "content": {
                    "title": {
                        "value": "Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning"
                    },
                    "authors": {
                        "value": [
                            "Oguzhan Akcin",
                            "Harsh Goel",
                            "Ruihan Zhao",
                            "Sandeep P. Chinchali"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Oguzhan_Akcin2",
                            "~Harsh_Goel1",
                            "~Ruihan_Zhao1",
                            "~Sandeep_P._Chinchali1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Fleet Learning",
                            "Distributed Data Collection",
                            "Submodular Maximization"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a two-stage data collection framework for robotic fleets that selects what to upload and label, with provable guarantees and real-world deployment results."
                    },
                    "abstract": {
                        "value": "In multi-robot systems, fleets are often deployed to collect data that improves the performance of machine learning models for downstream perception and planning. However, real-world robotic deployments generate vast amounts of data across diverse conditions, while only a small portion can be transmitted or labeled due to limited bandwidth, constrained onboard storage, and high annotation costs. To address these challenges, we propose Distributed Upload and Active Labeling (DUAL), a decentralized, two-stage data collection framework for resource-constrained robotic fleets. In the first stage, each robot independently selects a subset of its local observations to upload under storage and communication constraints. In the second stage, the cloud selects a subset of uploaded data to label, subject to a global annotation budget. We evaluate DUAL on classification tasks spanning multiple sensing modalities, as well as on RoadNet—a real-world dataset we collected from vehicle-mounted cameras for time and weather classification. We further validate our approach in a physical experiment using a Franka Emika Panda robot arm, where it learns to move a red cube to a green bowl. Finally, we test DUAL on trajectory prediction using the nuScenes autonomous driving dataset to assess generalization to complex prediction tasks. Across all settings, DUAL consistently outperforms state-of-the-art baselines, achieving up to 31.1\\% gain in classification accuracy and a 13\\% improvement in real-world robotics task completion rates."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/e94bfc7fafbb86a2ace178388c3d571221c3ef12.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nakcin2025distributed,\ntitle={Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning},\nauthor={Oguzhan Akcin and Harsh Goel and Ruihan Zhao and Sandeep P. Chinchali},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=M1e2PEMLp2}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/555abc827d51c349f98129d3fadab2724ee98198.zip"
                    },
                    "paperhash": {
                        "value": "akcin|distributed_upload_and_active_labeling_for_resourceconstrained_fleet_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission847/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission847/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission847/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745450331064,
                "pdate": 1754680637089,
                "odate": 1758062782707,
                "mdate": 1758062823879,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission847/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "3CnxNqmklv",
        "title": "DreamGen: Unlocking Generalization in Robot Learning through Video World Models",
        "abstract": "In this work, we unlock new capabilities in robot learning from neural trajectories, synthetic robot data generated from video world models. Our proposed recipe is simple, but powerful: we take the most recent state-of-the-art video generative models (world models), adapt them to the target robot embodiment, and generate new, synthetic robot data of the same task or even new behaviors. Since these video world models only generate videos, we explore two techniques of getting robot actions: extracting latent actions from a general-purpose latent action model and getting predicted actions from an inverse-dynamics model (IDM), giving flexibility across diverse scenarios. Our proposed approach unlocks behavior and environment generalization, allowing a humanoid robot to perform 20+ new behaviors in unseen environments while only collecting teleoperation data for pick and place in a single environment. By introducing a new world modeling benchmark, we demonstrate that stronger video world models directly correlate with improved downstream robot policy performance. This establishes a new scaling dimension beyond simply collecting additional teleoperation data, changing how we approach robot learning.",
        "keywords": [
            "Video World Models",
            "Synthetic Data",
            "Behavior Generalization",
            "Environment Generalization"
        ],
        "pdf_url": "https://openreview.net/pdf/943bc3995d991c081f37708f41b566c224b01f5d.pdf",
        "reviews": [
            {
                "id": "krr7YVvsc7",
                "forum": "3CnxNqmklv",
                "replyto": "3CnxNqmklv",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission836/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068305968,
                "mdate": 1754869474506,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "3CnxNqmklv",
                "forum": "3CnxNqmklv",
                "content": {
                    "title": {
                        "value": "DreamGen: Unlocking Generalization in Robot Learning through Video World Models"
                    },
                    "authors": {
                        "value": [
                            "Joel Jang",
                            "Seonghyeon Ye",
                            "Zongyu Lin",
                            "Jiannan Xiang",
                            "Johan Bjorck",
                            "Yu Fang",
                            "Fengyuan Hu",
                            "Spencer Huang",
                            "Kaushil Kundalia",
                            "Yen-Chen Lin",
                            "Loïc Magne",
                            "Ajay Mandlekar",
                            "Avnish Narayan",
                            "You Liang Tan",
                            "Guanzhi Wang",
                            "Jing Wang",
                            "Qi Wang",
                            "Yinzhen Xu",
                            "Xiaohui Zeng",
                            "Kaiyuan Zheng",
                            "Ruijie Zheng",
                            "Ming-Yu Liu",
                            "Luke Zettlemoyer",
                            "Dieter Fox",
                            "Jan Kautz",
                            "Scott Reed",
                            "Yuke Zhu",
                            "Linxi Fan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Joel_Jang1",
                            "~Seonghyeon_Ye1",
                            "~Zongyu_Lin1",
                            "~Jiannan_Xiang1",
                            "~Johan_Bjorck2",
                            "~Yu_Fang2",
                            "~Fengyuan_Hu2",
                            "spencerh@nvidia.com",
                            "~Kaushil_Kundalia1",
                            "~Yen-Chen_Lin1",
                            "~Loïc_Magne1",
                            "~Ajay_Mandlekar1",
                            "avnishn@nvidia.com",
                            "~You_Liang_Tan1",
                            "~Guanzhi_Wang1",
                            "~Jing_Wang79",
                            "qiwang@nvidia.com",
                            "~Yinzhen_Xu1",
                            "~Xiaohui_Zeng2",
                            "~Kaiyuan_Zheng1",
                            "~Ruijie_Zheng1",
                            "~Ming-Yu_Liu1",
                            "~Luke_Zettlemoyer1",
                            "~Dieter_Fox1",
                            "~Jan_Kautz1",
                            "~Scott_Reed1",
                            "~Yuke_Zhu1",
                            "~Linxi_Fan2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Video World Models",
                            "Synthetic Data",
                            "Behavior Generalization",
                            "Environment Generalization"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose neural trajectories, a new pipeline that enables augmenting robot training data, enabling robots to perform totally new actions in unseen environments."
                    },
                    "abstract": {
                        "value": "In this work, we unlock new capabilities in robot learning from neural trajectories, synthetic robot data generated from video world models. Our proposed recipe is simple, but powerful: we take the most recent state-of-the-art video generative models (world models), adapt them to the target robot embodiment, and generate new, synthetic robot data of the same task or even new behaviors. Since these video world models only generate videos, we explore two techniques of getting robot actions: extracting latent actions from a general-purpose latent action model and getting predicted actions from an inverse-dynamics model (IDM), giving flexibility across diverse scenarios. Our proposed approach unlocks behavior and environment generalization, allowing a humanoid robot to perform 20+ new behaviors in unseen environments while only collecting teleoperation data for pick and place in a single environment. By introducing a new world modeling benchmark, we demonstrate that stronger video world models directly correlate with improved downstream robot policy performance. This establishes a new scaling dimension beyond simply collecting additional teleoperation data, changing how we approach robot learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2682e8bf8a23c00c76e45c4c6d7f34f80cb0163f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/943bc3995d991c081f37708f41b566c224b01f5d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njang2025dreamgen,\ntitle={DreamGen: Unlocking Generalization in Robot Learning through Video World Models},\nauthor={Joel Jang and Seonghyeon Ye and Zongyu Lin and Jiannan Xiang and Johan Bjorck and Yu Fang and Fengyuan Hu and Spencer Huang and Kaushil Kundalia and Yen-Chen Lin and Lo{\\\"\\i}c Magne and Ajay Mandlekar and Avnish Narayan and You Liang Tan and Guanzhi Wang and Jing Wang and Qi Wang and Yinzhen Xu and Xiaohui Zeng and Kaiyuan Zheng and Ruijie Zheng and Ming-Yu Liu and Luke Zettlemoyer and Dieter Fox and Jan Kautz and Scott Reed and Yuke Zhu and Linxi Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=3CnxNqmklv}\n}"
                    },
                    "paperhash": {
                        "value": "jang|dreamgen_unlocking_generalization_in_robot_learning_through_video_world_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission836/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission836/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission836/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745448248039,
                "pdate": 1754680636727,
                "odate": 1758062782420,
                "mdate": 1758062823841,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission836/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission836/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "XaJkbK02Vm",
        "title": "LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing",
        "abstract": "Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they struggle with dynamic object interactions, where contact must be precisely sensed and controlled. To bridge this gap, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a particularly challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting or fastening mechanisms to maintain stability. For efficient large-area tactile sensing, we design a high-density distributed tactile sensor that covers the entire back of the robot. To effectively leverage tactile feedback for robot control, we develop a simulation environment with high-fidelity tactile signals, and train tactile-aware transport policies using a two-stage learning pipeline. Furthermore, we design a novel reward function to promote robust, symmetric, and frequency-adaptive locomotion gaits. After training in simulation, LocoTouch transfers zero-shot to the real world, reliably transporting a wide range of unsecured cylindrical objects with diverse sizes, weights, and surface properties. Moreover, it remains robust over long distances, on uneven terrain, and under severe perturbations.",
        "keywords": [
            "Tactile Quadrupedal Policy",
            "Tactile Sim-to-Real",
            "Legged Robots"
        ],
        "pdf_url": "https://openreview.net/pdf/052921b02d4c6aac0aa007d5e2bb62386ee212c9.pdf",
        "reviews": [
            {
                "id": "5f7W4KW8ca",
                "forum": "XaJkbK02Vm",
                "replyto": "XaJkbK02Vm",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission835/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068305959,
                "mdate": 1754869474361,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "XaJkbK02Vm",
                "forum": "XaJkbK02Vm",
                "content": {
                    "title": {
                        "value": "LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing"
                    },
                    "authors": {
                        "value": [
                            "Changyi Lin",
                            "Yuxin Ray Song",
                            "Boda Huo",
                            "Mingyang Yu",
                            "Yikai Wang",
                            "Shiqi Liu",
                            "Yuxiang Yang",
                            "Wenhao Yu",
                            "Tingnan Zhang",
                            "Jie Tan",
                            "Yiyue Luo",
                            "Ding Zhao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Changyi_Lin1",
                            "~Yuxin_Ray_Song1",
                            "~Boda_Huo1",
                            "~Mingyang_Yu4",
                            "~Yikai_Wang4",
                            "~Shiqi_Liu2",
                            "~Yuxiang_Yang2",
                            "~Wenhao_Yu1",
                            "~Tingnan_Zhang1",
                            "~Jie_Tan1",
                            "~Yiyue_Luo1",
                            "~Ding_Zhao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Quadrupedal Policy",
                            "Tactile Sim-to-Real",
                            "Legged Robots"
                        ]
                    },
                    "abstract": {
                        "value": "Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they struggle with dynamic object interactions, where contact must be precisely sensed and controlled. To bridge this gap, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a particularly challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting or fastening mechanisms to maintain stability. For efficient large-area tactile sensing, we design a high-density distributed tactile sensor that covers the entire back of the robot. To effectively leverage tactile feedback for robot control, we develop a simulation environment with high-fidelity tactile signals, and train tactile-aware transport policies using a two-stage learning pipeline. Furthermore, we design a novel reward function to promote robust, symmetric, and frequency-adaptive locomotion gaits. After training in simulation, LocoTouch transfers zero-shot to the real world, reliably transporting a wide range of unsecured cylindrical objects with diverse sizes, weights, and surface properties. Moreover, it remains robust over long distances, on uneven terrain, and under severe perturbations."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/052921b02d4c6aac0aa007d5e2bb62386ee212c9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlin2025locotouch,\ntitle={LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing},\nauthor={Changyi Lin and Yuxin Ray Song and Boda Huo and Mingyang Yu and Yikai Wang and Shiqi Liu and Yuxiang Yang and Wenhao Yu and Tingnan Zhang and Jie Tan and Yiyue Luo and Ding Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XaJkbK02Vm}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b2dba10c43d9d6b8ad3acf66f01dc699fab3c41a.mp4"
                    },
                    "paperhash": {
                        "value": "lin|locotouch_learning_dynamic_quadrupedal_transport_with_tactile_sensing"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission835/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission835/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission835/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745448228945,
                "pdate": 1754680636668,
                "odate": 1758062782386,
                "mdate": 1758062823676,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission835/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "up2ZpQZbeb",
        "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies",
        "abstract": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks that integrates a cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism for accurate tracking of multiple cutting pieces; (2) a comprehensive reward design that combines this topology discovery with a pose-invariant spectral reward model based on Laplace–Beltrami eigenanalysis, enabling consistent and robust assessment of cutting quality; and (3) an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP—Particle-based Score-Entropy Discrete Diffusion Policy—for goal-conditioned policy learning. Extensive experiments demonstrate that TopoCut enables trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals.",
        "keywords": [
            "Deformable object manipulation",
            "multi-step cutting",
            "topology tracking",
            "spectral reward",
            "perception",
            "discrete diffusion policy"
        ],
        "pdf_url": "https://openreview.net/pdf/22724b9b78190a8b3af974b34082e6abd714fe76.pdf",
        "reviews": [
            {
                "id": "x63Pb6l9dT",
                "forum": "up2ZpQZbeb",
                "replyto": "up2ZpQZbeb",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission831/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068305891,
                "mdate": 1754869474316,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "up2ZpQZbeb",
                "forum": "up2ZpQZbeb",
                "content": {
                    "title": {
                        "value": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies"
                    },
                    "authors": {
                        "value": [
                            "Liquan Wang",
                            "Jiangjie Bian",
                            "Eric Heiden",
                            "Animesh Garg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Liquan_Wang2",
                            "~Jiangjie_Bian1",
                            "~Eric_Heiden1",
                            "~Animesh_Garg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Deformable object manipulation",
                            "multi-step cutting",
                            "topology tracking",
                            "spectral reward",
                            "perception",
                            "discrete diffusion policy"
                        ]
                    },
                    "TLDR": {
                        "value": "We present TopoCut, a unified framework for learning goal-conditioned multi-step cutting of deformable objects, combining spectral rewards for precise evaluation with a discrete diffusion policy for stable and generalizable control."
                    },
                    "abstract": {
                        "value": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks that integrates a cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism for accurate tracking of multiple cutting pieces; (2) a comprehensive reward design that combines this topology discovery with a pose-invariant spectral reward model based on Laplace–Beltrami eigenanalysis, enabling consistent and robust assessment of cutting quality; and (3) an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP—Particle-based Score-Entropy Discrete Diffusion Policy—for goal-conditioned policy learning. Extensive experiments demonstrate that TopoCut enables trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals."
                    },
                    "supplementary_material": {
                        "value": "/attachment/df574d3cbcdf2acf6b81e8c1014b2cc2ff32711f.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/22724b9b78190a8b3af974b34082e6abd714fe76.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025topocut,\ntitle={TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies},\nauthor={Liquan Wang and Jiangjie Bian and Eric Heiden and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=up2ZpQZbeb}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0bb28b9405c643fc16f20fc8ba26ca0700520740.mp4"
                    },
                    "paperhash": {
                        "value": "wang|topocut_learning_multistep_cutting_with_spectral_rewards_and_discrete_diffusion_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission831/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission831/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission831/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745448024066,
                "pdate": 1754680636558,
                "odate": 1758062782290,
                "mdate": 1758062823657,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission831/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "KXzkAje2uQ",
        "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
        "abstract": "Active object localization remains a critical challenge for robots, requiring efficient\nexploration of partially observable environments. However, state-of-the-art robot\npolicies either struggle to generalize beyond demonstration datasets (e.g., imitation\nlearning methods) or fail to generate physically grounded actions (e.g., VLMs).\nTo address these limitations, we introduce WoMAP (World Models for Active\nPerception): a recipe for training open-vocabulary object localization policies that:\n(i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data\ngeneration without the need for expert demonstrations, (ii) distills dense rewards\nsignals from open-vocabulary object detectors, and (iii) leverages a latent world\nmodel for dynamics and rewards prediction to ground high-level action proposals\nat inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a wide range of zero-shot object localization tasks, with a 63\\% success rate compared to 10\\%success rate compared to a VLM baseline, and only a 10 - 20\\% drop in performance when directly transferring from sim to real.",
        "keywords": [
            "Active Perception",
            "World Models",
            "Object Localization"
        ],
        "pdf_url": "https://openreview.net/pdf/eb7bd99a3251b5113eeb03bdaeca671625a3536a.pdf",
        "reviews": [
            {
                "id": "iZnQBaD7hz",
                "forum": "KXzkAje2uQ",
                "replyto": "KXzkAje2uQ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission829/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068305760,
                "mdate": 1754869474250,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "KXzkAje2uQ",
                "forum": "KXzkAje2uQ",
                "content": {
                    "title": {
                        "value": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization"
                    },
                    "authors": {
                        "value": [
                            "Tenny Yin",
                            "Zhiting Mei",
                            "Tao Sun",
                            "Ola Sho",
                            "Anirudha Majumdar",
                            "Emily Zhou",
                            "Jeremy Bao",
                            "Miyu Yamane",
                            "Lihan Zha"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tenny_Yin1",
                            "~Zhiting_Mei1",
                            "~Tao_Sun25",
                            "~Ola_Sho1",
                            "~Anirudha_Majumdar1",
                            "~Emily_Zhou2",
                            "~Jeremy_Bao1",
                            "~Miyu_Yamane1",
                            "~Lihan_Zha1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Active Perception",
                            "World Models",
                            "Object Localization"
                        ]
                    },
                    "TLDR": {
                        "value": "We introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that are grounded in the physical world."
                    },
                    "abstract": {
                        "value": "Active object localization remains a critical challenge for robots, requiring efficient\nexploration of partially observable environments. However, state-of-the-art robot\npolicies either struggle to generalize beyond demonstration datasets (e.g., imitation\nlearning methods) or fail to generate physically grounded actions (e.g., VLMs).\nTo address these limitations, we introduce WoMAP (World Models for Active\nPerception): a recipe for training open-vocabulary object localization policies that:\n(i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data\ngeneration without the need for expert demonstrations, (ii) distills dense rewards\nsignals from open-vocabulary object detectors, and (iii) leverages a latent world\nmodel for dynamics and rewards prediction to ground high-level action proposals\nat inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a wide range of zero-shot object localization tasks, with a 63\\% success rate compared to 10\\%success rate compared to a VLM baseline, and only a 10 - 20\\% drop in performance when directly transferring from sim to real."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/eb7bd99a3251b5113eeb03bdaeca671625a3536a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyin2025womap,\ntitle={Wo{MAP}: World Models For Embodied Open-Vocabulary Object Localization},\nauthor={Tenny Yin and Zhiting Mei and Tao Sun and Ola Sho and Anirudha Majumdar and Emily Zhou and Jeremy Bao and Miyu Yamane and Lihan Zha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KXzkAje2uQ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/bb869f0ad56a41e027196f51a1c7c35020fb55ed.mp4"
                    },
                    "paperhash": {
                        "value": "yin|womap_world_models_for_embodied_openvocabulary_object_localization"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission829/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission829/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission829/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745447716335,
                "pdate": 1754680636555,
                "odate": 1758062782228,
                "mdate": 1758062823619,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission829/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission829/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "cUeY476ohd",
        "title": "MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs",
        "abstract": "Image-based behaviour cloning leverages demonstrations captured from ubiquitous RGB cameras, enabling impressive visuomotor performance. However, it remains constrained by the cost of collecting sufficiently diverse demonstrations, especially for generalizing across workspace variations. We propose MirrorDuo, a mirroring-based formulation that operates on image, proprioception, and full 6-DoF end-effector action tuples, generating a mirrored counterpart for each original demonstration, effectively achieving ``collect one, get one for free.\" It can be applied as a data augmentation strategy for existing learning pipelines, such as standard behaviour cloning or diffusion policy, or as a structural prior for reflection-equivariant policy networks. By leveraging the overlap between the original and mirrored domains, MirrorDuo achieves significantly improved performance under the same data budget when demonstrations are evenly distributed across both sides of the workspace. When demonstrations are confined to one side, MirrorDuo enables efficient skill transfer to the mirrored workspace with as few as zero or just 5 demonstrations in the target arrangement.",
        "keywords": [
            "Behavior Cloning",
            "Data Efficiency",
            "Robotic Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/c9df128e0cbb8f0f2d511028317205ec9c07e38c.pdf",
        "reviews": [
            {
                "id": "fc3IDxAtfS",
                "forum": "cUeY476ohd",
                "replyto": "cUeY476ohd",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission823/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068305510,
                "mdate": 1754869474183,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "cUeY476ohd",
                "forum": "cUeY476ohd",
                "content": {
                    "title": {
                        "value": "MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs"
                    },
                    "authors": {
                        "value": [
                            "Zheyu Zhuang",
                            "Ruiyu Wang",
                            "Giovanni Luca Marchetti",
                            "Florian T. Pokorny",
                            "Danica Kragic"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zheyu_Zhuang1",
                            "~Ruiyu_Wang3",
                            "~Giovanni_Luca_Marchetti1",
                            "~Florian_T._Pokorny1",
                            "~Danica_Kragic1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Behavior Cloning",
                            "Data Efficiency",
                            "Robotic Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "MirrorDuo is a mirroring-based formulation for image-based behaviour cloning that generates paired demonstrations to improve data efficiency, boost low-data performance, and enable skill transfer to mirrored scenes with minimal degradation."
                    },
                    "abstract": {
                        "value": "Image-based behaviour cloning leverages demonstrations captured from ubiquitous RGB cameras, enabling impressive visuomotor performance. However, it remains constrained by the cost of collecting sufficiently diverse demonstrations, especially for generalizing across workspace variations. We propose MirrorDuo, a mirroring-based formulation that operates on image, proprioception, and full 6-DoF end-effector action tuples, generating a mirrored counterpart for each original demonstration, effectively achieving ``collect one, get one for free.\" It can be applied as a data augmentation strategy for existing learning pipelines, such as standard behaviour cloning or diffusion policy, or as a structural prior for reflection-equivariant policy networks. By leveraging the overlap between the original and mirrored domains, MirrorDuo achieves significantly improved performance under the same data budget when demonstrations are evenly distributed across both sides of the workspace. When demonstrations are confined to one side, MirrorDuo enables efficient skill transfer to the mirrored workspace with as few as zero or just 5 demonstrations in the target arrangement."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7ac21d454036d4db519e1b88169c21e05cfa48a0.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c9df128e0cbb8f0f2d511028317205ec9c07e38c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhuang2025mirrorduo,\ntitle={MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs},\nauthor={Zheyu Zhuang and Ruiyu Wang and Giovanni Luca Marchetti and Florian T. Pokorny and Danica Kragic},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=cUeY476ohd}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/7c7cc44dafa0a9a6091dc36c11da07f09f771954.mp4"
                    },
                    "paperhash": {
                        "value": "zhuang|mirrorduo_reflectionconsistent_visuomotor_learning_from_mirrored_demonstration_pairs"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission823/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission823/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission823/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745447102621,
                "pdate": 1754680636306,
                "odate": 1758062782021,
                "mdate": 1758062823383,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission823/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "2dXMfk3qRU",
        "title": "First Order Model-Based RL through Decoupled Backpropagation",
        "abstract": "There is growing interest in reinforcement learning (RL) methods that leverage the simulator's derivatives to improve learning efficiency. While early gradient-based approaches have demonstrated superior performance compared to derivative-free methods, accessing simulator gradients is often impractical due to their implementation cost or unavailability. Model-based RL (MBRL) can approximate these gradients via learned dynamics models, but the solver efficiency suffers from compounding prediction errors during training rollouts, which can degrade policy performance. We propose an approach that decouples trajectory generation from gradient computation: trajectories are unrolled using a simulator, while gradients are computed via backpropagation through a learned differentiable model of the simulator. This hybrid design enables efficient and consistent first-order policy optimization, even when simulator gradients are unavailable, as well as learning a critic from simulation rollouts, which is more accurate. Our method achieves the sample efficiency and speed of specialized optimizers such as SHAC, while maintaining the generality of standard approaches like PPO and avoiding ill behaviors observed in other first-order MBRL methods. We empirically validate our algorithm on benchmark control tasks and demonstrate its effectiveness on a real Go2 quadruped robot, across both quadrupedal and bipedal locomotion tasks.",
        "keywords": [
            "Model-Based Reinforcement Learning",
            "Quadruped Locomotion",
            "Sim-to-Real Transfer"
        ],
        "pdf_url": "https://openreview.net/pdf/0bc5a5a592d3faeb7e8c4b48b5ba93354ba79563.pdf",
        "reviews": [
            {
                "id": "gl057U5Gqr",
                "forum": "2dXMfk3qRU",
                "replyto": "2dXMfk3qRU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission812/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068304830,
                "mdate": 1754869474028,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "2dXMfk3qRU",
                "forum": "2dXMfk3qRU",
                "content": {
                    "title": {
                        "value": "First Order Model-Based RL through Decoupled Backpropagation"
                    },
                    "authors": {
                        "value": [
                            "Joseph Amigo",
                            "Rooholla Khorrambakht",
                            "Elliot Chane-Sane",
                            "Nicolas Mansard",
                            "Ludovic Righetti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Joseph_Amigo1",
                            "~Rooholla_Khorrambakht1",
                            "~Elliot_Chane-Sane1",
                            "~Nicolas_Mansard1",
                            "~Ludovic_Righetti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Model-Based Reinforcement Learning",
                            "Quadruped Locomotion",
                            "Sim-to-Real Transfer"
                        ]
                    },
                    "TLDR": {
                        "value": "We introduce a model-based reinforcement learning method that decouples trajectory generation and gradient computation, enabling more sample efficient policy learning and demonstrating successful real-world deployment on a quadruped robot."
                    },
                    "abstract": {
                        "value": "There is growing interest in reinforcement learning (RL) methods that leverage the simulator's derivatives to improve learning efficiency. While early gradient-based approaches have demonstrated superior performance compared to derivative-free methods, accessing simulator gradients is often impractical due to their implementation cost or unavailability. Model-based RL (MBRL) can approximate these gradients via learned dynamics models, but the solver efficiency suffers from compounding prediction errors during training rollouts, which can degrade policy performance. We propose an approach that decouples trajectory generation from gradient computation: trajectories are unrolled using a simulator, while gradients are computed via backpropagation through a learned differentiable model of the simulator. This hybrid design enables efficient and consistent first-order policy optimization, even when simulator gradients are unavailable, as well as learning a critic from simulation rollouts, which is more accurate. Our method achieves the sample efficiency and speed of specialized optimizers such as SHAC, while maintaining the generality of standard approaches like PPO and avoiding ill behaviors observed in other first-order MBRL methods. We empirically validate our algorithm on benchmark control tasks and demonstrate its effectiveness on a real Go2 quadruped robot, across both quadrupedal and bipedal locomotion tasks."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0bc5a5a592d3faeb7e8c4b48b5ba93354ba79563.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\namigo2025first,\ntitle={First Order Model-Based {RL} through Decoupled Backpropagation},\nauthor={Joseph Amigo and Rooholla Khorrambakht and Elliot Chane-Sane and Nicolas Mansard and Ludovic Righetti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2dXMfk3qRU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d05c56d0e9180af41a9d4af1411d8e04e3f672f3.zip"
                    },
                    "paperhash": {
                        "value": "amigo|first_order_modelbased_rl_through_decoupled_backpropagation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission812/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission812/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission812/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745445163270,
                "pdate": 1754680635902,
                "odate": 1758062781675,
                "mdate": 1758062823351,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission812/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission812/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9f3klkpa4y",
        "title": "CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation",
        "abstract": "Recent advances in Behavior Cloning (BC) have led to strong performance in robotic manipulation, driven by expressive models, sequence modeling of actions, and large-scale demonstration data. However, BC faces significant challenges when applied to heterogeneous datasets, such as visual shift with different camera poses or object appearances, where performance degrades despite the benefits of learning at scale. This stems from BC's tendency to overfit individual demonstrations rather than capture shared structure, limiting generalization. To address this, we introduce Contrastive Learning via Action Sequence Supervision (CLASS), a method for learning behavioral representations from demonstrations using supervised contrastive learning. CLASS leverages weak supervision from similar action sequences identified via Dynamic Time Warping (DTW) and optimizes a soft InfoNCE loss with similarity-weighted positive pairs. We evaluate CLASS on 5 simulation benchmarks and 3 real-world tasks to achieve competitive results using retrieval-based control with representations only. Most notably, for downstream policy learning under significant visual shifts, CLASS achieves an average success rate of 70% with Diffusion Policy, while all other baseline methods fail to perform competitively.",
        "keywords": [
            "Supervised Contrastive Learning",
            "Imitation Learning",
            "Robot Manipulation",
            "Action Chunking"
        ],
        "pdf_url": "https://openreview.net/pdf/f05ddb08cf77b3c7cebcfbf387fabb668068d6ed.pdf",
        "reviews": [
            {
                "id": "c2IQull0lj",
                "forum": "9f3klkpa4y",
                "replyto": "9f3klkpa4y",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission803/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068304576,
                "mdate": 1754869473977,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9f3klkpa4y",
                "forum": "9f3klkpa4y",
                "content": {
                    "title": {
                        "value": "CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Sung-Wook Lee",
                            "Xuhui Kang",
                            "Brandon Y. Yang",
                            "Yen-Ling Kuo"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sung-Wook_Lee1",
                            "~Xuhui_Kang1",
                            "~Brandon_Y._Yang1",
                            "~Yen-Ling_Kuo1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Supervised Contrastive Learning",
                            "Imitation Learning",
                            "Robot Manipulation",
                            "Action Chunking"
                        ]
                    },
                    "TLDR": {
                        "value": "CLASS improves Behavior Cloning by using DTW-based soft contrastive learning to learn robust action representations, boosting generalization across visual shifts in both simulated and real robotic tasks."
                    },
                    "abstract": {
                        "value": "Recent advances in Behavior Cloning (BC) have led to strong performance in robotic manipulation, driven by expressive models, sequence modeling of actions, and large-scale demonstration data. However, BC faces significant challenges when applied to heterogeneous datasets, such as visual shift with different camera poses or object appearances, where performance degrades despite the benefits of learning at scale. This stems from BC's tendency to overfit individual demonstrations rather than capture shared structure, limiting generalization. To address this, we introduce Contrastive Learning via Action Sequence Supervision (CLASS), a method for learning behavioral representations from demonstrations using supervised contrastive learning. CLASS leverages weak supervision from similar action sequences identified via Dynamic Time Warping (DTW) and optimizes a soft InfoNCE loss with similarity-weighted positive pairs. We evaluate CLASS on 5 simulation benchmarks and 3 real-world tasks to achieve competitive results using retrieval-based control with representations only. Most notably, for downstream policy learning under significant visual shifts, CLASS achieves an average success rate of 70% with Diffusion Policy, while all other baseline methods fail to perform competitively."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e2eaabefe9cb350c33f47c635c2ec395a9b3711b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f05ddb08cf77b3c7cebcfbf387fabb668068d6ed.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlee2025class,\ntitle={{CLASS}: Contrastive Learning via Action Sequence Supervision for Robot Manipulation},\nauthor={Sung-Wook Lee and Xuhui Kang and Brandon Y. Yang and Yen-Ling Kuo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9f3klkpa4y}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/917251cbde43580471556034155474a1d3dab716.mp4"
                    },
                    "paperhash": {
                        "value": "lee|class_contrastive_learning_via_action_sequence_supervision_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission803/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission803/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission803/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745444392596,
                "pdate": 1754680635854,
                "odate": 1758062781569,
                "mdate": 1758062823367,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission803/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "J1Ekhe08QU",
        "title": "Articulated Object Estimation in the Wild",
        "abstract": "Understanding the 3D motion of articulated objects is essential in robotic scene understanding, mobile manipulation, and motion planning. Prior methods for articulation estimation have primarily focused on controlled settings, assuming either fixed camera viewpoints or direct observations of various object states, which tend to fail in more realistic, unconstrained environments. In contrast, humans effortlessly infer articulation modes by watching others manipulating objects. Inspired by this, we introduce ArtiPoint, a novel estimation framework capable of inferring articulated object models under dynamic camera motion and partial observability. By combining deep point tracking with a factor graph optimization framework, ArtiPoint robustly estimates articulated part trajectories and articulation axes directly from raw RGB-D videos. To foster future research in this domain, we introduce Arti4D, the first ego-centric in-the-wild dataset capturing articulated object interactions at a scene level, accompanied with articulation labels and ground truth camera poses. We benchmark ArtiPoint against a range of classical and modern deep learning baselines, demonstrating its superior performance on Arti4D. We make our code and Arti4D publicly available at redacted-for-review.",
        "keywords": [
            "Articulated Object Estimation",
            "3D Scene Understanding",
            "Interactive Perception"
        ],
        "pdf_url": "https://openreview.net/pdf/d623411930455ead0f13f7b12f0702eb95b51448.pdf",
        "reviews": [
            {
                "id": "u5vESz0AHb",
                "forum": "J1Ekhe08QU",
                "replyto": "J1Ekhe08QU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission792/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068304254,
                "mdate": 1754869473917,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "J1Ekhe08QU",
                "forum": "J1Ekhe08QU",
                "content": {
                    "title": {
                        "value": "Articulated Object Estimation in the Wild"
                    },
                    "authors": {
                        "value": [
                            "Abdelrhman Werby",
                            "Martin Büchner",
                            "Adrian Röfer",
                            "Chenguang Huang",
                            "Wolfram Burgard",
                            "Abhinav Valada"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Abdelrhman_Werby1",
                            "~Martin_Büchner1",
                            "~Adrian_Röfer1",
                            "~Chenguang_Huang1",
                            "~Wolfram_Burgard3",
                            "~Abhinav_Valada1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Articulated Object Estimation",
                            "3D Scene Understanding",
                            "Interactive Perception"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a novel method that estimates the motion model of articulated objects using point trajectories under partial or imperfect views of target objects given ego-centric RGB-D videos that we publish as the Arti4D dataset."
                    },
                    "abstract": {
                        "value": "Understanding the 3D motion of articulated objects is essential in robotic scene understanding, mobile manipulation, and motion planning. Prior methods for articulation estimation have primarily focused on controlled settings, assuming either fixed camera viewpoints or direct observations of various object states, which tend to fail in more realistic, unconstrained environments. In contrast, humans effortlessly infer articulation modes by watching others manipulating objects. Inspired by this, we introduce ArtiPoint, a novel estimation framework capable of inferring articulated object models under dynamic camera motion and partial observability. By combining deep point tracking with a factor graph optimization framework, ArtiPoint robustly estimates articulated part trajectories and articulation axes directly from raw RGB-D videos. To foster future research in this domain, we introduce Arti4D, the first ego-centric in-the-wild dataset capturing articulated object interactions at a scene level, accompanied with articulation labels and ground truth camera poses. We benchmark ArtiPoint against a range of classical and modern deep learning baselines, demonstrating its superior performance on Arti4D. We make our code and Arti4D publicly available at redacted-for-review."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a4cb98bb365673d30928ac5813c4c9ff14caf38b.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d623411930455ead0f13f7b12f0702eb95b51448.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwerby2025articulated,\ntitle={Articulated Object Estimation in the Wild},\nauthor={Abdelrhman Werby and Martin B{\\\"u}chner and Adrian R{\\\"o}fer and Chenguang Huang and Wolfram Burgard and Abhinav Valada},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=J1Ekhe08QU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/96c4cfb4955745f6d860db3e70fc7f4664e9de86.zip"
                    },
                    "paperhash": {
                        "value": "werby|articulated_object_estimation_in_the_wild"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission792/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission792/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission792/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745443125758,
                "pdate": 1754680635524,
                "odate": 1758062781334,
                "mdate": 1758062823038,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission792/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "r29CIl3ePP",
        "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
        "abstract": "Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
        "keywords": [
            "Generalized Traveling Salesman Problem",
            "Robotic Task Planning",
            "Multimodal Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/5e5cc091a369d23e202c12f31c631fd34b88bcb9.pdf",
        "reviews": [
            {
                "id": "W40AXyf2gc",
                "forum": "r29CIl3ePP",
                "replyto": "r29CIl3ePP",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission791/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068303997,
                "mdate": 1754869473809,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "r29CIl3ePP",
                "forum": "r29CIl3ePP",
                "content": {
                    "title": {
                        "value": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning"
                    },
                    "authors": {
                        "value": [
                            "Jiaqi Cheng",
                            "Mingfeng Fan",
                            "Xuefeng Zhang",
                            "Jingsong Liang",
                            "Yuhong Cao",
                            "Guohua Wu",
                            "Guillaume Adrien Sartoretti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiaqi_Cheng1",
                            "~Mingfeng_Fan2",
                            "zhang.xuefeng@u.nus.edu",
                            "~Jingsong_Liang1",
                            "~Yuhong_Cao1",
                            "guohuawu@csu.edu.cn",
                            "~Guillaume_Adrien_Sartoretti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Generalized Traveling Salesman Problem",
                            "Robotic Task Planning",
                            "Multimodal Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a07dc389264b3b18cae94acb82c301b88c7c223d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5e5cc091a369d23e202c12f31c631fd34b88bcb9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ncheng2025multimodal,\ntitle={Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning},\nauthor={Jiaqi Cheng and Mingfeng Fan and Xuefeng Zhang and Jingsong Liang and Yuhong Cao and Guohua Wu and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=r29CIl3ePP}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1c53feb5bd9be838a5b8b7a6944550eefd8fb9f0.zip"
                    },
                    "paperhash": {
                        "value": "cheng|multimodal_fused_learning_for_solving_the_generalized_traveling_salesman_problem_in_robotic_task_planning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission791/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission791/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission791/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745443116425,
                "pdate": 1754680635523,
                "odate": 1758062781301,
                "mdate": 1758062823048,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission791/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "gyihSZwQbR",
        "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration",
        "abstract": "Hand–object motion-capture (MoCap) repositories provide abundant, contact-rich human demonstrations for scaling dexterous manipulation on robots. Yet demonstration inaccuracy and embodiment gaps between human and robot hands challenge direct policy learning. Existing pipelines adapt a three-stage workflow: retargeting, tracking, and residual correction. This multi-step process may not fully utilize demonstrations and can introduce compound errors. We introduce Reference-Scoped Exploration (RSE), a unified, single-loop optimization that integrates retargeting and tracking to train a scalable robot control policy directly from MoCap. Instead of treating demonstrations as strict ground truth, we view them as soft guidance. From raw demonstrations, we construct adaptive spatial scopes—time-varying termination boundaries, and reinforcement learning promotes the policy to stay within these envelopes while minimizing control effort. This holistic approach preserves demonstration intent, lets robot-specific strategies emerge, boosts robustness to noise, and scales effortlessly with large-scale demonstrations. We distill the scaled tracking policy into a vision-based, skill-conditioned generative control policy. This distilled policy captures diverse manipulation skills within a rich latent representation, enabling generalization across various objects and real-world robotic manipulation.",
        "keywords": [
            "Dexterous Manipulation",
            "Reinforcement Learning",
            "Learning from Demonstrations"
        ],
        "pdf_url": "https://openreview.net/pdf/00f6b5659e435c9f6f3a6a18393842ce652ec011.pdf",
        "reviews": [
            {
                "id": "7clW0exTTH",
                "forum": "gyihSZwQbR",
                "replyto": "gyihSZwQbR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission787/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068303949,
                "mdate": 1754869473737,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "gyihSZwQbR",
                "forum": "gyihSZwQbR",
                "content": {
                    "title": {
                        "value": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration"
                    },
                    "authors": {
                        "value": [
                            "Sirui Xu",
                            "Yu-Wei Chao",
                            "Liuyu Bian",
                            "Arsalan Mousavian",
                            "Yu-Xiong Wang",
                            "Liangyan Gui",
                            "Wei Yang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sirui_Xu1",
                            "~Yu-Wei_Chao1",
                            "~Liuyu_Bian1",
                            "~Arsalan_Mousavian1",
                            "~Yu-Xiong_Wang1",
                            "~Liangyan_Gui1",
                            "~Wei_Yang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Manipulation",
                            "Reinforcement Learning",
                            "Learning from Demonstrations"
                        ]
                    },
                    "abstract": {
                        "value": "Hand–object motion-capture (MoCap) repositories provide abundant, contact-rich human demonstrations for scaling dexterous manipulation on robots. Yet demonstration inaccuracy and embodiment gaps between human and robot hands challenge direct policy learning. Existing pipelines adapt a three-stage workflow: retargeting, tracking, and residual correction. This multi-step process may not fully utilize demonstrations and can introduce compound errors. We introduce Reference-Scoped Exploration (RSE), a unified, single-loop optimization that integrates retargeting and tracking to train a scalable robot control policy directly from MoCap. Instead of treating demonstrations as strict ground truth, we view them as soft guidance. From raw demonstrations, we construct adaptive spatial scopes—time-varying termination boundaries, and reinforcement learning promotes the policy to stay within these envelopes while minimizing control effort. This holistic approach preserves demonstration intent, lets robot-specific strategies emerge, boosts robustness to noise, and scales effortlessly with large-scale demonstrations. We distill the scaled tracking policy into a vision-based, skill-conditioned generative control policy. This distilled policy captures diverse manipulation skills within a rich latent representation, enabling generalization across various objects and real-world robotic manipulation."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/00f6b5659e435c9f6f3a6a18393842ce652ec011.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025dexplore,\ntitle={Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration},\nauthor={Sirui Xu and Yu-Wei Chao and Liuyu Bian and Arsalan Mousavian and Yu-Xiong Wang and Liangyan Gui and Wei Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=gyihSZwQbR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c42e97d2c1f5b31a6ad0b792df4d603a883476a8.mp4"
                    },
                    "paperhash": {
                        "value": "xu|dexplore_scalable_neural_control_for_dexterous_manipulation_from_reference_scoped_exploration"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission787/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission787/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission787/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745442556031,
                "pdate": 1754680635383,
                "odate": 1758062781204,
                "mdate": 1758062823051,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission787/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission787/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "sA2Yv4QKMr",
        "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
        "abstract": "Many robot caregiving tasks, such as bathing, dressing, and transferring, require a robot arm to make contact with a human body at multiple points rather than solely at the end effector. However, varied human touch preferences can lead to unsafe or uncomfortable multi-contact interactions. To address this, we introduce PrioriTouch, a framework integrating a novel contextual bandit algorithm with hierarchical operational space control to learn user contact preferences and translate them into low-level pose and force control policies. PrioriTouch minimizes user discomfort by initially gathering real-world feedback and subsequently refining the policy using simulation-in-the-loop, thus avoiding unsafe user experimentation. Guided by insights from a user study on physical assistance preferences, we rigorously evaluate PrioriTouch in extensive simulation and real-world experiments, demonstrating effective adaptation to user contact preferences, maintained task performance, and enhanced safety and comfort.",
        "keywords": [
            "Physical Human-Robot Interaction",
            "Online Preference Learning",
            "Assistive Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/2822e25935b41dc6d1acf561966bfcec063b6c4b.pdf",
        "reviews": [
            {
                "id": "z3jjdSwKVs",
                "forum": "sA2Yv4QKMr",
                "replyto": "sA2Yv4QKMr",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission786/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068303789,
                "mdate": 1754869473573,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "sA2Yv4QKMr",
                "forum": "sA2Yv4QKMr",
                "content": {
                    "title": {
                        "value": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction"
                    },
                    "authors": {
                        "value": [
                            "Rishabh Madan",
                            "Jiawei Lin",
                            "Mahika Goel",
                            "Amber Li",
                            "Angchen Xie",
                            "Xiaoyu Liang",
                            "Marcus Lee",
                            "Justin Guo",
                            "Pranav N. Thakkar",
                            "Rohan Banerjee",
                            "Jose Barreiros",
                            "Kate Tsui",
                            "Tom Silver",
                            "Tapomayukh Bhattacharjee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Rishabh_Madan1",
                            "~Jiawei_Lin5",
                            "mg994@cornell.edu",
                            "adl94@cornell.edu",
                            "~Angchen_Xie1",
                            "~Xiaoyu_Liang3",
                            "mrl256@cornell.edu",
                            "jj283@cornell.edu",
                            "~Pranav_N._Thakkar1",
                            "~Rohan_Banerjee1",
                            "~Jose_Barreiros1",
                            "kate.tsui@tri.global",
                            "~Tom_Silver1",
                            "~Tapomayukh_Bhattacharjee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Physical Human-Robot Interaction",
                            "Online Preference Learning",
                            "Assistive Robotics"
                        ]
                    },
                    "abstract": {
                        "value": "Many robot caregiving tasks, such as bathing, dressing, and transferring, require a robot arm to make contact with a human body at multiple points rather than solely at the end effector. However, varied human touch preferences can lead to unsafe or uncomfortable multi-contact interactions. To address this, we introduce PrioriTouch, a framework integrating a novel contextual bandit algorithm with hierarchical operational space control to learn user contact preferences and translate them into low-level pose and force control policies. PrioriTouch minimizes user discomfort by initially gathering real-world feedback and subsequently refining the policy using simulation-in-the-loop, thus avoiding unsafe user experimentation. Guided by insights from a user study on physical assistance preferences, we rigorously evaluate PrioriTouch in extensive simulation and real-world experiments, demonstrating effective adaptation to user contact preferences, maintained task performance, and enhanced safety and comfort."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c3762a56130a2cb68af4ae9745f7db78c314edfb.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2822e25935b41dc6d1acf561966bfcec063b6c4b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmadan2025prioritouch,\ntitle={PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction},\nauthor={Rishabh Madan and Jiawei Lin and Mahika Goel and Amber Li and Angchen Xie and Xiaoyu Liang and Marcus Lee and Justin Guo and Pranav N. Thakkar and Rohan Banerjee and Jose Barreiros and Kate Tsui and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sA2Yv4QKMr}\n}"
                    },
                    "paperhash": {
                        "value": "madan|prioritouch_adapting_to_user_contact_preferences_for_wholearm_physical_humanrobot_interaction"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission786/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission786/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission786/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745442538880,
                "pdate": 1754680635293,
                "odate": 1758062781158,
                "mdate": 1758062822781,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission786/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission786/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "L5PJBd8ahD",
        "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation",
        "abstract": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. \n    However, acquiring such models can be costly and impractical. \n    Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry.\n    To this end, we propose $\\textit{UnPose}$, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. \n    Specifically, starting from a single-view RGB-D frame, $\\textit{UnPose}$ uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates.\n    As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model’s uncertainty,\n    thereby, continuously improving the pose estimation accuracy and 3D reconstruction quality. \n    To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field.\n    Extensive experiments demonstrate that $\\textit{UnPose}$ significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality.\n    We further showcase its practical applicability in real-world robotic manipulation tasks.",
        "keywords": [
            "6D Pose Estimation",
            "Diffusion Model",
            "Object Reconstruction"
        ],
        "pdf_url": "https://openreview.net/pdf/8e531b9b1e6ffa47162e3ead9a8ba8c38094526c.pdf",
        "reviews": [
            {
                "id": "Yg4DxIDkAq",
                "forum": "L5PJBd8ahD",
                "replyto": "L5PJBd8ahD",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission785/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068303786,
                "mdate": 1754869473574,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "L5PJBd8ahD",
                "forum": "L5PJBd8ahD",
                "content": {
                    "title": {
                        "value": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation"
                    },
                    "authors": {
                        "value": [
                            "Zhaodong Jiang",
                            "Ashish Sinha",
                            "Tongtong Cao",
                            "Yuan Ren",
                            "Bingbing Liu",
                            "Binbin Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhaodong_Jiang1",
                            "~Ashish_Sinha1",
                            "~Tongtong_Cao1",
                            "~Yuan_Ren2",
                            "~Bingbing_Liu2",
                            "~Binbin_Xu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "6D Pose Estimation",
                            "Diffusion Model",
                            "Object Reconstruction"
                        ]
                    },
                    "TLDR": {
                        "value": "A zero-shot, model-free 6D pose estimation and reconstruction framework that incrementally refines a 3D Gaussian Splatting model using diffusion priors and uncertainty-guided fusion from RGB-D inputs."
                    },
                    "abstract": {
                        "value": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. \n    However, acquiring such models can be costly and impractical. \n    Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry.\n    To this end, we propose $\\textit{UnPose}$, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. \n    Specifically, starting from a single-view RGB-D frame, $\\textit{UnPose}$ uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates.\n    As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model’s uncertainty,\n    thereby, continuously improving the pose estimation accuracy and 3D reconstruction quality. \n    To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field.\n    Extensive experiments demonstrate that $\\textit{UnPose}$ significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality.\n    We further showcase its practical applicability in real-world robotic manipulation tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9e39eaacdc7d4a5b3166caaf2ac3d10731fe0c4c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/8e531b9b1e6ffa47162e3ead9a8ba8c38094526c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njiang2025unpose,\ntitle={UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation},\nauthor={Zhaodong Jiang and Ashish Sinha and Tongtong Cao and Yuan Ren and Bingbing Liu and Binbin Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=L5PJBd8ahD}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/443156c604d87056c2a3d51b9de39adc75f1e924.zip"
                    },
                    "paperhash": {
                        "value": "jiang|unpose_uncertaintyguided_diffusion_priors_for_zeroshot_pose_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission785/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission785/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission785/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745442436704,
                "pdate": 1754680635291,
                "odate": 1758062781110,
                "mdate": 1758062822567,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission785/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "93bWCbhXJR",
        "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention",
        "abstract": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20\\% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets to promote reproducibility and follow-up work.",
        "keywords": [
            "LLM-enabled Robots",
            "LLM Distillation"
        ],
        "pdf_url": "https://openreview.net/pdf/7ebcf0f7c3d4ccca577af267d97b0f03dece670c.pdf",
        "reviews": [
            {
                "id": "8eMrjDzSyd",
                "forum": "93bWCbhXJR",
                "replyto": "93bWCbhXJR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission774/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068302834,
                "mdate": 1754869473480,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "93bWCbhXJR",
                "forum": "93bWCbhXJR",
                "content": {
                    "title": {
                        "value": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention"
                    },
                    "authors": {
                        "value": [
                            "Zachary Ravichandran",
                            "Ignacio Hounie",
                            "Fernando Cladera",
                            "Alejandro Ribeiro",
                            "George J. Pappas",
                            "Vijay Kumar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zachary_Ravichandran1",
                            "~Ignacio_Hounie1",
                            "~Fernando_Cladera1",
                            "~Alejandro_Ribeiro1",
                            "~George_J._Pappas1",
                            "~Vijay_Kumar2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "LLM-enabled Robots",
                            "LLM Distillation"
                        ]
                    },
                    "abstract": {
                        "value": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20\\% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets to promote reproducibility and follow-up work."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ddb9c952393a7a0b1e3c0313a4f20364b48e5b6f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A framework for distilling on-device language model-enabled robot planners with minimal human intervention."
                    },
                    "pdf": {
                        "value": "/pdf/7ebcf0f7c3d4ccca577af267d97b0f03dece670c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nravichandran2025distilling,\ntitle={Distilling On-device Language Models for Robot Planning with Minimal Human Intervention},\nauthor={Zachary Ravichandran and Ignacio Hounie and Fernando Cladera and Alejandro Ribeiro and George J. Pappas and Vijay Kumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=93bWCbhXJR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/82ff5e08598a4db9de1613ab7ae506803fda69bf.mp4"
                    },
                    "paperhash": {
                        "value": "ravichandran|distilling_ondevice_language_models_for_robot_planning_with_minimal_human_intervention"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission774/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission774/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission774/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745440712506,
                "pdate": 1754680634985,
                "odate": 1758062780814,
                "mdate": 1758062822626,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission774/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission774/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "YvsUD8C9QS",
        "title": "Mechanistic Interpretability for Steering Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control---establishing a new paradigm for transparent and steerable foundation models in robotics.",
        "keywords": [
            "Mechanistic Interpretability",
            "Vision-Language-Action Models",
            "Foundation Models for Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/b39be62fae34fd96c66ab4b8ff09b8eb21a1acf4.pdf",
        "reviews": [
            {
                "id": "gJYIvaLzn0",
                "forum": "YvsUD8C9QS",
                "replyto": "YvsUD8C9QS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission772/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068302827,
                "mdate": 1754869473308,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "YvsUD8C9QS",
                "forum": "YvsUD8C9QS",
                "content": {
                    "title": {
                        "value": "Mechanistic Interpretability for Steering Vision-Language-Action Models"
                    },
                    "authors": {
                        "value": [
                            "Bear Häon",
                            "Kaylene Caswell Stocking",
                            "Ian Chuang",
                            "Claire Tomlin"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Bear_Häon1",
                            "~Kaylene_Caswell_Stocking1",
                            "~Ian_Chuang1",
                            "~Claire_Tomlin1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Mechanistic Interpretability",
                            "Vision-Language-Action Models",
                            "Foundation Models for Robotics"
                        ]
                    },
                    "abstract": {
                        "value": "Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control---establishing a new paradigm for transparent and steerable foundation models in robotics."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We show that you can steer robot behavior in real time by directly activating semantically meaningful VLA neurons - unlocking a new, interpretable interface for zero-shot robot control."
                    },
                    "pdf": {
                        "value": "/pdf/b39be62fae34fd96c66ab4b8ff09b8eb21a1acf4.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhaon2025mechanistic,\ntitle={Mechanistic Interpretability for Steering Vision-Language-Action Models},\nauthor={Bear H{\\\"a}on and Kaylene Caswell Stocking and Ian Chuang and Claire Tomlin},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=YvsUD8C9QS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f3b878060689ba0ff6c72a3faed17da64776d714.mp4"
                    },
                    "paperhash": {
                        "value": "häon|mechanistic_interpretability_for_steering_visionlanguageaction_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission772/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission772/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission772/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745440426020,
                "pdate": 1754680634840,
                "odate": 1758062780737,
                "mdate": 1758062822260,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission772/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bt1Tovn0SW",
        "title": "Training Strategies for Efficient Embodied Reasoning",
        "abstract": "Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.",
        "keywords": [
            "robot reasoning",
            "vision-language-action models"
        ],
        "pdf_url": "https://openreview.net/pdf/1e0e49a42de0166e1fb838c8d93584e4a414a3ec.pdf",
        "reviews": [
            {
                "id": "AcrPTMAFeI",
                "forum": "bt1Tovn0SW",
                "replyto": "bt1Tovn0SW",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission770/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068302298,
                "mdate": 1754869457769,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bt1Tovn0SW",
                "forum": "bt1Tovn0SW",
                "content": {
                    "title": {
                        "value": "Training Strategies for Efficient Embodied Reasoning"
                    },
                    "authors": {
                        "value": [
                            "William Chen",
                            "Suneel Belkhale",
                            "Suvir Mirchandani",
                            "Karl Pertsch",
                            "Danny Driess",
                            "Oier Mees",
                            "Sergey Levine"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~William_Chen1",
                            "~Suneel_Belkhale1",
                            "~Suvir_Mirchandani1",
                            "~Karl_Pertsch1",
                            "~Danny_Driess1",
                            "~Oier_Mees1",
                            "~Sergey_Levine1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "robot reasoning",
                            "vision-language-action models"
                        ]
                    },
                    "abstract": {
                        "value": "Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/474f7822277274f751baca7030030ff3731bba97.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "By exploring hypotheses for why embodied reasoning improves robot policies, we introduce lightweight strategies for training fast and effective VLAs with robot reasoning data."
                    },
                    "pdf": {
                        "value": "/pdf/1e0e49a42de0166e1fb838c8d93584e4a414a3ec.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchen2025training,\ntitle={Training Strategies for Efficient Embodied Reasoning},\nauthor={William Chen and Suneel Belkhale and Suvir Mirchandani and Karl Pertsch and Danny Driess and Oier Mees and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bt1Tovn0SW}\n}"
                    },
                    "paperhash": {
                        "value": "chen|training_strategies_for_efficient_embodied_reasoning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission770/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission770/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission770/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745440397964,
                "pdate": 1754680634771,
                "odate": 1758062780634,
                "mdate": 1758062822209,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission770/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission770/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "uWFlkufjFJ",
        "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference",
        "abstract": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions.",
        "keywords": [
            "Dexterous Grasping",
            "Normalizing Flows",
            "Uncertainty-Awareness"
        ],
        "pdf_url": "https://openreview.net/pdf/0fdbeae8a29f2eb76f7a8ac1281b02f030ce063a.pdf",
        "reviews": [
            {
                "id": "Zu3yTeT17r",
                "forum": "uWFlkufjFJ",
                "replyto": "uWFlkufjFJ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission752/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068301100,
                "mdate": 1754869473259,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "uWFlkufjFJ",
                "forum": "uWFlkufjFJ",
                "content": {
                    "title": {
                        "value": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference"
                    },
                    "authors": {
                        "value": [
                            "Qian Feng",
                            "Jianxiang Feng",
                            "Zhaopeng Chen",
                            "Rudolph Triebel",
                            "Alois Knoll"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Qian_Feng2",
                            "~Jianxiang_Feng1",
                            "zhaopeng.chen@agile-robots.com",
                            "~Rudolph_Triebel1",
                            "~Alois_Knoll1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Grasping",
                            "Normalizing Flows",
                            "Uncertainty-Awareness"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds."
                    },
                    "abstract": {
                        "value": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9bcc193443f02ade37dde548a9ca6b8acf46c45e.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0fdbeae8a29f2eb76f7a8ac1281b02f030ce063a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfeng2025ffhflow,\ntitle={{FFHF}low: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference},\nauthor={Qian Feng and Jianxiang Feng and Zhaopeng Chen and Rudolph Triebel and Alois Knoll},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=uWFlkufjFJ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/21d8a89c97071138ed65acb161d9086f106c9733.zip"
                    },
                    "paperhash": {
                        "value": "feng|ffhflow_diverse_and_uncertaintyaware_dexterous_grasp_generation_via_flow_variational_inference"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission752/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission752/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission752/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745438368595,
                "pdate": 1754680633810,
                "odate": 1758062779971,
                "mdate": 1758062822119,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission752/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ZA8iXa45P2",
        "title": "Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use",
        "abstract": "Tool use is essential for enabling robots to perform complex real-world tasks, but learning such skills requires extensive datasets. While teleoperation is widely used, it is slow, delay-sensitive, and poorly suited for dynamic tasks. In contrast, human videos provide a natural way for data collection without specialized hardware, though they pose challenges on robot learning due to viewpoint variations and embodiment gaps. To address these challenges, we propose a framework that transfers tool-use knowledge from humans to robots. To improve the policy's robustness to viewpoint variations, we use two RGB cameras to reconstruct 3D scenes and apply Gaussian splatting for novel view synthesis. We reduce the embodiment gap using segmented observations and tool-centric, task-space actions to achieve embodiment-invariant visuomotor policy learning. Our method achieves a 71\\% improvement in task success and a 77\\% reduction in data collection time compared to diffusion policies trained on teleoperation with equivalent time budgets. Our method also reduces data collection time by 41\\% compared with the state-of-the-art data collection interface.",
        "keywords": [
            "Tool Use",
            "Data Collection",
            "Learning from Video"
        ],
        "pdf_url": "https://openreview.net/pdf/585026b4e3abb467d099850b12d2e3d46daabbd2.pdf",
        "reviews": [
            {
                "id": "cQo9KZ3RSE",
                "forum": "ZA8iXa45P2",
                "replyto": "ZA8iXa45P2",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission751/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300916,
                "mdate": 1754869473166,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ZA8iXa45P2",
                "forum": "ZA8iXa45P2",
                "content": {
                    "title": {
                        "value": "Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use"
                    },
                    "authors": {
                        "value": [
                            "Haonan Chen",
                            "Cheng Zhu",
                            "Shuijing Liu",
                            "Yunzhu Li",
                            "Katherine Rose Driggs-Campbell"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haonan_Chen6",
                            "~Cheng_Zhu8",
                            "~Shuijing_Liu1",
                            "~Yunzhu_Li1",
                            "~Katherine_Rose_Driggs-Campbell1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tool Use",
                            "Data Collection",
                            "Learning from Video"
                        ]
                    },
                    "abstract": {
                        "value": "Tool use is essential for enabling robots to perform complex real-world tasks, but learning such skills requires extensive datasets. While teleoperation is widely used, it is slow, delay-sensitive, and poorly suited for dynamic tasks. In contrast, human videos provide a natural way for data collection without specialized hardware, though they pose challenges on robot learning due to viewpoint variations and embodiment gaps. To address these challenges, we propose a framework that transfers tool-use knowledge from humans to robots. To improve the policy's robustness to viewpoint variations, we use two RGB cameras to reconstruct 3D scenes and apply Gaussian splatting for novel view synthesis. We reduce the embodiment gap using segmented observations and tool-centric, task-space actions to achieve embodiment-invariant visuomotor policy learning. Our method achieves a 71\\% improvement in task success and a 77\\% reduction in data collection time compared to diffusion policies trained on teleoperation with equivalent time budgets. Our method also reduces data collection time by 41\\% compared with the state-of-the-art data collection interface."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/585026b4e3abb467d099850b12d2e3d46daabbd2.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchen2025toolasinterface,\ntitle={Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use},\nauthor={Haonan Chen and Cheng Zhu and Shuijing Liu and Yunzhu Li and Katherine Rose Driggs-Campbell},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZA8iXa45P2}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/608ff5f14b9621302f4141bd2882432e19292588.zip"
                    },
                    "paperhash": {
                        "value": "chen|toolasinterface_learning_robot_policies_from_observing_human_tool_use"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission751/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission751/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission751/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745438308840,
                "pdate": 1754680633734,
                "odate": 1758062779926,
                "mdate": 1758062821854,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission751/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "NOQwVh1Gib",
        "title": "DiWA: Diffusion Policy Adaptation with World Models",
        "abstract": "Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Additionally, standard RL methods require millions of physical interaction steps, making fine-tuning even more challenging. Prior work models the denoising steps in diffusion policies as a Markov Decision Process to adapt to RL policy updates, but its heavy reliance on environment interactions still leads to inefficiency. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at _redacted-for-review_.",
        "keywords": [
            "World Models",
            "Imitation Learning",
            "Reinforcement Learning",
            "Diffusion Policies"
        ],
        "pdf_url": "https://openreview.net/pdf/28c1cd89c4e80c6507e0dcbd72cdb9ae39274084.pdf",
        "reviews": [
            {
                "id": "m0kAYbbTol",
                "forum": "NOQwVh1Gib",
                "replyto": "NOQwVh1Gib",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission744/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300403,
                "mdate": 1754869457516,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "NOQwVh1Gib",
                "forum": "NOQwVh1Gib",
                "content": {
                    "title": {
                        "value": "DiWA: Diffusion Policy Adaptation with World Models"
                    },
                    "authors": {
                        "value": [
                            "Akshay L Chandra",
                            "Iman Nematollahi",
                            "Chenguang Huang",
                            "Tim Welschehold",
                            "Wolfram Burgard",
                            "Abhinav Valada"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Akshay_L_Chandra1",
                            "~Iman_Nematollahi1",
                            "~Chenguang_Huang1",
                            "~Tim_Welschehold1",
                            "~Wolfram_Burgard3",
                            "~Abhinav_Valada1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "World Models",
                            "Imitation Learning",
                            "Reinforcement Learning",
                            "Diffusion Policies"
                        ]
                    },
                    "TLDR": {
                        "value": "Fine-tuning diffusion policies by exploring entirely inside frozen world models learned from unstructured play data."
                    },
                    "abstract": {
                        "value": "Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Additionally, standard RL methods require millions of physical interaction steps, making fine-tuning even more challenging. Prior work models the denoising steps in diffusion policies as a Markov Decision Process to adapt to RL policy updates, but its heavy reliance on environment interactions still leads to inefficiency. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at _redacted-for-review_."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/28c1cd89c4e80c6507e0dcbd72cdb9ae39274084.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchandra2025diwa,\ntitle={Di{WA}: Diffusion Policy Adaptation with World Models},\nauthor={Akshay L Chandra and Iman Nematollahi and Chenguang Huang and Tim Welschehold and Wolfram Burgard and Abhinav Valada},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=NOQwVh1Gib}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f25eccd75beb23969de18e86ed2900d553815c87.mp4"
                    },
                    "paperhash": {
                        "value": "chandra|diwa_diffusion_policy_adaptation_with_world_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission744/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission744/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission744/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745437782024,
                "pdate": 1754680633528,
                "odate": 1758062779758,
                "mdate": 1758062821846,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission744/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission744/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "OXHF0BvmRT",
        "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States",
        "abstract": "We introduce a model for monocular RGB relative pose estimation of a ground robot that trains from scratch without pose labels nor prior knowledge about the robot's shape or appearance.\nAt training time, we assume: (i) a robot fitted with multiple LEDs, whose states are independent and known at each frame; (ii) knowledge of the approximate viewing direction of each LED; and (iii) availability of a calibration image with a known target distance, to address the ambiguity of monocular depth estimation.\nTraining data is collected by a pair of robots moving randomly without needing external infrastructure or human supervision.\nOur model trains on the task of predicting from an image the state of each LED on the robot.\nIn doing so, it learns to predict the position of the robot in the image, its distance, and its relative bearing.\nAt inference time, the state of the LEDs is unknown, can be arbitrary, and does not affect the pose estimation performance.\nQuantitative experiments indicate that our approach: is competitive with SoA approaches that require supervision from pose labels or a CAD model of the robot; generalizes to different domains; and handles multi-robot pose estimation.",
        "keywords": [
            "Self-supervised Learning",
            "Pretext Task",
            "Visual Pose Estimation"
        ],
        "pdf_url": "https://openreview.net/pdf/d946b8703610806aca948d266183d4cee22ac378.pdf",
        "reviews": [
            {
                "id": "P6fOJBdf2h",
                "forum": "OXHF0BvmRT",
                "replyto": "OXHF0BvmRT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission742/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300571,
                "mdate": 1754869473083,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "OXHF0BvmRT",
                "forum": "OXHF0BvmRT",
                "content": {
                    "title": {
                        "value": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States"
                    },
                    "authors": {
                        "value": [
                            "Nicholas Carlotti",
                            "Mirko Nava",
                            "Alessandro Giusti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Nicholas_Carlotti1",
                            "~Mirko_Nava2",
                            "~Alessandro_Giusti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Self-supervised Learning",
                            "Pretext Task",
                            "Visual Pose Estimation"
                        ]
                    },
                    "abstract": {
                        "value": "We introduce a model for monocular RGB relative pose estimation of a ground robot that trains from scratch without pose labels nor prior knowledge about the robot's shape or appearance.\nAt training time, we assume: (i) a robot fitted with multiple LEDs, whose states are independent and known at each frame; (ii) knowledge of the approximate viewing direction of each LED; and (iii) availability of a calibration image with a known target distance, to address the ambiguity of monocular depth estimation.\nTraining data is collected by a pair of robots moving randomly without needing external infrastructure or human supervision.\nOur model trains on the task of predicting from an image the state of each LED on the robot.\nIn doing so, it learns to predict the position of the robot in the image, its distance, and its relative bearing.\nAt inference time, the state of the LEDs is unknown, can be arbitrary, and does not affect the pose estimation performance.\nQuantitative experiments indicate that our approach: is competitive with SoA approaches that require supervision from pose labels or a CAD model of the robot; generalizes to different domains; and handles multi-robot pose estimation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/4b55d114fbad89460ca3ba3ed96602a03999f6a9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d946b8703610806aca948d266183d4cee22ac378.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ncarlotti2025selfsupervised,\ntitle={Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying {LED} States},\nauthor={Nicholas Carlotti and Mirko Nava and Alessandro Giusti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=OXHF0BvmRT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d06c6a38c0b68004fc16b9d74cded7e574231339.zip"
                    },
                    "paperhash": {
                        "value": "carlotti|selfsupervised_learning_of_visual_pose_estimation_without_pose_labels_by_classifying_led_states"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission742/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission742/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission742/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745437753872,
                "pdate": 1754680633403,
                "odate": 1758062779718,
                "mdate": 1758062821870,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission742/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "AO0BKxf3ss",
        "title": "Diffusion-Guided Multi-Arm Motion Planning",
        "abstract": "Multi-arm motion planning is fundamental for enabling arms to complete collaborative tasks in shared spaces but current methods struggle with scalability due to exponential state-space growth and reliance on large training datasets for learned models. Inspired by Multi-Agent Path Finding (MAPF), which decomposes planning into single-agent problems coupled with collision resolution, we propose a novel diffusion-guided multi-arm planner (DG-MAP) that enhances scalability of learning-based models while reducing their reliance on massive multi-arm datasets. Recognizing that collisions are primarily pairwise, we train two conditional diffusion models, one to generate feasible single-arm trajectories, and a second, to model the dual-arm dynamics required for effective pairwise collision resolution. By integrating these specialized generative models within a MAPF-inspired structured decomposition, our planner efficiently scales to larger number of arms. Evaluations against alternative learning-based methods across various team sizes demonstrate our method's effectiveness and practical applicability. Code and data will be made publicly available. View video demonstrations in our supplementary material.",
        "keywords": [
            "multi-agent",
            "planning",
            "diffusion"
        ],
        "pdf_url": "https://openreview.net/pdf/dfe8896273e54a526e8657f71030bd55aeafef27.pdf",
        "reviews": [
            {
                "id": "FCTSPqA83R",
                "forum": "AO0BKxf3ss",
                "replyto": "AO0BKxf3ss",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission741/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300498,
                "mdate": 1754869472992,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "AO0BKxf3ss",
                "forum": "AO0BKxf3ss",
                "content": {
                    "title": {
                        "value": "Diffusion-Guided Multi-Arm Motion Planning"
                    },
                    "authors": {
                        "value": [
                            "Viraj Parimi",
                            "Brian C. Williams"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Viraj_Parimi2",
                            "~Brian_C._Williams1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "multi-agent",
                            "planning",
                            "diffusion"
                        ]
                    },
                    "abstract": {
                        "value": "Multi-arm motion planning is fundamental for enabling arms to complete collaborative tasks in shared spaces but current methods struggle with scalability due to exponential state-space growth and reliance on large training datasets for learned models. Inspired by Multi-Agent Path Finding (MAPF), which decomposes planning into single-agent problems coupled with collision resolution, we propose a novel diffusion-guided multi-arm planner (DG-MAP) that enhances scalability of learning-based models while reducing their reliance on massive multi-arm datasets. Recognizing that collisions are primarily pairwise, we train two conditional diffusion models, one to generate feasible single-arm trajectories, and a second, to model the dual-arm dynamics required for effective pairwise collision resolution. By integrating these specialized generative models within a MAPF-inspired structured decomposition, our planner efficiently scales to larger number of arms. Evaluations against alternative learning-based methods across various team sizes demonstrate our method's effectiveness and practical applicability. Code and data will be made publicly available. View video demonstrations in our supplementary material."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9d16a8adb3dd2ab219f455d243bec744a55fea01.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/dfe8896273e54a526e8657f71030bd55aeafef27.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nparimi2025diffusionguided,\ntitle={Diffusion-Guided Multi-Arm Motion Planning},\nauthor={Viraj Parimi and Brian C. Williams},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AO0BKxf3ss}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/4171cc2bbff6814d030c8a837e48fff49c498ddb.mp4"
                    },
                    "paperhash": {
                        "value": "parimi|diffusionguided_multiarm_motion_planning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission741/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission741/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission741/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745437484499,
                "pdate": 1754680633392,
                "odate": 1758062779683,
                "mdate": 1758062821623,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission741/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Yy9EVIajH5",
        "title": "GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering",
        "abstract": "In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps, and further demonstrate GraphEQA in two separate real world environments. Videos and code are available at https://grapheqa.github.io.",
        "keywords": [
            "Embodied Question Answering",
            "Vision Language Models",
            "Robot Planning",
            "Real-time 3D Scene Graphs",
            "Guided Exploration"
        ],
        "pdf_url": "https://openreview.net/pdf/1160131e84d4ec89ba94297e328e73dafcf0dc95.pdf",
        "reviews": [
            {
                "id": "6VIEeFxpQZ",
                "forum": "Yy9EVIajH5",
                "replyto": "Yy9EVIajH5",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission740/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300218,
                "mdate": 1754869472915,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Yy9EVIajH5",
                "forum": "Yy9EVIajH5",
                "content": {
                    "title": {
                        "value": "GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering"
                    },
                    "authors": {
                        "value": [
                            "Saumya Saxena",
                            "Blake Buchanan",
                            "Chris Paxton",
                            "Peiqi Liu",
                            "Bingqing Chen",
                            "Narunas Vaskevicius",
                            "Luigi Palmieri",
                            "Jonathan Francis",
                            "Oliver Kroemer"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Saumya_Saxena1",
                            "~Blake_Buchanan1",
                            "~Chris_Paxton1",
                            "~Peiqi_Liu1",
                            "~Bingqing_Chen2",
                            "~Narunas_Vaskevicius2",
                            "~Luigi_Palmieri1",
                            "~Jonathan_Francis1",
                            "~Oliver_Kroemer1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Embodied Question Answering",
                            "Vision Language Models",
                            "Robot Planning",
                            "Real-time 3D Scene Graphs",
                            "Guided Exploration"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSG) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments."
                    },
                    "abstract": {
                        "value": "In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps, and further demonstrate GraphEQA in two separate real world environments. Videos and code are available at https://grapheqa.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1b0b3fb7c4d7592d41e0d7224cf1c68c077ae535.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/1160131e84d4ec89ba94297e328e73dafcf0dc95.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsaxena2025grapheqa,\ntitle={Graph{EQA}: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering},\nauthor={Saumya Saxena and Blake Buchanan and Chris Paxton and Peiqi Liu and Bingqing Chen and Narunas Vaskevicius and Luigi Palmieri and Jonathan Francis and Oliver Kroemer},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Yy9EVIajH5}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/21064168e58396ee796481d6ad86500b520ec8c5.mp4"
                    },
                    "paperhash": {
                        "value": "saxena|grapheqa_using_3d_semantic_scene_graphs_for_realtime_embodied_question_answering"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission740/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission740/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission740/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745437469220,
                "pdate": 1754680633361,
                "odate": 1758062779646,
                "mdate": 1758062821607,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission740/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission740/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "4eMWCoWUKR",
        "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering",
        "abstract": "As robots become increasingly capable of operating over extended periods—spanning days, weeks, and even months—they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.",
        "keywords": [
            "embodied question answering",
            "long-term reasoning",
            "vision-language navigation"
        ],
        "pdf_url": "https://openreview.net/pdf/f33e43132b4631d116c3f990958f15cb6f10704e.pdf",
        "reviews": [
            {
                "id": "Cql5lEATLX",
                "forum": "4eMWCoWUKR",
                "replyto": "4eMWCoWUKR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission736/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068300032,
                "mdate": 1754869472849,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "4eMWCoWUKR",
                "forum": "4eMWCoWUKR",
                "content": {
                    "title": {
                        "value": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering"
                    },
                    "authors": {
                        "value": [
                            "Muhammad Fadhil Ginting",
                            "Dong-Ki Kim",
                            "Xiangyun Meng",
                            "Andrzej Marek Reinke",
                            "Bandi Jai Krishna",
                            "Navid Kayhani",
                            "Oriana Peltzer",
                            "David Fan",
                            "Amirreza Shaban",
                            "Sung-Kyun Kim",
                            "Mykel Kochenderfer",
                            "Ali-akbar Agha-mohammadi",
                            "Shayegan Omidshafiei"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Muhammad_Fadhil_Ginting1",
                            "~Dong-Ki_Kim1",
                            "~Xiangyun_Meng1",
                            "~Andrzej_Marek_Reinke1",
                            "jai@fieldai.com",
                            "~Navid_Kayhani1",
                            "oriana@fieldai.com",
                            "~David_Fan1",
                            "~Amirreza_Shaban1",
                            "skysk7@gmail.com",
                            "~Mykel_Kochenderfer1",
                            "~Ali-akbar_Agha-mohammadi1",
                            "~Shayegan_Omidshafiei1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "embodied question answering",
                            "long-term reasoning",
                            "vision-language navigation"
                        ]
                    },
                    "abstract": {
                        "value": "As robots become increasingly capable of operating over extended periods—spanning days, weeks, and even months—they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We formulate a new, challenging problem—Long-term Active Embodied Question Answering —and present Mind Palace Exploration, a method for reasoning and planning through long-term memory retrieval and active exploration, along with the new benchmark."
                    },
                    "pdf": {
                        "value": "/pdf/f33e43132b4631d116c3f990958f15cb6f10704e.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nginting2025enter,\ntitle={Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering},\nauthor={Muhammad Fadhil Ginting and Dong-Ki Kim and Xiangyun Meng and Andrzej Marek Reinke and Bandi Jai Krishna and Navid Kayhani and Oriana Peltzer and David Fan and Amirreza Shaban and Sung-Kyun Kim and Mykel Kochenderfer and Ali-akbar Agha-mohammadi and Shayegan Omidshafiei},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4eMWCoWUKR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/aeafcfb6143e44bb08ed8f34f87aed95fe9cca74.mp4"
                    },
                    "paperhash": {
                        "value": "ginting|enter_the_mind_palace_reasoning_and_planning_for_longterm_active_embodied_question_answering"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission736/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission736/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission736/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745436955414,
                "pdate": 1754680633143,
                "odate": 1758062779518,
                "mdate": 1758062821536,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission736/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission736/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "oOCa85Z1Ho",
        "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
        "abstract": "Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals.\nWe do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution.\nOur method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments.",
        "keywords": [
            "Scene Generation",
            "Simulation",
            "Diffusion",
            "MCTS"
        ],
        "pdf_url": "https://openreview.net/pdf/524e890f07a53aa17137d6dd0e9e605c5bcd5050.pdf",
        "reviews": [
            {
                "id": "fSSbiI73gK",
                "forum": "oOCa85Z1Ho",
                "replyto": "oOCa85Z1Ho",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission733/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068299871,
                "mdate": 1754869472769,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "oOCa85Z1Ho",
                "forum": "oOCa85Z1Ho",
                "content": {
                    "title": {
                        "value": "Steerable Scene Generation with Post Training and Inference-Time Search"
                    },
                    "authors": {
                        "value": [
                            "Nicholas Ezra Pfaff",
                            "Hongkai Dai",
                            "Sergey Zakharov",
                            "Shun Iwase",
                            "Russ Tedrake"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Nicholas_Ezra_Pfaff1",
                            "~Hongkai_Dai1",
                            "~Sergey_Zakharov1",
                            "~Shun_Iwase1",
                            "~Russ_Tedrake1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Scene Generation",
                            "Simulation",
                            "Diffusion",
                            "MCTS"
                        ]
                    },
                    "abstract": {
                        "value": "Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals.\nWe do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution.\nOur method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/8fca0992a3e50a1339d33c5bfc2371f7f7d6fc9e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/524e890f07a53aa17137d6dd0e9e605c5bcd5050.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\npfaff2025steerable,\ntitle={Steerable Scene Generation with Post Training and Inference-Time Search},\nauthor={Nicholas Ezra Pfaff and Hongkai Dai and Sergey Zakharov and Shun Iwase and Russ Tedrake},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oOCa85Z1Ho}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/4040a671a6550cb0779b9fa37cc1d949b793022d.zip"
                    },
                    "paperhash": {
                        "value": "pfaff|steerable_scene_generation_with_post_training_and_inferencetime_search"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission733/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission733/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission733/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745436676398,
                "pdate": 1754680632950,
                "odate": 1758062779407,
                "mdate": 1758062821356,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission733/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "6AASPlloSt",
        "title": "RICL:  Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
        "abstract": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $\\pi_0$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$\\pi_0$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks",
        "keywords": [
            "Vision-Language-Action (VLA) models",
            "In-Context Learning (ICL)",
            "Retrieval-Augmented Generation (RAG)"
        ],
        "pdf_url": "https://openreview.net/pdf/f6c4ea90884ce3fb22cd6fa1783830e77430ff13.pdf",
        "reviews": [
            {
                "id": "ANL0idQSKy",
                "forum": "6AASPlloSt",
                "replyto": "6AASPlloSt",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission731/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068299579,
                "mdate": 1754869472685,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "6AASPlloSt",
                "forum": "6AASPlloSt",
                "content": {
                    "title": {
                        "value": "RICL:  Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models"
                    },
                    "authors": {
                        "value": [
                            "Kaustubh Sridhar",
                            "Souradeep Dutta",
                            "Dinesh Jayaraman",
                            "Insup Lee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kaustubh_Sridhar1",
                            "~Souradeep_Dutta2",
                            "~Dinesh_Jayaraman2",
                            "~Insup_Lee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action (VLA) models",
                            "In-Context Learning (ICL)",
                            "Retrieval-Augmented Generation (RAG)"
                        ]
                    },
                    "abstract": {
                        "value": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $\\pi_0$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$\\pi_0$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks"
                    },
                    "supplementary_material": {
                        "value": "/attachment/f95f7a23e04a81202d98a8dbe70152630813a602.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We added in-context adaptability to pre-trained VLAs so that robots can perform new tasks simply via RAG and in-context learning"
                    },
                    "pdf": {
                        "value": "/pdf/f6c4ea90884ce3fb22cd6fa1783830e77430ff13.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsridhar2025ricl,\ntitle={{RICL}:  Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models},\nauthor={Kaustubh Sridhar and Souradeep Dutta and Dinesh Jayaraman and Insup Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=6AASPlloSt}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/3bcaa92928d00e11981442a71b861f4fc2113bf2.zip"
                    },
                    "paperhash": {
                        "value": "sridhar|ricl_adding_incontext_adaptability_to_pretrained_visionlanguageaction_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission731/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission731/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission731/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745436390259,
                "pdate": 1754680632791,
                "odate": 1758062779238,
                "mdate": 1758062821359,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission731/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission731/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "a2RMXJbkJ8",
        "title": "ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly",
        "abstract": "Generalizable long-horizon robotic assembly requires reasoning at multiple levels of abstraction. While end-to-end imitation learning (IL) is a promising approach, it typically requires large amounts of expert demonstration data and often struggles to achieve the high precision demanded by assembly tasks. Reinforcement learning (RL) approaches, on the other hand, have shown some success in high-precision assembly, but suffer from sample inefficiency, which limits their effectiveness in long-horizon tasks. To address these challenges, we propose a hierarchical modular approach, named Adaptive Robotic Compositional Hierarchy (ARCH), which enables long-horizon, high-precision robotic assembly in contact-rich settings. ARCH employs a hierarchical planning framework, including a low-level primitive library of parameterized skills and a high-level policy. The low-level primitive library includes essential skills for assembly tasks, such as grasping and inserting. These primitives consist of both RL and model-based controllers. The high-level policy, learned via IL from a handful of demonstrations, without the need for teleoperation, selects the appropriate primitive skills and instantiates them with input parameters. We extensively evaluate our approach in simulation and on a real robotic manipulation platform. We show that ARCH generalizes well to unseen objects and outperforms baseline methods in terms of success rate and data efficiency.",
        "keywords": [
            "Long-horizon learning",
            "hybrid learning",
            "robotic assembly"
        ],
        "pdf_url": "https://openreview.net/pdf/86bea7739bc4d2deeac306075efe80aa4a1c135c.pdf",
        "reviews": [
            {
                "id": "cHh9yu11el",
                "forum": "a2RMXJbkJ8",
                "replyto": "a2RMXJbkJ8",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission729/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068299430,
                "mdate": 1754869472629,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "a2RMXJbkJ8",
                "forum": "a2RMXJbkJ8",
                "content": {
                    "title": {
                        "value": "ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly"
                    },
                    "authors": {
                        "value": [
                            "Jiankai Sun",
                            "Aidan Curtis",
                            "Yang You",
                            "Yan Xu",
                            "Michael Koehle",
                            "Qianzhong Chen",
                            "Suning Huang",
                            "Leonidas Guibas",
                            "Sachin Chitta",
                            "Mac Schwager",
                            "Hui Li"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiankai_Sun6",
                            "~Aidan_Curtis2",
                            "~Yang_You2",
                            "~Yan_Xu8",
                            "michael.koehle@autodesk.com",
                            "~Qianzhong_Chen2",
                            "~Suning_Huang1",
                            "~Leonidas_Guibas1",
                            "~Sachin_Chitta1",
                            "~Mac_Schwager1",
                            "~Hui_Li6"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Long-horizon learning",
                            "hybrid learning",
                            "robotic assembly"
                        ]
                    },
                    "abstract": {
                        "value": "Generalizable long-horizon robotic assembly requires reasoning at multiple levels of abstraction. While end-to-end imitation learning (IL) is a promising approach, it typically requires large amounts of expert demonstration data and often struggles to achieve the high precision demanded by assembly tasks. Reinforcement learning (RL) approaches, on the other hand, have shown some success in high-precision assembly, but suffer from sample inefficiency, which limits their effectiveness in long-horizon tasks. To address these challenges, we propose a hierarchical modular approach, named Adaptive Robotic Compositional Hierarchy (ARCH), which enables long-horizon, high-precision robotic assembly in contact-rich settings. ARCH employs a hierarchical planning framework, including a low-level primitive library of parameterized skills and a high-level policy. The low-level primitive library includes essential skills for assembly tasks, such as grasping and inserting. These primitives consist of both RL and model-based controllers. The high-level policy, learned via IL from a handful of demonstrations, without the need for teleoperation, selects the appropriate primitive skills and instantiates them with input parameters. We extensively evaluate our approach in simulation and on a real robotic manipulation platform. We show that ARCH generalizes well to unseen objects and outperforms baseline methods in terms of success rate and data efficiency."
                    },
                    "supplementary_material": {
                        "value": "/attachment/125344729e2bd329a3034df086b14a220bc45c7b.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/86bea7739bc4d2deeac306075efe80aa4a1c135c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsun2025arch,\ntitle={{ARCH}: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly},\nauthor={Jiankai Sun and Aidan Curtis and Yang You and Yan Xu and Michael Koehle and Qianzhong Chen and Suning Huang and Leonidas Guibas and Sachin Chitta and Mac Schwager and Hui Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a2RMXJbkJ8}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/40945782c12ec3bbb6a11e03ce4179bc8ae22fe9.mp4"
                    },
                    "paperhash": {
                        "value": "sun|arch_hierarchical_hybrid_learning_for_longhorizon_contactrich_robotic_assembly"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission729/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission729/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission729/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745436261908,
                "pdate": 1754680632691,
                "odate": 1758062779232,
                "mdate": 1758062821282,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission729/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission729/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "h2K52fhsDU",
        "title": "Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures",
        "abstract": "Pointing gestures provide a natural and efficient way to communicate spatial information in human-machine interaction, yet their potential for 3D object referral remains largely under-explored. To fill this gap, we introduce the task of pointing-based 3D segmentation. In this task, given an image of a person pointing at an object and the 3D point cloud of the environment, the goal is to predict the 3D segmentation mask of the referred object. To enable the standardized evaluation of this task, we introduce POINTR3D, a curated dataset of over 65,000 frames captured with three cameras across four indoor scenes, featuring diverse pointing scenarios. Each frame is annotated with the information of the active hand, the corresponding object ID, and the 3D segmentation mask of the object. To showcase the application of the proposed dataset, we further introduce Pointing3D, a transformer-based architecture that predicts the pointing direction from RGB images and uses this prediction as a prompt to segment the referred object in the 3D point cloud. Experimental results show that Pointing3D outperforms other strong baselines we introduce and lays the groundwork for future research. The dataset, source code, and evaluation tools will be made publicly available to support further research in this area, enabling a natural human-machine interaction.",
        "keywords": [
            "Object Referral",
            "Pointing Gesture",
            "3D Segmentation"
        ],
        "pdf_url": "https://openreview.net/pdf/3b85344249bbbfd79766d3dd729e4c75adf02050.pdf",
        "reviews": [
            {
                "id": "tlrNWLBh4f",
                "forum": "h2K52fhsDU",
                "replyto": "h2K52fhsDU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission722/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068298987,
                "mdate": 1754869472541,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "h2K52fhsDU",
                "forum": "h2K52fhsDU",
                "content": {
                    "title": {
                        "value": "Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures"
                    },
                    "authors": {
                        "value": [
                            "Mert Arslanoglu",
                            "Kadir Yilmaz",
                            "Cemhan Kaan Özaltan",
                            "Timm Linder",
                            "Bastian Leibe"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mert_Arslanoglu1",
                            "~Kadir_Yilmaz1",
                            "~Cemhan_Kaan_Özaltan1",
                            "~Timm_Linder1",
                            "~Bastian_Leibe3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Object Referral",
                            "Pointing Gesture",
                            "3D Segmentation"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a new benchmark for the evaluation of pointing gesture based object referral in 3D point clouds."
                    },
                    "abstract": {
                        "value": "Pointing gestures provide a natural and efficient way to communicate spatial information in human-machine interaction, yet their potential for 3D object referral remains largely under-explored. To fill this gap, we introduce the task of pointing-based 3D segmentation. In this task, given an image of a person pointing at an object and the 3D point cloud of the environment, the goal is to predict the 3D segmentation mask of the referred object. To enable the standardized evaluation of this task, we introduce POINTR3D, a curated dataset of over 65,000 frames captured with three cameras across four indoor scenes, featuring diverse pointing scenarios. Each frame is annotated with the information of the active hand, the corresponding object ID, and the 3D segmentation mask of the object. To showcase the application of the proposed dataset, we further introduce Pointing3D, a transformer-based architecture that predicts the pointing direction from RGB images and uses this prediction as a prompt to segment the referred object in the 3D point cloud. Experimental results show that Pointing3D outperforms other strong baselines we introduce and lays the groundwork for future research. The dataset, source code, and evaluation tools will be made publicly available to support further research in this area, enabling a natural human-machine interaction."
                    },
                    "supplementary_material": {
                        "value": "/attachment/4a4a96b8fd47c3b349c07ea1921d4566c8090d1a.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3b85344249bbbfd79766d3dd729e4c75adf02050.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\narslanoglu2025pointingd,\ntitle={Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures},\nauthor={Mert Arslanoglu and Kadir Yilmaz and Cemhan Kaan {\\\"O}zaltan and Timm Linder and Bastian Leibe},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=h2K52fhsDU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/33c4b0bf445b21a2543ee938f20d4161723e1341.mp4"
                    },
                    "paperhash": {
                        "value": "arslanoglu|pointing3d_a_benchmark_for_3d_object_referral_via_pointing_gestures"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission722/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission722/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission722/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745434508979,
                "pdate": 1754680632267,
                "odate": 1758062778858,
                "mdate": 1758062821125,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission722/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zK2SK6WbYn",
        "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation",
        "abstract": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a merge and disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3.",
        "keywords": [
            "reinforcement learning",
            "rl",
            "visual reinforcement learning",
            "robot learning",
            "robot manipulation",
            "representation learning",
            "augmentation",
            "augmentations reinforcement learning",
            "multi view reinforcement learning",
            "multi view robot learning"
        ],
        "pdf_url": "https://openreview.net/pdf/398d447705152655c7453e3783c3dc773231a305.pdf",
        "reviews": [
            {
                "id": "WjSMiyxpn4",
                "forum": "zK2SK6WbYn",
                "replyto": "zK2SK6WbYn",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission719/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068298839,
                "mdate": 1754869472488,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zK2SK6WbYn",
                "forum": "zK2SK6WbYn",
                "content": {
                    "title": {
                        "value": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Abdulaziz Almuzairee",
                            "Rohan Prashant Patil",
                            "Dwait Bhatt",
                            "Henrik I Christensen"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Abdulaziz_Almuzairee1",
                            "~Rohan_Prashant_Patil1",
                            "~Dwait_Bhatt1",
                            "~Henrik_I_Christensen1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "reinforcement learning",
                            "rl",
                            "visual reinforcement learning",
                            "robot learning",
                            "robot manipulation",
                            "representation learning",
                            "augmentation",
                            "augmentations reinforcement learning",
                            "multi view reinforcement learning",
                            "multi view robot learning"
                        ]
                    },
                    "abstract": {
                        "value": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a merge and disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/398d447705152655c7453e3783c3dc773231a305.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nalmuzairee2025merging,\ntitle={Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation},\nauthor={Abdulaziz Almuzairee and Rohan Prashant Patil and Dwait Bhatt and Henrik I Christensen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zK2SK6WbYn}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/720e840b3d1745db4dce7ef7cc6dad8001b5ab26.mp4"
                    },
                    "paperhash": {
                        "value": "almuzairee|merging_and_disentangling_views_in_visual_reinforcement_learning_for_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission719/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission719/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745433833662,
                "pdate": 1754680632119,
                "odate": 1758062778757,
                "mdate": 1758062821096,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission719/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission719/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "KHWIHwnYbn",
        "title": "Learning Deployable Locomotion Control via Differentiable Simulation",
        "abstract": "Differentiable simulators promise to improve sample efficiency in robot learning by providing analytic gradients of the system dynamics. Yet, their application to contact-rich tasks like locomotion is complicated by the inherently non-smooth nature of contact, impeding effective gradient-based optimization. Existing works thus often rely on soft contact models that provide smooth gradients but lack physical accuracy, constraining results to simulation. To address this limitation, we propose a differentiable contact model designed to provide informative gradients while maintaining high physical fidelity. We demonstrate the efficacy of our approach by training a quadrupedal locomotion policy within our differentiable simulator leveraging analytic gradients and successfully transferring the learned policy zero-shot to the real world. To the best of our knowledge, this represents the first successful sim-to-real transfer of a legged locomotion policy learned entirely within a differentiable simulator, establishing the feasibility of using differentiable simulation for real-world locomotion control.",
        "keywords": [
            "Differentiable Simulation",
            "Contact Modeling",
            "Quadruped Locomotion"
        ],
        "pdf_url": "https://openreview.net/pdf/2fb81df00ddc2e5143c63178463e3a68156a6fd0.pdf",
        "reviews": [
            {
                "id": "QXRBQpq1R5",
                "forum": "KHWIHwnYbn",
                "replyto": "KHWIHwnYbn",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission710/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068298150,
                "mdate": 1754869472421,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "KHWIHwnYbn",
                "forum": "KHWIHwnYbn",
                "content": {
                    "title": {
                        "value": "Learning Deployable Locomotion Control via Differentiable Simulation"
                    },
                    "authors": {
                        "value": [
                            "Clemens Schwarke",
                            "Victor Klemm",
                            "Joshua Bagajo",
                            "Jean Pierre Sleiman",
                            "Ignat Georgiev",
                            "Jesus Tordesillas Torres",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Clemens_Schwarke1",
                            "~Victor_Klemm1",
                            "~Joshua_Bagajo1",
                            "~Jean_Pierre_Sleiman1",
                            "~Ignat_Georgiev1",
                            "~Jesus_Tordesillas_Torres1",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Differentiable Simulation",
                            "Contact Modeling",
                            "Quadruped Locomotion"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper proposes a differentiable contact model that allows learning legged locomotion policies leveraging analytic gradients, while maintaining physical accuracy for successful sim-to-real transfer."
                    },
                    "abstract": {
                        "value": "Differentiable simulators promise to improve sample efficiency in robot learning by providing analytic gradients of the system dynamics. Yet, their application to contact-rich tasks like locomotion is complicated by the inherently non-smooth nature of contact, impeding effective gradient-based optimization. Existing works thus often rely on soft contact models that provide smooth gradients but lack physical accuracy, constraining results to simulation. To address this limitation, we propose a differentiable contact model designed to provide informative gradients while maintaining high physical fidelity. We demonstrate the efficacy of our approach by training a quadrupedal locomotion policy within our differentiable simulator leveraging analytic gradients and successfully transferring the learned policy zero-shot to the real world. To the best of our knowledge, this represents the first successful sim-to-real transfer of a legged locomotion policy learned entirely within a differentiable simulator, establishing the feasibility of using differentiable simulation for real-world locomotion control."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2fb81df00ddc2e5143c63178463e3a68156a6fd0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nschwarke2025learning,\ntitle={Learning Deployable Locomotion Control via Differentiable Simulation},\nauthor={Clemens Schwarke and Victor Klemm and Joshua Bagajo and Jean Pierre Sleiman and Ignat Georgiev and Jesus Tordesillas Torres and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KHWIHwnYbn}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/de0ff39f7c18df3c54af485931bb3fbc81b33573.zip"
                    },
                    "paperhash": {
                        "value": "schwarke|learning_deployable_locomotion_control_via_differentiable_simulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission710/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission710/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission710/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745431113739,
                "pdate": 1754680631622,
                "odate": 1758062778437,
                "mdate": 1758062821000,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission710/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission710/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "v2KevjWScT",
        "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
        "abstract": "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/.",
        "keywords": [
            "Whole-Body Manipulation",
            "Mobile Manipulation",
            "Household Tasks",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/501ca557743547ea4005cd3edb8e38357f82fe86.pdf",
        "reviews": [
            {
                "id": "VP2SNi6qcZ",
                "forum": "v2KevjWScT",
                "replyto": "v2KevjWScT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission707/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297873,
                "mdate": 1754869472352,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "v2KevjWScT",
                "forum": "v2KevjWScT",
                "content": {
                    "title": {
                        "value": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities"
                    },
                    "authors": {
                        "value": [
                            "Yunfan Jiang",
                            "Ruohan Zhang",
                            "Josiah Wong",
                            "Chen Wang",
                            "Yanjie Ze",
                            "Hang Yin",
                            "Cem Gokmen",
                            "Shuran Song",
                            "Jiajun Wu",
                            "Li Fei-Fei"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yunfan_Jiang1",
                            "~Ruohan_Zhang1",
                            "~Josiah_Wong1",
                            "~Chen_Wang16",
                            "~Yanjie_Ze1",
                            "~Hang_Yin11",
                            "~Cem_Gokmen1",
                            "~Shuran_Song3",
                            "~Jiajun_Wu1",
                            "~Li_Fei-Fei1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Whole-Body Manipulation",
                            "Mobile Manipulation",
                            "Household Tasks",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "BRS is a comprehensive framework for learning whole-body manipulation in real-world household tasks, combining an advanced policy learning method with novel hardware design."
                    },
                    "abstract": {
                        "value": "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/501ca557743547ea4005cd3edb8e38357f82fe86.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njiang2025behavior,\ntitle={{BEHAVIOR} Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities},\nauthor={Yunfan Jiang and Ruohan Zhang and Josiah Wong and Chen Wang and Yanjie Ze and Hang Yin and Cem Gokmen and Shuran Song and Jiajun Wu and Li Fei-Fei},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=v2KevjWScT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d4354f1068a4c85c5dc2ed06dce6f726c9a58146.zip"
                    },
                    "paperhash": {
                        "value": "jiang|behavior_robot_suite_streamlining_realworld_wholebody_manipulation_for_everyday_household_activities"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission707/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission707/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission707/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745430844724,
                "pdate": 1754680631549,
                "odate": 1758062778346,
                "mdate": 1758062820840,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission707/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission707/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "aZwWRycAXi",
        "title": "GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping",
        "abstract": "Dexterous robotic hands enable versatile interactions through the flexibility and adaptability of a multi-finger setup, allowing for a wise range of task-specific grasp configurations in diverse environments.\nHowever, access to diverse and high-quality grasp data is essential to fully exploit the capabilities of dexterous hands, be it to train grasp prediction models from point clouds, train manipulation policies, or to support high-level task planning with a broader range of action options.\nExisting approaches for dataset generation rely on sampling-based algorithms or simplified force-closure analysis, which tend to converge to power grasps and often exhibit limited diversity.\nIn this work, we propose a method to synthesize large-scale, diverse, and physically feasible grasps that additionally go beyond simple power grasps to more refined manipulation, such as pinches or tri-finger precision grasps.\nWe introduce a rigorous differentiable energy formulation of force closure, implicitly defined through a Quadratic Program (QP).\nIn addition, we present an adjusted optimization method (MALA*) that improves performance by dynamically rejecting gradient steps based on the global sample distribution.\nWe extensively evaluate our approach and demonstrate significant improvements in both grasp diversity and the stability of final grasp predictions. Finally, we provide a new, large-scale grasp dataset for the 5'700 objects from DexGraspNet, consisting of five different grippers and three different grasp types.",
        "keywords": [
            "Grasping",
            "Manipulation",
            "Optimization"
        ],
        "pdf_url": "https://openreview.net/pdf/bf6154f7e9e0ff8beda93f75d742f343492f4a65.pdf",
        "reviews": [
            {
                "id": "floHF2vv02",
                "forum": "aZwWRycAXi",
                "replyto": "aZwWRycAXi",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission706/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297854,
                "mdate": 1754869472316,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "aZwWRycAXi",
                "forum": "aZwWRycAXi",
                "content": {
                    "title": {
                        "value": "GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping"
                    },
                    "authors": {
                        "value": [
                            "René Zurbrügg",
                            "Andrei Cramariuc",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~René_Zurbrügg1",
                            "~Andrei_Cramariuc1",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Grasping",
                            "Manipulation",
                            "Optimization"
                        ]
                    },
                    "abstract": {
                        "value": "Dexterous robotic hands enable versatile interactions through the flexibility and adaptability of a multi-finger setup, allowing for a wise range of task-specific grasp configurations in diverse environments.\nHowever, access to diverse and high-quality grasp data is essential to fully exploit the capabilities of dexterous hands, be it to train grasp prediction models from point clouds, train manipulation policies, or to support high-level task planning with a broader range of action options.\nExisting approaches for dataset generation rely on sampling-based algorithms or simplified force-closure analysis, which tend to converge to power grasps and often exhibit limited diversity.\nIn this work, we propose a method to synthesize large-scale, diverse, and physically feasible grasps that additionally go beyond simple power grasps to more refined manipulation, such as pinches or tri-finger precision grasps.\nWe introduce a rigorous differentiable energy formulation of force closure, implicitly defined through a Quadratic Program (QP).\nIn addition, we present an adjusted optimization method (MALA*) that improves performance by dynamically rejecting gradient steps based on the global sample distribution.\nWe extensively evaluate our approach and demonstrate significant improvements in both grasp diversity and the stability of final grasp predictions. Finally, we provide a new, large-scale grasp dataset for the 5'700 objects from DexGraspNet, consisting of five different grippers and three different grasp types."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/bf6154f7e9e0ff8beda93f75d742f343492f4a65.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzurbrugg2025graspqp,\ntitle={Grasp{QP}: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping},\nauthor={Ren{\\'e} Zurbr{\\\"u}gg and Andrei Cramariuc and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=aZwWRycAXi}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a6ffb88add8ebd59b4ca4c8da145e1c3dac7bf25.mp4"
                    },
                    "paperhash": {
                        "value": "zurbrügg|graspqp_differentiable_optimization_of_force_closure_for_diverse_and_robust_dexterous_grasping"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission706/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission706/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission706/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745430831955,
                "pdate": 1754680631447,
                "odate": 1758062778299,
                "mdate": 1758062820847,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission706/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission706/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Ddb8w8FVV9",
        "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors",
        "abstract": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn diverse behaviors without task-specific rewards. While recent USD methods have shown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in safety, interpretability, and deployability of the learned skills.\nOur approach factorizes the state space to learn disentangled skill representations and assigns different skill discovery algorithms to each factor based on the desired intrinsic reward function.\nTo encourage structured morphology-aware skills, we introduce symmetry-based inductive biases tailored to individual factors. We also incorporate a style factor and regularization penalties to promote safe and robust behaviors.\nWe evaluate our framework in simulation using a quadrupedal robot and demonstrate zero-shot transfer of the learned skills to real hardware. Our results show that factorization and symmetry lead to the discovery of structured, human-interpretable behaviors, while the style factor and penalties enhance safety and diversity. Additionally, we show that the learned skills can be used for downstream tasks and perform on par with oracle policies trained with hand-crafted rewards.\nTo facilitate future research, we will release our code upon publication.",
        "keywords": [
            "unsupervised skill discovery",
            "reinforcement learning",
            "legged robots"
        ],
        "pdf_url": "https://openreview.net/pdf/7e3f14c6f6ce0c2405376e567d4319520b46dba1.pdf",
        "reviews": [
            {
                "id": "tAe7AzWnUo",
                "forum": "Ddb8w8FVV9",
                "replyto": "Ddb8w8FVV9",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission702/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297588,
                "mdate": 1754869457484,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Ddb8w8FVV9",
                "forum": "Ddb8w8FVV9",
                "content": {
                    "title": {
                        "value": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors"
                    },
                    "authors": {
                        "value": [
                            "Rafael Cathomen",
                            "Mayank Mittal",
                            "Marin Vlastelica",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Rafael_Cathomen1",
                            "~Mayank_Mittal1",
                            "~Marin_Vlastelica1",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "unsupervised skill discovery",
                            "reinforcement learning",
                            "legged robots"
                        ]
                    },
                    "abstract": {
                        "value": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn diverse behaviors without task-specific rewards. While recent USD methods have shown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in safety, interpretability, and deployability of the learned skills.\nOur approach factorizes the state space to learn disentangled skill representations and assigns different skill discovery algorithms to each factor based on the desired intrinsic reward function.\nTo encourage structured morphology-aware skills, we introduce symmetry-based inductive biases tailored to individual factors. We also incorporate a style factor and regularization penalties to promote safe and robust behaviors.\nWe evaluate our framework in simulation using a quadrupedal robot and demonstrate zero-shot transfer of the learned skills to real hardware. Our results show that factorization and symmetry lead to the discovery of structured, human-interpretable behaviors, while the style factor and penalties enhance safety and diversity. Additionally, we show that the learned skills can be used for downstream tasks and perform on par with oracle policies trained with hand-crafted rewards.\nTo facilitate future research, we will release our code upon publication."
                    },
                    "supplementary_material": {
                        "value": "/attachment/03b040533a446a9ea1481ebba4b0ae110557be5b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A modular framework for unsupervised skill discovery that factorizes the state space, integrates symmetry and safety priors, and enables zero-shot deployment of structured, interpretable skills on real quadrupedal robots."
                    },
                    "pdf": {
                        "value": "/pdf/7e3f14c6f6ce0c2405376e567d4319520b46dba1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ncathomen2025divide,\ntitle={Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors},\nauthor={Rafael Cathomen and Mayank Mittal and Marin Vlastelica and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ddb8w8FVV9}\n}"
                    },
                    "paperhash": {
                        "value": "cathomen|divide_discover_deploy_factorized_skill_learning_with_symmetry_and_style_priors"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission702/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission702/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission702/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745430544097,
                "pdate": 1754680631364,
                "odate": 1758062778215,
                "mdate": 1758062820724,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission702/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1TdRe3wPqK",
        "title": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence",
        "abstract": "End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions---but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by checking if current observations are OOD and the most similar training observations show divergent behaviors (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback.",
        "keywords": [
            "Visuomotor Policy",
            "Out-of-Distribution Generalization",
            "Functional Correspondence",
            "Deployment-Time Adaptation"
        ],
        "pdf_url": "https://openreview.net/pdf/4eefb16c2c1e438fda3eeb43d2befc007e9ef071.pdf",
        "reviews": [
            {
                "id": "RufEUHGuDK",
                "forum": "1TdRe3wPqK",
                "replyto": "1TdRe3wPqK",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission700/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297509,
                "mdate": 1754869472258,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1TdRe3wPqK",
                "forum": "1TdRe3wPqK",
                "content": {
                    "title": {
                        "value": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence"
                    },
                    "authors": {
                        "value": [
                            "Pranay Gupta",
                            "Henny Admoni",
                            "Andrea Bajcsy"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Pranay_Gupta1",
                            "~Henny_Admoni1",
                            "~Andrea_Bajcsy1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Visuomotor Policy",
                            "Out-of-Distribution Generalization",
                            "Functional Correspondence",
                            "Deployment-Time Adaptation"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a novel method that establishes functional correspondences between out-of-distribution and in-distribution observations for deployment-time generalization of visuomotor policies, through minimal feedback."
                    },
                    "abstract": {
                        "value": "End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions---but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by checking if current observations are OOD and the most similar training observations show divergent behaviors (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback."
                    },
                    "supplementary_material": {
                        "value": "/attachment/70d35e6fd5c51e7140c3657a09a60ac4452147c3.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4eefb16c2c1e438fda3eeb43d2befc007e9ef071.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngupta2025adapting,\ntitle={Adapting by Analogy: {OOD} Generalization of Visuomotor Policies via Functional Correspondence},\nauthor={Pranay Gupta and Henny Admoni and Andrea Bajcsy},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1TdRe3wPqK}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ac6cf69df82a112a6d5a05dde0d8d049de0009b5.mp4"
                    },
                    "paperhash": {
                        "value": "gupta|adapting_by_analogy_ood_generalization_of_visuomotor_policies_via_functional_correspondence"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission700/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission700/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission700/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745430340474,
                "pdate": 1754680631204,
                "odate": 1758062778166,
                "mdate": 1758062820619,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission700/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "QtVZUPCKrY",
        "title": "Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps",
        "abstract": "A robot navigating an outdoor environment with no prior knowledge of the space must rely on its local sensing, which is in the form of a local metric map or local policy with some fixed horizon. A limited planning horizon can often result in myopic decisions leading the robot off course or worse, into very difficult terrain. In this work, we make a key observation that long range navigation only necessitates identifying good frontier directions for planning instead of full map knowledge. To address this, we introduce Long Range Navigator (LRN), which learns to predict ‘affordable’ frontier directions from high-dimensional camera images. LRN is trained entirely on unlabeled egocentric videos, making it scalable and adaptable. In off-road tests on Spot and a large vehicle, LRN reduces human interventions and improves decision speed when integrated into existing navigation stacks.",
        "keywords": [
            "Robot Perception",
            "Sensing & Vision",
            "Robot Planning",
            "Navigation",
            "Field Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/44cf524b75c9002992cf547b85ec49e8a735a82f.pdf",
        "reviews": [
            {
                "id": "1L0PeDac8u",
                "forum": "QtVZUPCKrY",
                "replyto": "QtVZUPCKrY",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission696/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297354,
                "mdate": 1754869472166,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "QtVZUPCKrY",
                "forum": "QtVZUPCKrY",
                "content": {
                    "title": {
                        "value": "Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps"
                    },
                    "authors": {
                        "value": [
                            "Matt Schmittle",
                            "Rohan Baijal",
                            "Nathan Hatch",
                            "Rosario Scalise",
                            "Mateo Guaman Castro",
                            "Sidharth Talia",
                            "Khimya Khetarpal",
                            "Byron Boots",
                            "Siddhartha Srinivasa"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Matt_Schmittle1",
                            "~Rohan_Baijal1",
                            "~Nathan_Hatch1",
                            "~Rosario_Scalise1",
                            "~Mateo_Guaman_Castro1",
                            "~Sidharth_Talia1",
                            "~Khimya_Khetarpal1",
                            "~Byron_Boots1",
                            "~Siddhartha_Srinivasa1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Perception",
                            "Sensing & Vision",
                            "Robot Planning",
                            "Navigation",
                            "Field Robotics"
                        ]
                    },
                    "TLDR": {
                        "value": "We present the Long Range Navigator (LRN), a learned system that uses camera data to identify affordable frontiers beyond the range of local maps, extending the planning horizon for robots navigating without prior maps."
                    },
                    "abstract": {
                        "value": "A robot navigating an outdoor environment with no prior knowledge of the space must rely on its local sensing, which is in the form of a local metric map or local policy with some fixed horizon. A limited planning horizon can often result in myopic decisions leading the robot off course or worse, into very difficult terrain. In this work, we make a key observation that long range navigation only necessitates identifying good frontier directions for planning instead of full map knowledge. To address this, we introduce Long Range Navigator (LRN), which learns to predict ‘affordable’ frontier directions from high-dimensional camera images. LRN is trained entirely on unlabeled egocentric videos, making it scalable and adaptable. In off-road tests on Spot and a large vehicle, LRN reduces human interventions and improves decision speed when integrated into existing navigation stacks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/64336ea9bb331ae6e6f9904de1d02d8c32d6657e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/44cf524b75c9002992cf547b85ec49e8a735a82f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nschmittle2025long,\ntitle={Long Range Navigator ({LRN}): Extending robot planning horizons beyond metric maps},\nauthor={Matt Schmittle and Rohan Baijal and Nathan Hatch and Rosario Scalise and Mateo Guaman Castro and Sidharth Talia and Khimya Khetarpal and Byron Boots and Siddhartha Srinivasa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=QtVZUPCKrY}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/fe9ed51700fe4a06992bf36fc0cd770102cfa54a.mp4"
                    },
                    "paperhash": {
                        "value": "schmittle|long_range_navigator_lrn_extending_robot_planning_horizons_beyond_metric_maps"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission696/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission696/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission696/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745429639768,
                "pdate": 1754680631051,
                "odate": 1758062778057,
                "mdate": 1758062820588,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission696/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9lCTcsmZMV",
        "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams",
        "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.",
        "keywords": [
            "Robot Soccer",
            "Multi-Agent Reinforcement Learning",
            "Legged Robots"
        ],
        "pdf_url": "https://openreview.net/pdf/db2a2d8d29893b7ef0b01bcd86fb3f48ffaf41f8.pdf",
        "reviews": [
            {
                "id": "6HJBfHoPpI",
                "forum": "9lCTcsmZMV",
                "replyto": "9lCTcsmZMV",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission692/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297069,
                "mdate": 1754869472116,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9lCTcsmZMV",
                "forum": "9lCTcsmZMV",
                "content": {
                    "title": {
                        "value": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams"
                    },
                    "authors": {
                        "value": [
                            "Zhi Su",
                            "Yuman Gao",
                            "Emily Lukas",
                            "Yunfei Li",
                            "Jiaze Cai",
                            "Faris Tulbah",
                            "Fei Gao",
                            "Chao Yu",
                            "Zhongyu Li",
                            "Yi Wu",
                            "Koushil Sreenath"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhi_Su1",
                            "~Yuman_Gao1",
                            "~Emily_Lukas1",
                            "~Yunfei_Li1",
                            "~Jiaze_Cai1",
                            "~Faris_Tulbah1",
                            "~Fei_Gao16",
                            "~Chao_Yu1",
                            "~Zhongyu_Li3",
                            "~Yi_Wu1",
                            "~Koushil_Sreenath1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Soccer",
                            "Multi-Agent Reinforcement Learning",
                            "Legged Robots"
                        ]
                    },
                    "abstract": {
                        "value": "Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c7e9dd28c008c5b2f0ce4590ec1c74125c95b669.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/db2a2d8d29893b7ef0b01bcd86fb3f48ffaf41f8.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsu2025toward,\ntitle={Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams},\nauthor={Zhi Su and Yuman Gao and Emily Lukas and Yunfei Li and Jiaze Cai and Faris Talubah and Fei Gao and Chao Yu and Zhongyu Li and Yi Wu and Koushil Sreenath},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9lCTcsmZMV}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5d7a7bdde1ed971fd091350bfd6892b7b9369295.zip"
                    },
                    "paperhash": {
                        "value": "su|toward_realworld_cooperative_and_competitive_soccer_with_quadrupedal_robot_teams"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission692/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission692/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission692/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745428622084,
                "pdate": 1754680630920,
                "odate": 1758062777892,
                "mdate": 1759282689520,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission692/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "sVWKm4UiTL",
        "title": "Multi-critic Learning for Whole-body End-effector Twist Tracking",
        "abstract": "Learning whole-body control for locomotion and arm motions in a single policy has challenges, as the two tasks have conflicting goals. For instance, efficient locomotion typically favors a horizontal base orientation, while end-effector tracking may benefit from base tilting to extend reachability. Additionally, current Reinforcement Learning (RL) approaches using a pose-based task specification lack the ability to directly control the end-effector velocity, making smoothly executing trajectories very challenging. To address these limitations, we propose an RL-based framework that allows for dynamic, velocity-aware whole-body end-effector control. Our method introduces a multi-critic actor architecture that decouples the reward signals for locomotion and manipulation, simplifying reward tuning and allowing the policy to resolve task conflicts more effectively. Furthermore, we design a twist-based end-effector task formulation that can track both discrete poses and motion trajectories. We validate our approach through a set of simulation and hardware experiments using a quadruped robot equipped with a robotic arm. The resulting controller can simultaneously walk and move its end-effector and shows emergent whole-body behaviors, where the base assists the arm in extending the workspace, despite a lack of explicit formulations.",
        "keywords": [
            "Loco-Manipulation",
            "Multi-critic Reinforcement Learning",
            "Whole-Body Control"
        ],
        "pdf_url": "https://openreview.net/pdf/6ce9785277c45f56484741fc4678003292e8abae.pdf",
        "reviews": [
            {
                "id": "12Ct2z56kq",
                "forum": "sVWKm4UiTL",
                "replyto": "sVWKm4UiTL",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission688/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068297112,
                "mdate": 1754869471972,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "sVWKm4UiTL",
                "forum": "sVWKm4UiTL",
                "content": {
                    "title": {
                        "value": "Multi-critic Learning for Whole-body End-effector Twist Tracking"
                    },
                    "authors": {
                        "value": [
                            "Aravind Elanjimattathil Vijayan",
                            "Andrei Cramariuc",
                            "Mattia Risiglione",
                            "Christian Gehring",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Aravind_Elanjimattathil_Vijayan1",
                            "~Andrei_Cramariuc1",
                            "~Mattia_Risiglione1",
                            "cgehring@anybotics.com",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Loco-Manipulation",
                            "Multi-critic Reinforcement Learning",
                            "Whole-Body Control"
                        ]
                    },
                    "abstract": {
                        "value": "Learning whole-body control for locomotion and arm motions in a single policy has challenges, as the two tasks have conflicting goals. For instance, efficient locomotion typically favors a horizontal base orientation, while end-effector tracking may benefit from base tilting to extend reachability. Additionally, current Reinforcement Learning (RL) approaches using a pose-based task specification lack the ability to directly control the end-effector velocity, making smoothly executing trajectories very challenging. To address these limitations, we propose an RL-based framework that allows for dynamic, velocity-aware whole-body end-effector control. Our method introduces a multi-critic actor architecture that decouples the reward signals for locomotion and manipulation, simplifying reward tuning and allowing the policy to resolve task conflicts more effectively. Furthermore, we design a twist-based end-effector task formulation that can track both discrete poses and motion trajectories. We validate our approach through a set of simulation and hardware experiments using a quadruped robot equipped with a robotic arm. The resulting controller can simultaneously walk and move its end-effector and shows emergent whole-body behaviors, where the base assists the arm in extending the workspace, despite a lack of explicit formulations."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7e1e732da63987820a2194624912d00e12899949.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6ce9785277c45f56484741fc4678003292e8abae.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nvijayan2025multicritic,\ntitle={Multi-critic Learning for Whole-body End-effector Twist Tracking},\nauthor={Aravind Elanjimattathil Vijayan and Andrei Cramariuc and Mattia Risiglione and Christian Gehring and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sVWKm4UiTL}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8b8b516cc970cb64d6ec2153584bedab21d6f127.mp4"
                    },
                    "paperhash": {
                        "value": "vijayan|multicritic_learning_for_wholebody_endeffector_twist_tracking"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission688/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission688/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission688/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745427685395,
                "pdate": 1754680630776,
                "odate": 1758062777822,
                "mdate": 1758062820376,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission688/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission688/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JEGOuknVml",
        "title": "SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps",
        "abstract": "We present SIREN for registration of multi-robot\nGaussian Splatting (GSplat) maps, with zero access to camera\nposes, images, and inter-map transforms for initialization or fusion\nof local submaps. To realize these capabilities, SIREN harnesses\nthe versatility and robustness of semantics in three critical ways to\nderive a rigorous registration pipeline for multi-robot GSplat maps.\nFirst, SIREN utilizes semantics to identify feature-rich regions\nof the local maps where the registration problem is better posed,\neliminating the need for any initialization which is generally\nrequired in prior work. Second, SIREN identifies candidate\ncorrespondences between Gaussians in the local maps using\nrobust semantic features, constituting the foundation for robust\ngeometric optimization, coarsely aligning 3D Gaussian primitives\nextracted from the local maps. Third, this key step enables\nsubsequent photometric refinement of the transformation between\nthe submaps, where SIREN leverages novel-view synthesis in\nGSplat maps along with a semantics-based image filter to compute\na high-accuracy non-rigid transformation for the generation of a\nhigh-fidelity fused map. We demonstrate the superior performance\nof SIREN compared to competing baselines across a range of\nreal-world datasets, and in particular, across the most widely\nused robot hardware platforms, including a manipulator, drone,\nand quadruped. \nIn fact, in the most challenging scenes\nwhere accurate feature matching is extremely challenging,\nSIREN achieves about 90x\nsmaller rotation errors, 300x smaller translation errors, and\n44x smaller scale errors, compared to\ncompeting methods.\nWe will release the code and provide\na link to the project page after the review process.",
        "keywords": [
            "Multi-Robot Radiance Field Mapping",
            "Gaussian Splatting"
        ],
        "pdf_url": "https://openreview.net/pdf/f4b4dd5c017a6e5ddc60750f72971402e949216c.pdf",
        "reviews": [
            {
                "id": "yjOZF2igDp",
                "forum": "JEGOuknVml",
                "replyto": "JEGOuknVml",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission682/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068296690,
                "mdate": 1754869471898,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JEGOuknVml",
                "forum": "JEGOuknVml",
                "content": {
                    "title": {
                        "value": "SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps"
                    },
                    "authors": {
                        "value": [
                            "Olao Shorinwa",
                            "Jiankai Sun",
                            "Mac Schwager",
                            "Anirudha Majumdar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Olao_Shorinwa1",
                            "~Jiankai_Sun6",
                            "~Mac_Schwager1",
                            "~Anirudha_Majumdar1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-Robot Radiance Field Mapping",
                            "Gaussian Splatting"
                        ]
                    },
                    "TLDR": {
                        "value": "We introduce an algorithm for registering multi-robot Gaussian Splatting maps, leveraging rich robust semantic distillation and feature matching for high-accuracy map fusion with no initialization."
                    },
                    "abstract": {
                        "value": "We present SIREN for registration of multi-robot\nGaussian Splatting (GSplat) maps, with zero access to camera\nposes, images, and inter-map transforms for initialization or fusion\nof local submaps. To realize these capabilities, SIREN harnesses\nthe versatility and robustness of semantics in three critical ways to\nderive a rigorous registration pipeline for multi-robot GSplat maps.\nFirst, SIREN utilizes semantics to identify feature-rich regions\nof the local maps where the registration problem is better posed,\neliminating the need for any initialization which is generally\nrequired in prior work. Second, SIREN identifies candidate\ncorrespondences between Gaussians in the local maps using\nrobust semantic features, constituting the foundation for robust\ngeometric optimization, coarsely aligning 3D Gaussian primitives\nextracted from the local maps. Third, this key step enables\nsubsequent photometric refinement of the transformation between\nthe submaps, where SIREN leverages novel-view synthesis in\nGSplat maps along with a semantics-based image filter to compute\na high-accuracy non-rigid transformation for the generation of a\nhigh-fidelity fused map. We demonstrate the superior performance\nof SIREN compared to competing baselines across a range of\nreal-world datasets, and in particular, across the most widely\nused robot hardware platforms, including a manipulator, drone,\nand quadruped. \nIn fact, in the most challenging scenes\nwhere accurate feature matching is extremely challenging,\nSIREN achieves about 90x\nsmaller rotation errors, 300x smaller translation errors, and\n44x smaller scale errors, compared to\ncompeting methods.\nWe will release the code and provide\na link to the project page after the review process."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f4b4dd5c017a6e5ddc60750f72971402e949216c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nshorinwa2025siren,\ntitle={{SIREN}: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps},\nauthor={Olao Shorinwa and Jiankai Sun and Mac Schwager and Anirudha Majumdar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JEGOuknVml}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/76ef307809714792a0f0e8447e0cf52e4fddc5a4.mp4"
                    },
                    "paperhash": {
                        "value": "shorinwa|siren_semantic_initializationfree_registration_of_multirobot_gaussian_splatting_maps"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission682/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission682/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission682/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745427171979,
                "pdate": 1754680630439,
                "odate": 1758062777632,
                "mdate": 1758062820349,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission682/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission682/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "2CIKnIwSta",
        "title": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference",
        "abstract": "With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system.",
        "keywords": [
            "Passive Impedance Control",
            "Learning Residual Inverse Dynamics",
            "Model Mismatch Estimation"
        ],
        "pdf_url": "https://openreview.net/pdf/2ce45d18471477f2628d66dd182265518c03766b.pdf",
        "reviews": [
            {
                "id": "VssRsqVM3i",
                "forum": "2CIKnIwSta",
                "replyto": "2CIKnIwSta",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission675/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068296574,
                "mdate": 1754869471821,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "2CIKnIwSta",
                "forum": "2CIKnIwSta",
                "content": {
                    "title": {
                        "value": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference"
                    },
                    "authors": {
                        "value": [
                            "Mateusz Jaszczuk",
                            "Nadia Figueroa"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mateusz_Jaszczuk1",
                            "~Nadia_Figueroa1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Passive Impedance Control",
                            "Learning Residual Inverse Dynamics",
                            "Model Mismatch Estimation"
                        ]
                    },
                    "abstract": {
                        "value": "With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e07ba61eab56bfa68b852afda8956dbbc4c18dec.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We propose an online, probabilistic framework for estimating the mismatch in the end-effector dynamics model, allowing impedance-controlled robots to manipulate heavy, unknown objects, showcasing fast and safe adaptation."
                    },
                    "pdf": {
                        "value": "/pdf/2ce45d18471477f2628d66dd182265518c03766b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njaszczuk2025rapid,\ntitle={Rapid Mismatch Estimation via Neural Network Informed Variational Inference},\nauthor={Mateusz Jaszczuk and Nadia Figueroa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2CIKnIwSta}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c6cb9c4525c122f65543e94c8c668561b27145b4.mp4"
                    },
                    "paperhash": {
                        "value": "jaszczuk|rapid_mismatch_estimation_via_neural_network_informed_variational_inference"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission675/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission675/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission675/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745425739179,
                "pdate": 1754680630137,
                "odate": 1758062777448,
                "mdate": 1758062820269,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission675/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "IV35hjIZwz",
        "title": "In-Context Iterative Policy Improvement for Dynamic Manipulation",
        "abstract": "Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime.",
        "keywords": [
            "In-Context Learning",
            "Dynamic Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/440d9fc29da1715ab45c2e18ba55155acc2fff6e.pdf",
        "reviews": [
            {
                "id": "V8gtUA2HqA",
                "forum": "IV35hjIZwz",
                "replyto": "IV35hjIZwz",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission674/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068296416,
                "mdate": 1754869471701,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "IV35hjIZwz",
                "forum": "IV35hjIZwz",
                "content": {
                    "title": {
                        "value": "In-Context Iterative Policy Improvement for Dynamic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Mark Van der Merwe",
                            "Devesh K. Jha"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mark_Van_der_Merwe1",
                            "~Devesh_K._Jha1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "In-Context Learning",
                            "Dynamic Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/440d9fc29da1715ab45c2e18ba55155acc2fff6e.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmerwe2025incontext,\ntitle={In-Context Iterative Policy Improvement for Dynamic Manipulation},\nauthor={Mark Van der Merwe and Devesh K. Jha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=IV35hjIZwz}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c2623b810f543fa802f8a68645c0d87db5556391.mp4"
                    },
                    "paperhash": {
                        "value": "merwe|incontext_iterative_policy_improvement_for_dynamic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission674/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission674/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission674/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745425571100,
                "pdate": 1754680629951,
                "odate": 1758062777419,
                "mdate": 1758062820136,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission674/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission674/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "f2Y549UzM5",
        "title": "Cost-aware Discovery of Contextual Failures using Bayesian Active Learning",
        "abstract": "Ensuring the robustness of robotic systems is crucial for their deployment in safety-critical domains. Failure discovery, or falsification, is a widely used approach for evaluating robustness, with recent advancements focusing on improving sample efficiency and generalization through probabilistic sampling techniques and learning-theoretic approaches. However, existing methods typically rely on explicitly defined analytical cost functions to characterize failures, often overlooking the underlying causes and diversity of discovered failure scenarios. In this work, we propose a novel failure discovery framework that integrates contextual reasoning in the falsification process, specifically tailored for high evaluation-cost applications. Our method incorporates expert-in-the-loop feedback to construct a probabilistic surrogate model of failures using Bayesian inference. This model is iteratively refined and leveraged to guide an active learning strategy that prioritizes the discovery of diverse failure cases. We empirically validate our approach across a range of tasks for high-cost contextual falsification in robotic manipulation and autonomous driving.",
        "keywords": [
            "Failure discovery",
            "Testing",
            "Contextual failures"
        ],
        "pdf_url": "https://openreview.net/pdf/0c4bbc4ecee323522e632879cc6fa741268a8c13.pdf",
        "reviews": [
            {
                "id": "Uc8uCk5ED6",
                "forum": "f2Y549UzM5",
                "replyto": "f2Y549UzM5",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission670/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068296272,
                "mdate": 1754869471703,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "f2Y549UzM5",
                "forum": "f2Y549UzM5",
                "content": {
                    "title": {
                        "value": "Cost-aware Discovery of Contextual Failures using Bayesian Active Learning"
                    },
                    "authors": {
                        "value": [
                            "Anjali Parashar",
                            "Joseph Zhang",
                            "Yingke Li",
                            "Chuchu Fan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Anjali_Parashar1",
                            "~Joseph_Zhang1",
                            "~Yingke_Li1",
                            "~Chuchu_Fan2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Failure discovery",
                            "Testing",
                            "Contextual failures"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a method for contextual failure discovery of robotic systems with high cost of evaluation using a Bayesian Active Learning approach"
                    },
                    "abstract": {
                        "value": "Ensuring the robustness of robotic systems is crucial for their deployment in safety-critical domains. Failure discovery, or falsification, is a widely used approach for evaluating robustness, with recent advancements focusing on improving sample efficiency and generalization through probabilistic sampling techniques and learning-theoretic approaches. However, existing methods typically rely on explicitly defined analytical cost functions to characterize failures, often overlooking the underlying causes and diversity of discovered failure scenarios. In this work, we propose a novel failure discovery framework that integrates contextual reasoning in the falsification process, specifically tailored for high evaluation-cost applications. Our method incorporates expert-in-the-loop feedback to construct a probabilistic surrogate model of failures using Bayesian inference. This model is iteratively refined and leveraged to guide an active learning strategy that prioritizes the discovery of diverse failure cases. We empirically validate our approach across a range of tasks for high-cost contextual falsification in robotic manipulation and autonomous driving."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b6f0f40f857d581ef2644aa27d669cab59679078.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0c4bbc4ecee323522e632879cc6fa741268a8c13.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nparashar2025costaware,\ntitle={Cost-aware Discovery of Contextual Failures using Bayesian Active Learning},\nauthor={Anjali Parashar and Joseph Zhang and Yingke Li and Chuchu Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=f2Y549UzM5}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ce7eedf74a149357725ccd76dee1690e3055a9ea.mp4"
                    },
                    "paperhash": {
                        "value": "parashar|costaware_discovery_of_contextual_failures_using_bayesian_active_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission670/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission670/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission670/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745424752472,
                "pdate": 1754680629578,
                "odate": 1758062777288,
                "mdate": 1758062820132,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission670/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "w0zDVjLscj",
        "title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection",
        "abstract": "Learning robot policies that capture multimodality in the training data has been a long-standing open challenge for behavior cloning. Recent approaches tackle the problem by modeling the conditional action distribution with generative models. One of these approaches is Diffusion Policy, which relies on a diffusion model to denoise random points into robot action trajectories. While achieving state-of-the-art performance, it has two main drawbacks that may lead the robot out of the data distribution during policy execution. First, the stochasticity of the denoising process can highly impact on the quality of generated trajectory of actions. Second, being a supervised learning approach, it can learn data outliers from the dataset used for training. Recent work focuses on mitigating these limitations by combining Diffusion Policy either with large-scale training or with classical behavior cloning algorithms. Instead, we propose KDPE, a Kernel Density Estimation-based strategy that filters out potentially harmful trajectories output of Diffusion Policy while keeping a low test-time computational overhead.\nFor Kernel Density Estimation, we propose a manifold-aware kernel to model a probability density function for actions composed of end-effector Cartesian position, orientation, and gripper state. KDPE overall achieves better performance than Diffusion Policy on simulated single-arm RoboMimic and MimicGen tasks, and on three real robot experiments:PickPlush, a tabletop grasping task, CubeSort, a multimodal pick and place task, and CoffeeMaking, a task that requires long-horizon capabilities and precise execution.\n\nThe code will be released upon acceptance and additional material is provided on our anonymized project page:https://kdpe-robotics.github.io.",
        "keywords": [
            "Behavior Cloning",
            "Manipulation",
            "Trajectory Selection"
        ],
        "pdf_url": "https://openreview.net/pdf/65170276d09abd9623d361e91b20d5b6b48bf21d.pdf",
        "reviews": [
            {
                "id": "x0hz0ZE0pQ",
                "forum": "w0zDVjLscj",
                "replyto": "w0zDVjLscj",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission666/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295947,
                "mdate": 1754869471616,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "w0zDVjLscj",
                "forum": "w0zDVjLscj",
                "content": {
                    "title": {
                        "value": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection"
                    },
                    "authors": {
                        "value": [
                            "Andrea Rosasco",
                            "Federico Ceola",
                            "Giulia Pasquale",
                            "Lorenzo Natale"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Andrea_Rosasco1",
                            "~Federico_Ceola1",
                            "~Giulia_Pasquale1",
                            "~Lorenzo_Natale1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Behavior Cloning",
                            "Manipulation",
                            "Trajectory Selection"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose KDPE, a method to select Diffusion Policy action trajectories via Kernel Density Estimation."
                    },
                    "abstract": {
                        "value": "Learning robot policies that capture multimodality in the training data has been a long-standing open challenge for behavior cloning. Recent approaches tackle the problem by modeling the conditional action distribution with generative models. One of these approaches is Diffusion Policy, which relies on a diffusion model to denoise random points into robot action trajectories. While achieving state-of-the-art performance, it has two main drawbacks that may lead the robot out of the data distribution during policy execution. First, the stochasticity of the denoising process can highly impact on the quality of generated trajectory of actions. Second, being a supervised learning approach, it can learn data outliers from the dataset used for training. Recent work focuses on mitigating these limitations by combining Diffusion Policy either with large-scale training or with classical behavior cloning algorithms. Instead, we propose KDPE, a Kernel Density Estimation-based strategy that filters out potentially harmful trajectories output of Diffusion Policy while keeping a low test-time computational overhead.\nFor Kernel Density Estimation, we propose a manifold-aware kernel to model a probability density function for actions composed of end-effector Cartesian position, orientation, and gripper state. KDPE overall achieves better performance than Diffusion Policy on simulated single-arm RoboMimic and MimicGen tasks, and on three real robot experiments:PickPlush, a tabletop grasping task, CubeSort, a multimodal pick and place task, and CoffeeMaking, a task that requires long-horizon capabilities and precise execution.\n\nThe code will be released upon acceptance and additional material is provided on our anonymized project page:https://kdpe-robotics.github.io."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/65170276d09abd9623d361e91b20d5b6b48bf21d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nrosasco2025kdpe,\ntitle={{KDPE}: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection},\nauthor={Andrea Rosasco and Federico Ceola and Giulia Pasquale and Lorenzo Natale},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=w0zDVjLscj}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c9e8acd513e4f873ec1425880eb968dfe71f5a89.mp4"
                    },
                    "paperhash": {
                        "value": "rosasco|kdpe_a_kernel_density_estimation_strategy_for_diffusion_policy_trajectory_selection"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission666/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission666/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission666/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745424313482,
                "pdate": 1754680629212,
                "odate": 1758062777114,
                "mdate": 1758062820023,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission666/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "xVDj9uq6K3",
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation",
        "abstract": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding, including spatiotemporal awareness and the ability to interpret human intentions. Recent Vision-Language Models (VLMs) show exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding—that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can reliably perform the complex spatiotemporal reasoning and intent inference needed for safe and socially compliant robot navigation. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms a simpler rule-based approach and human consensus, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. We will open source the code and release the benchmark.",
        "keywords": [
            "social robot navigation",
            "scene understanding",
            "vision-language models",
            "VLM",
            "benchmark"
        ],
        "pdf_url": "https://openreview.net/pdf/2c283b3683eb2383910566e04051d0d633a79f6c.pdf",
        "reviews": [
            {
                "id": "T9UFaJtuQc",
                "forum": "xVDj9uq6K3",
                "replyto": "xVDj9uq6K3",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission664/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295842,
                "mdate": 1754869471480,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "xVDj9uq6K3",
                "forum": "xVDj9uq6K3",
                "content": {
                    "title": {
                        "value": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation"
                    },
                    "authors": {
                        "value": [
                            "Michael Joseph Munje",
                            "Chen Tang",
                            "Shuijing Liu",
                            "Zichao Hu",
                            "Yifeng Zhu",
                            "Jiaxun Cui",
                            "Garrett Warnell",
                            "Joydeep Biswas",
                            "Peter Stone"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Michael_Joseph_Munje1",
                            "~Chen_Tang2",
                            "~Shuijing_Liu1",
                            "~Zichao_Hu1",
                            "~Yifeng_Zhu2",
                            "~Jiaxun_Cui1",
                            "~Garrett_Warnell1",
                            "~Joydeep_Biswas1",
                            "~Peter_Stone1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "social robot navigation",
                            "scene understanding",
                            "vision-language models",
                            "VLM",
                            "benchmark"
                        ]
                    },
                    "TLDR": {
                        "value": "A VLM benchmark for scene understanding of social robot navigation scenarios."
                    },
                    "abstract": {
                        "value": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding, including spatiotemporal awareness and the ability to interpret human intentions. Recent Vision-Language Models (VLMs) show exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding—that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can reliably perform the complex spatiotemporal reasoning and intent inference needed for safe and socially compliant robot navigation. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms a simpler rule-based approach and human consensus, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. We will open source the code and release the benchmark."
                    },
                    "supplementary_material": {
                        "value": "/attachment/746552d7d02197aa902f3a3e5cc3273ae288367b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2c283b3683eb2383910566e04051d0d633a79f6c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmunje2025socialnavsub,\ntitle={SocialNav-{SUB}: Benchmarking {VLM}s for Scene Understanding in Social Robot Navigation},\nauthor={Michael Joseph Munje and Chen Tang and Shuijing Liu and Zichao Hu and Yifeng Zhu and Jiaxun Cui and Garrett Warnell and Joydeep Biswas and Peter Stone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xVDj9uq6K3}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d86275f6104cc4892e2065c0d182d33cd5ee2e6f.mp4"
                    },
                    "paperhash": {
                        "value": "munje|socialnavsub_benchmarking_vlms_for_scene_understanding_in_social_robot_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission664/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission664/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission664/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745423910595,
                "pdate": 1754680629044,
                "odate": 1758062777015,
                "mdate": 1758062819892,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission664/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission664/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9HwVDqndnv",
        "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
        "abstract": "Large vision and language models are being increasingly deployed on real robots, leading to an immediate need for ensuring robot safety under AI-control. In this paper, we develop the ASIMOV Benchmark — a collection of large-scale semantic safety datasets grounded in real-world visual scenes and human injury reports from hospitals (500k situations, 3M instructions). We propose a scalable recipe for data generation leveraging text and image generation techniques to synthesize safety-relevant scenarios. As a second contribution, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot’s behavior using Constitutional AI mechanisms. We report a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. We argue that human interpretability and modifiability of constitutions inferred from data make them an ideal medium for behavior governance of AI-controlled robots.",
        "keywords": [
            "constitutional ai",
            "constitution",
            "safety",
            "benchmark",
            "semantic safety"
        ],
        "pdf_url": "https://openreview.net/pdf/1378ad1e10c9b9be5ce3cf2a2c58f070d98e06ac.pdf",
        "reviews": [
            {
                "id": "XuSefe7eeS",
                "forum": "9HwVDqndnv",
                "replyto": "9HwVDqndnv",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission662/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295738,
                "mdate": 1754869471413,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9HwVDqndnv",
                "forum": "9HwVDqndnv",
                "content": {
                    "title": {
                        "value": "Generating Robot Constitutions & Benchmarks for Semantic Safety"
                    },
                    "authors": {
                        "value": [
                            "Pierre Sermanet",
                            "Anirudha Majumdar",
                            "Alex Irpan",
                            "Dmitry Kalashnikov",
                            "Vikas Sindhwani"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Pierre_Sermanet1",
                            "~Anirudha_Majumdar1",
                            "~Alex_Irpan1",
                            "~Dmitry_Kalashnikov1",
                            "~Vikas_Sindhwani1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "constitutional ai",
                            "constitution",
                            "safety",
                            "benchmark",
                            "semantic safety"
                        ]
                    },
                    "TLDR": {
                        "value": "Robot constitutions for improving robot behavior + benchmark to evaluate robot behavior"
                    },
                    "abstract": {
                        "value": "Large vision and language models are being increasingly deployed on real robots, leading to an immediate need for ensuring robot safety under AI-control. In this paper, we develop the ASIMOV Benchmark — a collection of large-scale semantic safety datasets grounded in real-world visual scenes and human injury reports from hospitals (500k situations, 3M instructions). We propose a scalable recipe for data generation leveraging text and image generation techniques to synthesize safety-relevant scenarios. As a second contribution, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot’s behavior using Constitutional AI mechanisms. We report a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. We argue that human interpretability and modifiability of constitutions inferred from data make them an ideal medium for behavior governance of AI-controlled robots."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/1378ad1e10c9b9be5ce3cf2a2c58f070d98e06ac.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsermanet2025generating,\ntitle={Generating Robot Constitutions \\& Benchmarks for Semantic Safety},\nauthor={Pierre Sermanet and Anirudha Majumdar and Alex Irpan and Dmitry Kalashnikov and Vikas Sindhwani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9HwVDqndnv}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/aba31ce50de845aee27680cad181d356ed74828a.zip"
                    },
                    "paperhash": {
                        "value": "sermanet|generating_robot_constitutions_benchmarks_for_semantic_safety"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission662/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission662/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745423883332,
                "pdate": 1754680628960,
                "odate": 1758062776971,
                "mdate": 1758062819894,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission662/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission662/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "sMs4pJYhWi",
        "title": "Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation",
        "abstract": "We present TacX, the first multisensory touch representations across four tactile modalities: image, audio, motion, and pressure. Trained on ~1M contact-rich interactions collected with the Digit 360 sensor, TacX captures complementary touch signals at diverse temporal and spatial scales. By leveraging self-supervised learning, TacX fuses these modalities into a unified representation that captures physical properties useful for downstream robot manipulation tasks. We study how to effectively integrate real-world touch representations for both imitation learning and tactile adaptation of sim-trained policies, showing that TacX boosts policy success rates by 63% over an end-to-end model using tactile images and improves robustness by 90% in recovering object states from touch. Finally, we benchmark TacX’s ability to make inference about physical properties, such as object-action identification, material-quantity estimation and force estimation. TacX improves accuracy in characterizing physical properties by 48% compared to end-to-end approaches, demonstrating the advantages of multisensory pretraining for capturing features essential for dexterous manipulation.",
        "keywords": [
            "Multi-sensory Touch",
            "Self-Supervised Learning",
            "Tactile Adaptation"
        ],
        "pdf_url": "https://openreview.net/pdf/5e355577bd3a9fc68176085234a943b5732ba80d.pdf",
        "reviews": [
            {
                "id": "T2xjcBhgJ4",
                "forum": "sMs4pJYhWi",
                "replyto": "sMs4pJYhWi",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission661/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295698,
                "mdate": 1754869457470,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "sMs4pJYhWi",
                "forum": "sMs4pJYhWi",
                "content": {
                    "title": {
                        "value": "Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Carolina Higuera",
                            "Akash Sharma",
                            "Taosha Fan",
                            "Chaithanya Krishna Bodduluri",
                            "Byron Boots",
                            "Michael Kaess",
                            "Mike Lambeta",
                            "Tingfan Wu",
                            "Zixi Liu",
                            "Francois Robert Hogan",
                            "Mustafa Mukadam"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Carolina_Higuera1",
                            "~Akash_Sharma1",
                            "~Taosha_Fan1",
                            "~Chaithanya_Krishna_Bodduluri1",
                            "~Byron_Boots1",
                            "~Michael_Kaess1",
                            "~Mike_Lambeta1",
                            "~Tingfan_Wu2",
                            "~Zixi_Liu1",
                            "~Francois_Robert_Hogan1",
                            "~Mustafa_Mukadam1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-sensory Touch",
                            "Self-Supervised Learning",
                            "Tactile Adaptation"
                        ]
                    },
                    "TLDR": {
                        "value": "TacX, the first self-supervised general-purpose multisensory touch representations across four key modalities: image, audio, inertial measurements (IMU), and pressure."
                    },
                    "abstract": {
                        "value": "We present TacX, the first multisensory touch representations across four tactile modalities: image, audio, motion, and pressure. Trained on ~1M contact-rich interactions collected with the Digit 360 sensor, TacX captures complementary touch signals at diverse temporal and spatial scales. By leveraging self-supervised learning, TacX fuses these modalities into a unified representation that captures physical properties useful for downstream robot manipulation tasks. We study how to effectively integrate real-world touch representations for both imitation learning and tactile adaptation of sim-trained policies, showing that TacX boosts policy success rates by 63% over an end-to-end model using tactile images and improves robustness by 90% in recovering object states from touch. Finally, we benchmark TacX’s ability to make inference about physical properties, such as object-action identification, material-quantity estimation and force estimation. TacX improves accuracy in characterizing physical properties by 48% compared to end-to-end approaches, demonstrating the advantages of multisensory pretraining for capturing features essential for dexterous manipulation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9fe69a3221f349b1b65bbed922e074036d352ac8.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5e355577bd3a9fc68176085234a943b5732ba80d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhiguera2025tactile,\ntitle={Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation},\nauthor={Carolina Higuera and Akash Sharma and Taosha Fan and Chaithanya Krishna Bodduluri and Byron Boots and Michael Kaess and Mike Lambeta and Tingfan Wu and Mustafa Mukadam and Zixi Liu and Francois Robert Hogan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sMs4pJYhWi}\n}"
                    },
                    "paperhash": {
                        "value": "higuera|tactile_beyond_pixels_multisensory_touch_representations_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission661/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission661/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission661/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745423745783,
                "pdate": 1754680628879,
                "odate": 1758062776908,
                "mdate": 1758639345425,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission661/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bILubVwPoD",
        "title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting",
        "abstract": "Imitation learning enables intelligent systems to acquire complex behaviors with minimal supervision. However, existing methods often focus on short-horizon skills, require large datasets, and struggle to solve long-horizon tasks or generalize across task variations and distribution shifts. We propose a novel neuro-symbolic framework that jointly learns continuous control policies and symbolic domain abstractions from a few skill demonstrations. Our method abstracts high-level task structures into a graph, discovers symbolic rules via an Answer Set Programming solver, and trains low-level controllers using diffusion policy imitation learning. A high-level oracle filters task-relevant information to focus each controller on a minimal observation and action space. Our graph-based neuro-symbolic framework enables capturing complex state transitions, including non-spatial and temporal relations, that data-driven learning or clustering techniques often fail to discover in limited demonstration datasets. We validate our approach in six domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers of Hanoi environments, and a distinct Automated Forklift domain with two environments. The results demonstrate high data efficiency with as few as five skill demonstrations, strong zero- and few-shot generalizations, and interpretable decision making. Our code is publicly available.",
        "keywords": [
            "Neuro-symbolic",
            "Imitation Learning",
            "Task and Motion Planning",
            "Symbolic Planning",
            "Skill Learning",
            "Human-Robot Interaction"
        ],
        "pdf_url": "https://openreview.net/pdf/4bdbbcf939e131b796348766fe6e70439827bfd2.pdf",
        "reviews": [
            {
                "id": "mXRJW9qbPb",
                "forum": "bILubVwPoD",
                "replyto": "bILubVwPoD",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission657/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295468,
                "mdate": 1754869471310,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bILubVwPoD",
                "forum": "bILubVwPoD",
                "content": {
                    "title": {
                        "value": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting"
                    },
                    "authors": {
                        "value": [
                            "Pierrick Lorang",
                            "Hong Lu",
                            "Johannes Huemer",
                            "Patrik Zips",
                            "Matthias Scheutz"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Pierrick_Lorang1",
                            "hong.lu663424@tufts.edu",
                            "~Johannes_Huemer1",
                            "~Patrik_Zips1",
                            "~Matthias_Scheutz1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Neuro-symbolic",
                            "Imitation Learning",
                            "Task and Motion Planning",
                            "Symbolic Planning",
                            "Skill Learning",
                            "Human-Robot Interaction"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper introduces a neuro-symbolic imitation learning framework that enables robots to generalize from just a few skills demonstrations to complex long-horizon tasks."
                    },
                    "abstract": {
                        "value": "Imitation learning enables intelligent systems to acquire complex behaviors with minimal supervision. However, existing methods often focus on short-horizon skills, require large datasets, and struggle to solve long-horizon tasks or generalize across task variations and distribution shifts. We propose a novel neuro-symbolic framework that jointly learns continuous control policies and symbolic domain abstractions from a few skill demonstrations. Our method abstracts high-level task structures into a graph, discovers symbolic rules via an Answer Set Programming solver, and trains low-level controllers using diffusion policy imitation learning. A high-level oracle filters task-relevant information to focus each controller on a minimal observation and action space. Our graph-based neuro-symbolic framework enables capturing complex state transitions, including non-spatial and temporal relations, that data-driven learning or clustering techniques often fail to discover in limited demonstration datasets. We validate our approach in six domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers of Hanoi environments, and a distinct Automated Forklift domain with two environments. The results demonstrate high data efficiency with as few as five skill demonstrations, strong zero- and few-shot generalizations, and interpretable decision making. Our code is publicly available."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1dfa3e7419a3f87ed508f7836cc3def06ca1aecf.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4bdbbcf939e131b796348766fe6e70439827bfd2.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlorang2025fewshot,\ntitle={Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting},\nauthor={Pierrick Lorang and Hong Lu and Johannes Huemer and Patrik Zips and Matthias Scheutz},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bILubVwPoD}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ebb8ad6e31e680779765203b1240dd7607882dab.mp4"
                    },
                    "paperhash": {
                        "value": "lorang|fewshot_neurosymbolic_imitation_learning_for_longhorizon_planning_and_acting"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission657/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission657/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission657/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745423224293,
                "pdate": 1754680628737,
                "odate": 1758062776874,
                "mdate": 1758062819633,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission657/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission657/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "qoKo2caB9B",
        "title": "Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination",
        "abstract": "Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. \nHowever, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency.\nShared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a *soft weight sharing* architecture that uses hypernetworks to efficiently learn a *flexible* shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables *zero-shot generalization* to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80\\% fewer learnable parameters.",
        "keywords": [
            "Multi-Robot Learning",
            "Heterogeneous Teams",
            "Parameter Sharing"
        ],
        "pdf_url": "https://openreview.net/pdf/2879577fed27d4bd9db7e337ba52133706368c96.pdf",
        "reviews": [
            {
                "id": "0BmMqjbmVq",
                "forum": "qoKo2caB9B",
                "replyto": "qoKo2caB9B",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission653/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295263,
                "mdate": 1754869471206,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "qoKo2caB9B",
                "forum": "qoKo2caB9B",
                "content": {
                    "title": {
                        "value": "Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination"
                    },
                    "authors": {
                        "value": [
                            "Kevin Fu",
                            "Shalin Jain",
                            "Pierce Howell",
                            "Harish Ravichandar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kevin_Fu2",
                            "~Shalin_Jain1",
                            "~Pierce_Howell1",
                            "~Harish_Ravichandar1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-Robot Learning",
                            "Heterogeneous Teams",
                            "Parameter Sharing"
                        ]
                    },
                    "abstract": {
                        "value": "Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. \nHowever, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency.\nShared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a *soft weight sharing* architecture that uses hypernetworks to efficiently learn a *flexible* shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables *zero-shot generalization* to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80\\% fewer learnable parameters."
                    },
                    "supplementary_material": {
                        "value": "/attachment/488899dd5304ebda50ac93eead211d8a34f37f99.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2879577fed27d4bd9db7e337ba52133706368c96.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfu2025capabilityaware,\ntitle={Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination},\nauthor={Kevin Fu and Shalin Jain and Pierce Howell and Harish Ravichandar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qoKo2caB9B}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1796c62f840ccd88a617def2ced638ced680ba95.mp4"
                    },
                    "paperhash": {
                        "value": "fu|capabilityaware_shared_hypernetworks_for_flexible_heterogeneous_multirobot_coordination"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission653/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission653/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission653/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745422749056,
                "pdate": 1754680628597,
                "odate": 1758062776702,
                "mdate": 1758062819611,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission653/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "brTSiML1nh",
        "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies",
        "abstract": "In this paper, we propose AimBot, a lightweight visual augmentation technique that provides explicit spatial cues to improve visuomotor policy learning in robotic manipulation. AimBot overlays shooting lines and scope reticles onto multi-view RGB images, offering auxiliary visual guidance that encodes the end-effector's state. The overlays are computed from depth images, camera extrinsics, and the current end-effector pose, explicitly conveying spatial relationships between the gripper and objects in the scene. AimBot incurs minimal computational overhead (less than 1 ms) and requires no changes to model architectures, as it simply replaces original RGB images with augmented counterparts. Despite its simplicity, our results show that AimBot consistently improves the performance of various visuomotor policies in both simulation and real-world settings, highlighting the benefits of spatially grounded visual feedback. More videos can be found at https://aimbot-reticle.github.io/",
        "keywords": [
            "Robotic Manipulation",
            "Visuomotor Policy",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/3024bb605df083d243c8d7827620eb3181a0c8c1.pdf",
        "reviews": [
            {
                "id": "lLQANUwMkD",
                "forum": "brTSiML1nh",
                "replyto": "brTSiML1nh",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission649/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068295002,
                "mdate": 1754869471152,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "brTSiML1nh",
                "forum": "brTSiML1nh",
                "content": {
                    "title": {
                        "value": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies"
                    },
                    "authors": {
                        "value": [
                            "Yinpei Dai",
                            "Jayjun Lee",
                            "Yichi Zhang",
                            "Ziqiao Ma",
                            "Jianing Yang",
                            "Amir Zadeh",
                            "Chuan Li",
                            "Nima Fazeli",
                            "Joyce Chai"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yinpei_Dai1",
                            "~Jayjun_Lee1",
                            "~Yichi_Zhang1",
                            "~Ziqiao_Ma1",
                            "~Jianing_Yang1",
                            "~Amir_Zadeh2",
                            "~Chuan_Li4",
                            "~Nima_Fazeli1",
                            "~Joyce_Chai2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robotic Manipulation",
                            "Visuomotor Policy",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We augment RGB images with auxiliary visual guidances to provide 2.5D spatial cues for better robotic manipulation"
                    },
                    "abstract": {
                        "value": "In this paper, we propose AimBot, a lightweight visual augmentation technique that provides explicit spatial cues to improve visuomotor policy learning in robotic manipulation. AimBot overlays shooting lines and scope reticles onto multi-view RGB images, offering auxiliary visual guidance that encodes the end-effector's state. The overlays are computed from depth images, camera extrinsics, and the current end-effector pose, explicitly conveying spatial relationships between the gripper and objects in the scene. AimBot incurs minimal computational overhead (less than 1 ms) and requires no changes to model architectures, as it simply replaces original RGB images with augmented counterparts. Despite its simplicity, our results show that AimBot consistently improves the performance of various visuomotor policies in both simulation and real-world settings, highlighting the benefits of spatially grounded visual feedback. More videos can be found at https://aimbot-reticle.github.io/"
                    },
                    "supplementary_material": {
                        "value": "/attachment/22d47e32a123bbe303711bf32a3890b83a3c7a2d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3024bb605df083d243c8d7827620eb3181a0c8c1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndai2025aimbot,\ntitle={AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies},\nauthor={Yinpei Dai and Jayjun Lee and Yichi Zhang and Ziqiao Ma and Jianing Yang and Amir Zadeh and Chuan Li and Nima Fazeli and Joyce Chai},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=brTSiML1nh}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c32a3184c51a94be0105adcb475e0cfe89b229b3.zip"
                    },
                    "paperhash": {
                        "value": "dai|aimbot_a_simple_auxiliary_visual_cue_to_enhance_spatial_awareness_of_visuomotor_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission649/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission649/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission649/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745422142672,
                "pdate": 1754680628406,
                "odate": 1758062776489,
                "mdate": 1758062819486,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission649/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "8v0mlyKk5q",
        "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC",
        "abstract": "Model Predictive Control (MPC) is among the most widely adopted and reliable methods for robot control, relying critically on an accurate dynamics model. However, existing dynamics models used in the gradient-based MPC are limited by computational complexity and state representation. To address this limitation, we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we project the unmodeled dynamics onto a time-dependent dynamics model. This time-dependency is captured through time-varying model parameters, whose evolution over the MPC prediction horizon is learned using a neural network. Such formulation preserves the computational efficiency and robustness of the base model while equipping it with the capacity to anticipate previously unmodeled phenomena. We evaluated the proposed approach on several challenging systems, including real-world F1TENTH autonomous racing, and demonstrated that it significantly reduces long-horizon prediction errors. Moreover, when integrated within the MPC framework (HyperMPC), our method consistently outperforms existing state-of-the-art techniques.",
        "keywords": [
            "Model Learning for Robot Control",
            "Dynamics Model Learning",
            "Model Predictive Control",
            "MPC"
        ],
        "pdf_url": "https://openreview.net/pdf/0cf4d7af0a7b451c539e4d33b16a392380468491.pdf",
        "reviews": [
            {
                "id": "7nuQQsR27s",
                "forum": "8v0mlyKk5q",
                "replyto": "8v0mlyKk5q",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission642/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068294314,
                "mdate": 1754869471055,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "8v0mlyKk5q",
                "forum": "8v0mlyKk5q",
                "content": {
                    "title": {
                        "value": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC"
                    },
                    "authors": {
                        "value": [
                            "Jan Węgrzynowski",
                            "Piotr Kicki",
                            "Grzegorz Czechmanowski",
                            "Maciej Piotr Krupka",
                            "Krzysztof Walas"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jan_Węgrzynowski1",
                            "~Piotr_Kicki1",
                            "~Grzegorz_Czechmanowski1",
                            "~Maciej_Piotr_Krupka1",
                            "~Krzysztof_Walas2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Model Learning for Robot Control",
                            "Dynamics Model Learning",
                            "Model Predictive Control",
                            "MPC"
                        ]
                    },
                    "TLDR": {
                        "value": "Hyper Prediction Model - a neural network forecasts the trajectories of a dynamic model's parameters over the prediction horizon, allowing it to compensate for unmodeled dynamics."
                    },
                    "abstract": {
                        "value": "Model Predictive Control (MPC) is among the most widely adopted and reliable methods for robot control, relying critically on an accurate dynamics model. However, existing dynamics models used in the gradient-based MPC are limited by computational complexity and state representation. To address this limitation, we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we project the unmodeled dynamics onto a time-dependent dynamics model. This time-dependency is captured through time-varying model parameters, whose evolution over the MPC prediction horizon is learned using a neural network. Such formulation preserves the computational efficiency and robustness of the base model while equipping it with the capacity to anticipate previously unmodeled phenomena. We evaluated the proposed approach on several challenging systems, including real-world F1TENTH autonomous racing, and demonstrated that it significantly reduces long-horizon prediction errors. Moreover, when integrated within the MPC framework (HyperMPC), our method consistently outperforms existing state-of-the-art techniques."
                    },
                    "supplementary_material": {
                        "value": "/attachment/42c824124d793f6b3de9290c90fc297a5fa32337.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0cf4d7af0a7b451c539e4d33b16a392380468491.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwegrzynowski2025beyond,\ntitle={Beyond Constant Parameters: Hyper Prediction Models and Hyper{MPC}},\nauthor={Jan W{\\k{e}}grzynowski and Piotr Kicki and Grzegorz Czechmanowski and Maciej Piotr Krupka and Krzysztof Walas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8v0mlyKk5q}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1358d34e39cb0414cc8f45b6681edc459e951180.mp4"
                    },
                    "paperhash": {
                        "value": "wgrzynowski|beyond_constant_parameters_hyper_prediction_models_and_hypermpc"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission642/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission642/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission642/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745421397960,
                "pdate": 1754680628158,
                "odate": 1758062776256,
                "mdate": 1758062819398,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission642/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission642/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JeppaebLRD",
        "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models",
        "abstract": "Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to 50% of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by 20% through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers a 25.9% improvement over state-of-the-art baselines across 190 tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. All code, pretrained weights, and training recipes are publicly released to democratize efficient VLA development.",
        "keywords": [
            "Imitation Learning",
            "VLA",
            "Language-conditioned Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/2cbb7f3f792d37512d9d055e3db47bdc21b3a28e.pdf",
        "reviews": [
            {
                "id": "FYMSjtO2kg",
                "forum": "JeppaebLRD",
                "replyto": "JeppaebLRD",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission640/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068294044,
                "mdate": 1754869470997,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JeppaebLRD",
                "forum": "JeppaebLRD",
                "content": {
                    "title": {
                        "value": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models"
                    },
                    "authors": {
                        "value": [
                            "Moritz Reuss",
                            "Hongyi Zhou",
                            "Marcel Rühle",
                            "Ömer Erdinç Yağmurlu",
                            "Fabian Otto",
                            "Rudolf Lioutikov"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Moritz_Reuss1",
                            "~Hongyi_Zhou1",
                            "~Marcel_Rühle1",
                            "~Ömer_Erdinç_Yağmurlu1",
                            "~Fabian_Otto1",
                            "~Rudolf_Lioutikov1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "VLA",
                            "Language-conditioned Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "FLOWER is a resource-efficient flow-based Vision-Language-Action Policy that achieves sota performance across diverse robotics tasks while substantially lowering computation and enabling broad accessibility."
                    },
                    "abstract": {
                        "value": "Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to 50% of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by 20% through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers a 25.9% improvement over state-of-the-art baselines across 190 tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. All code, pretrained weights, and training recipes are publicly released to democratize efficient VLA development."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a5ba977e161f881e35e9141b2d11717a6fa66f2d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2cbb7f3f792d37512d9d055e3db47bdc21b3a28e.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nreuss2025flower,\ntitle={{FLOWER}: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models},\nauthor={Moritz Reuss and Hongyi Zhou and Marcel R{\\\"u}hle and {\\\"O}mer Erdin{\\c{c}} Ya{\\u{g}}murlu and Fabian Otto and Rudolf Lioutikov},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JeppaebLRD}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1a0db8b0b12badead79da214614087d3abbd341c.mp4"
                    },
                    "paperhash": {
                        "value": "reuss|flower_democratizing_generalist_robot_policies_with_efficient_visionlanguageflow_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission640/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission640/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission640/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745421109602,
                "pdate": 1754680628068,
                "odate": 1758062776160,
                "mdate": 1758062819365,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission640/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ZmASpafbOc",
        "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation",
        "abstract": "Coordinated multi-robot navigation is essential for robots to operate as a team in diverse environments. \nDuring navigation, robot teams usually need to maintain specific formations, such as circular formations to protect human teammates at the center. \nHowever, in complex scenarios such as narrow corridors, rigidly preserving predefined formations can become infeasible. \nTherefore, robot teams must be capable of dynamically splitting into smaller subteams and adaptively controlling the subteams to navigate through such scenarios while preserving formations.\nTo enable this capability, we introduce a novel method for SubTeaming and Adaptive Formation (STAF), which is built upon a unified hierarchical learning framework:\n(1) high-level deep graph cut for team splitting, (2) intermediate-level graph learning for facilitating coordinated navigation among subteams, \nand (3) low-level policy learning for controlling individual mobile robots to reach their goal positions while avoiding collisions.\nTo evaluate STAF, we conducted extensive experiments in both indoor and outdoor environments using robotics simulations and physical robot teams.\nExperimental results show that STAF enables the novel capability for subteaming and adaptive formation control, and achieves promising performance in coordinated multi-robot navigation through challenging scenarios.\nMore details are available on the project website: https://anonymous188.github.io/STAF/.",
        "keywords": [
            "Coordinated multi-robot navigation",
            "Hierarchical learning"
        ],
        "pdf_url": "https://openreview.net/pdf/0fb7b9114093992c0e4f7f0fd7719e3adfd8bc8f.pdf",
        "reviews": [
            {
                "id": "ebwJ8ZYfQZ",
                "forum": "ZmASpafbOc",
                "replyto": "ZmASpafbOc",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission634/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068293423,
                "mdate": 1754869470930,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ZmASpafbOc",
                "forum": "ZmASpafbOc",
                "content": {
                    "title": {
                        "value": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation"
                    },
                    "authors": {
                        "value": [
                            "Zihao Deng",
                            "Peng Gao",
                            "Williard Joshua Jose",
                            "Maggie Wigness",
                            "John G. Rogers III",
                            "Brian Reily",
                            "Christopher M. Reardon",
                            "Hao Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zihao_Deng4",
                            "~Peng_Gao4",
                            "~Williard_Joshua_Jose1",
                            "~Maggie_Wigness4",
                            "~John_G._Rogers_III1",
                            "~Brian_Reily1",
                            "~Christopher_M._Reardon1",
                            "~Hao_Zhang34"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Coordinated multi-robot navigation",
                            "Hierarchical learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose SubTeaming and Adaptive Formation (STAF), a hierarchical learning approach enabling subteam division, formation adaptation, and recovery for coordinated multi-robot navigation in complex scenarios when the entire team cannot pass through."
                    },
                    "abstract": {
                        "value": "Coordinated multi-robot navigation is essential for robots to operate as a team in diverse environments. \nDuring navigation, robot teams usually need to maintain specific formations, such as circular formations to protect human teammates at the center. \nHowever, in complex scenarios such as narrow corridors, rigidly preserving predefined formations can become infeasible. \nTherefore, robot teams must be capable of dynamically splitting into smaller subteams and adaptively controlling the subteams to navigate through such scenarios while preserving formations.\nTo enable this capability, we introduce a novel method for SubTeaming and Adaptive Formation (STAF), which is built upon a unified hierarchical learning framework:\n(1) high-level deep graph cut for team splitting, (2) intermediate-level graph learning for facilitating coordinated navigation among subteams, \nand (3) low-level policy learning for controlling individual mobile robots to reach their goal positions while avoiding collisions.\nTo evaluate STAF, we conducted extensive experiments in both indoor and outdoor environments using robotics simulations and physical robot teams.\nExperimental results show that STAF enables the novel capability for subteaming and adaptive formation control, and achieves promising performance in coordinated multi-robot navigation through challenging scenarios.\nMore details are available on the project website: https://anonymous188.github.io/STAF/."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0fb7b9114093992c0e4f7f0fd7719e3adfd8bc8f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndeng2025subteaming,\ntitle={Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation},\nauthor={Zihao Deng and Peng Gao and Williard Joshua Jose and Maggie Wigness and John G. Rogers III and Brian Reily and Christopher M. Reardon and Hao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZmASpafbOc}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/353032dcdea24b37406dfb364818d4102c6ae53f.zip"
                    },
                    "paperhash": {
                        "value": "deng|subteaming_and_adaptive_formation_control_for_coordinated_multirobot_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission634/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission634/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745420217627,
                "pdate": 1754680627794,
                "odate": 1758062775851,
                "mdate": 1758062819247,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission634/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1HW2UhshIT",
        "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions",
        "abstract": "Robot-assisted dressing has the potential to significantly improve the lives of individuals with mobility impairments. To ensure an effective and comfortable dressing experience, the robot must be able to handle challenging deformable garments, apply appropriate forces, and adapt to limb movements throughout the dressing process. Prior work often makes simplifying assumptions—such as static human limbs during dressing—which limits real-world applicability. In this work, we develop a robot-assisted dressing system capable of handling partial observations with visual occlusions, as well as robustly adapting to arm motions during the dressing process. Given a policy trained in simulation with partial observations, we propose a method to fine-tune it in the real world using a small amount of data and multi-modal feedback from vision and force sensing, to further improve the policy’s adaptability to arm motions and enhance safety. We evaluate our method in simulation with simplified articulated human meshes and in a real world human study with 12 participants across 264 dressing trials. Our policy successfully dresses two long-sleeve everyday garments onto the participants while being adaptive to various kinds of arm motions, and greatly outperforms prior baselines in terms of task completion and user feedback.",
        "keywords": [
            "Robot-Assisted Dressing",
            "Multi-Modal Learning",
            "Physical Human Robot Interaction",
            "Deformable Object Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/bc9a183ac21bc07266fa632a12a3b22b34a94a9d.pdf",
        "reviews": [
            {
                "id": "GV6TST7Zf7",
                "forum": "1HW2UhshIT",
                "replyto": "1HW2UhshIT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission630/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068293048,
                "mdate": 1754869470828,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1HW2UhshIT",
                "forum": "1HW2UhshIT",
                "content": {
                    "title": {
                        "value": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions"
                    },
                    "authors": {
                        "value": [
                            "Alexis Yihong Hao",
                            "Yufei Wang",
                            "Navin Sriram Ravie",
                            "Bharath Hegde",
                            "David Held",
                            "Zackory Erickson"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Alexis_Yihong_Hao1",
                            "~Yufei_Wang4",
                            "~Navin_Sriram_Ravie1",
                            "~Bharath_Hegde2",
                            "~David_Held1",
                            "~Zackory_Erickson1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot-Assisted Dressing",
                            "Multi-Modal Learning",
                            "Physical Human Robot Interaction",
                            "Deformable Object Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Robot-assisted dressing has the potential to significantly improve the lives of individuals with mobility impairments. To ensure an effective and comfortable dressing experience, the robot must be able to handle challenging deformable garments, apply appropriate forces, and adapt to limb movements throughout the dressing process. Prior work often makes simplifying assumptions—such as static human limbs during dressing—which limits real-world applicability. In this work, we develop a robot-assisted dressing system capable of handling partial observations with visual occlusions, as well as robustly adapting to arm motions during the dressing process. Given a policy trained in simulation with partial observations, we propose a method to fine-tune it in the real world using a small amount of data and multi-modal feedback from vision and force sensing, to further improve the policy’s adaptability to arm motions and enhance safety. We evaluate our method in simulation with simplified articulated human meshes and in a real world human study with 12 participants across 264 dressing trials. Our policy successfully dresses two long-sleeve everyday garments onto the participants while being adaptive to various kinds of arm motions, and greatly outperforms prior baselines in terms of task completion and user feedback."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/bc9a183ac21bc07266fa632a12a3b22b34a94a9d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhao2025forcemodulated,\ntitle={Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions},\nauthor={Alexis Yihong Hao and Yufei Wang and Navin Sriram Ravie and Bharath Hegde and David Held and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1HW2UhshIT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/71d39c8dcb80f9c3cbac6fe3ca7b943c195fd6de.mp4"
                    },
                    "paperhash": {
                        "value": "hao|forcemodulated_visual_policy_for_robotassisted_dressing_with_arm_motions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission630/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission630/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission630/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745419635584,
                "pdate": 1754680627443,
                "odate": 1758062775615,
                "mdate": 1758062819180,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission630/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "FBsawSyYBM",
        "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion",
        "abstract": "This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. \nThe challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot’s skill set expands. For example, “overtake the pedestrian while staying on the right side of the road” consists of two specifications: *\"overtake the pedestrian\"* and *\"walk on the right side of the road.\"*\nTo tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. \nUsing diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. \nAdditionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives.\nThrough simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines.",
        "keywords": [
            "Diffusion Motion Planning",
            "Diffusion Model",
            "Compositionality",
            "Instruction-following Navigation in Dynamic Environments"
        ],
        "pdf_url": "https://openreview.net/pdf/faeaf7fe5c477085bc968503b31bb8fec5f861f1.pdf",
        "reviews": [
            {
                "id": "uUNPfFjlGW",
                "forum": "FBsawSyYBM",
                "replyto": "FBsawSyYBM",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission627/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068292697,
                "mdate": 1754869470767,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "FBsawSyYBM",
                "forum": "FBsawSyYBM",
                "content": {
                    "title": {
                        "value": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion"
                    },
                    "authors": {
                        "value": [
                            "Zichao Hu",
                            "Chen Tang",
                            "Michael Joseph Munje",
                            "Yifeng Zhu",
                            "Alex Liu",
                            "Shuijing Liu",
                            "Garrett Warnell",
                            "Peter Stone",
                            "Joydeep Biswas"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zichao_Hu1",
                            "~Chen_Tang2",
                            "~Michael_Joseph_Munje1",
                            "~Yifeng_Zhu2",
                            "~Alex_Liu4",
                            "~Shuijing_Liu1",
                            "~Garrett_Warnell1",
                            "~Peter_Stone1",
                            "~Joydeep_Biswas1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Diffusion Motion Planning",
                            "Diffusion Model",
                            "Compositionality",
                            "Instruction-following Navigation in Dynamic Environments"
                        ]
                    },
                    "abstract": {
                        "value": "This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. \nThe challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot’s skill set expands. For example, “overtake the pedestrian while staying on the right side of the road” consists of two specifications: *\"overtake the pedestrian\"* and *\"walk on the right side of the road.\"*\nTo tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. \nUsing diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. \nAdditionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives.\nThrough simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines."
                    },
                    "supplementary_material": {
                        "value": "/attachment/5e4559bc27b1cc6bdbb98b461f3d99ad6ff07541.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We present ComposableNav, a composable, diffusion-based motion planner that composes motion primitives based on the instruction specifications to generate instruction-following motion trajectories."
                    },
                    "pdf": {
                        "value": "/pdf/faeaf7fe5c477085bc968503b31bb8fec5f861f1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhu2025composablenav,\ntitle={ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion},\nauthor={Zichao Hu and Chen Tang and Michael Joseph Munje and Yifeng Zhu and Alex Liu and Shuijing Liu and Garrett Warnell and Peter Stone and Joydeep Biswas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FBsawSyYBM}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/44e413dc73f7f18a2a5a430ccaf41a0ff461a907.mp4"
                    },
                    "paperhash": {
                        "value": "hu|composablenav_instructionfollowing_navigation_in_dynamic_environments_via_composable_diffusion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission627/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission627/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission627/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745418810262,
                "pdate": 1754680627301,
                "odate": 1758062775494,
                "mdate": 1758062819141,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission627/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission627/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "lJWUourMTT",
        "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
        "abstract": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree  combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs, to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree has comparable runtimes to a standalone DP (4x faster than classical SBPs), while improving the success rate over DP and SBPs (on average).",
        "keywords": [
            "Kinodynamic motion planning",
            "diffusion models",
            "nonlinear systems"
        ],
        "pdf_url": "https://openreview.net/pdf/fd9238b56dca40187dc8404fd9d52c1fa4c0d9f3.pdf",
        "reviews": [
            {
                "id": "crs0kLrxpH",
                "forum": "lJWUourMTT",
                "replyto": "lJWUourMTT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission619/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068292408,
                "mdate": 1754869470718,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "lJWUourMTT",
                "forum": "lJWUourMTT",
                "content": {
                    "title": {
                        "value": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees"
                    },
                    "authors": {
                        "value": [
                            "Yaniv Hassidof",
                            "Tom Jurgenson",
                            "Kiril Solovey"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yaniv_Hassidof1",
                            "~Tom_Jurgenson1",
                            "~Kiril_Solovey2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Kinodynamic motion planning",
                            "diffusion models",
                            "nonlinear systems"
                        ]
                    },
                    "TLDR": {
                        "value": "Using diffusion policies trained on a single environment as action samplers for sampling-based planners in unseen environments"
                    },
                    "abstract": {
                        "value": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree  combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs, to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree has comparable runtimes to a standalone DP (4x faster than classical SBPs), while improving the success rate over DP and SBPs (on average)."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/fd9238b56dca40187dc8404fd9d52c1fa4c0d9f3.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhassidof2025trainonce,\ntitle={Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees},\nauthor={Yaniv Hassidof and Tom Jurgenson and Kiril Solovey},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=lJWUourMTT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a531ea3dc6162041fc90b102ed0a0e9d12b53901.mp4"
                    },
                    "paperhash": {
                        "value": "hassidof|trainonce_plananywhere_kinodynamic_motion_planning_via_diffusion_trees"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission619/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission619/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission619/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745417400811,
                "pdate": 1754680627144,
                "odate": 1758062775358,
                "mdate": 1758062818996,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission619/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission619/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "XF69ltYlMU",
        "title": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement",
        "abstract": "Multi-object rearrangement is a challenging task that requires robots to reason about a physical 3D scene and the effects of a sequence of actions.\nWhile traditional task planning methods are shown to be effective for long-horizon manipulation, they require discretizing the continuous state and action space into symbolic descriptions of objects, object relationships, and actions.\nOur proposed method is instead able to take in a partially-observed point cloud observation of an initial scene and plan to a goal-satisfying configuration, without needing to discretize the set of actions or object relationships.\nTo enable this, we formulate the planning problem as an A* search over the space of possible point cloud rearrangements.\nWe sample point cloud transformations from a learned, domain-specific prior and then search for a sequence of such point cloud transformations that leads from the initial state to a goal.\nWe evaluate our method in terms of task planning success and task execution success on a real-world, multi-step table bussing environment and a simulation block stacking environment.\nWe experimentally demonstrate that our method produces successful plans and outperforms a policy-learning approach; we also perform ablations that show the importance of search in our approach.",
        "keywords": [
            "Robot Learning",
            "Robot Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/51697a87241b90da0b133b7a030c00fc767a07e9.pdf",
        "reviews": [
            {
                "id": "kAXFUAeXIq",
                "forum": "XF69ltYlMU",
                "replyto": "XF69ltYlMU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission605/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068291938,
                "mdate": 1754869457303,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "XF69ltYlMU",
                "forum": "XF69ltYlMU",
                "content": {
                    "title": {
                        "value": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement"
                    },
                    "authors": {
                        "value": [
                            "Kallol Saha",
                            "Amber Li",
                            "Angela Rodriguez-Izquierdo",
                            "Lifan Yu",
                            "Ben Eisner",
                            "Maxim Likhachev",
                            "David Held"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kallol_Saha1",
                            "~Amber_Li1",
                            "~Angela_Rodriguez-Izquierdo2",
                            "~Lifan_Yu1",
                            "~Ben_Eisner1",
                            "~Maxim_Likhachev1",
                            "~David_Held1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Learning",
                            "Robot Planning"
                        ]
                    },
                    "abstract": {
                        "value": "Multi-object rearrangement is a challenging task that requires robots to reason about a physical 3D scene and the effects of a sequence of actions.\nWhile traditional task planning methods are shown to be effective for long-horizon manipulation, they require discretizing the continuous state and action space into symbolic descriptions of objects, object relationships, and actions.\nOur proposed method is instead able to take in a partially-observed point cloud observation of an initial scene and plan to a goal-satisfying configuration, without needing to discretize the set of actions or object relationships.\nTo enable this, we formulate the planning problem as an A* search over the space of possible point cloud rearrangements.\nWe sample point cloud transformations from a learned, domain-specific prior and then search for a sequence of such point cloud transformations that leads from the initial state to a goal.\nWe evaluate our method in terms of task planning success and task execution success on a real-world, multi-step table bussing environment and a simulation block stacking environment.\nWe experimentally demonstrate that our method produces successful plans and outperforms a policy-learning approach; we also perform ablations that show the importance of search in our approach."
                    },
                    "supplementary_material": {
                        "value": "/attachment/6529685cafe6a39918edf7eca15ffbed7cfb8972.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/51697a87241b90da0b133b7a030c00fc767a07e9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsaha2025planning,\ntitle={Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement},\nauthor={Kallol Saha and Amber Li and Angela Rodriguez-Izquierdo and Lifan Yu and Ben Eisner and Maxim Likhachev and David Held},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XF69ltYlMU}\n}"
                    },
                    "paperhash": {
                        "value": "saha|planning_from_point_clouds_over_continuous_actions_for_multiobject_rearrangement"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission605/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission605/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission605/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745413081240,
                "pdate": 1754680626670,
                "odate": 1758062774946,
                "mdate": 1758062818938,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission605/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "R5oDzOZYwb",
        "title": "ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning",
        "abstract": "The computational burden of model predictive control (MPC) limits its application on real-time systems, such as robots, and often requires the use of short prediction horizons. This not only affects the control performance, but also increases the difficulty of designing MPC cost functions that reflect the desired long-term objective. This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC. It improves performance over alternative methods, such as approximate explicit MPC and automatic cost parameter tuning, in particular in terms of i) optimizing the long-term objective; ii) maintaining computational costs comparable to a short-horizon MPC; iii) ensuring constraint satisfaction; and iv) generalizing control behaviour to environments not observed during training. For this purpose, ZipMPC leverages the concept of differentiable MPC with neural networks to propagate gradients of the imitation loss through the MPC optimization. We validate our proposed method in simulation and real-world experiments on autonomous racing. ZipMPC consistently completes laps faster than selected baselines, achieving lap times close to the long-horizon MPC baseline. In challenging scenarios where the short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In particular, these performance gains are also observed on tracks unseen during training.",
        "keywords": [
            "Imitation Learning",
            "Model Predictive Control",
            "Context-Dependent Control"
        ],
        "pdf_url": "https://openreview.net/pdf/7e51401523ecf3a630c3d240ba8a0e9472fb9f8f.pdf",
        "reviews": [
            {
                "id": "RD3oT5IFQd",
                "forum": "R5oDzOZYwb",
                "replyto": "R5oDzOZYwb",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission591/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068291373,
                "mdate": 1754869470607,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "R5oDzOZYwb",
                "forum": "R5oDzOZYwb",
                "content": {
                    "title": {
                        "value": "ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning"
                    },
                    "authors": {
                        "value": [
                            "Rahel Rickenbach",
                            "Alan Lahoud",
                            "Erik Schaffernicht",
                            "Melanie Zeilinger",
                            "Johannes A. Stork"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Rahel_Rickenbach1",
                            "~Alan_Lahoud1",
                            "~Erik_Schaffernicht1",
                            "~Melanie_Zeilinger1",
                            "~Johannes_A._Stork1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Model Predictive Control",
                            "Context-Dependent Control"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC to enhance computational efficiency."
                    },
                    "abstract": {
                        "value": "The computational burden of model predictive control (MPC) limits its application on real-time systems, such as robots, and often requires the use of short prediction horizons. This not only affects the control performance, but also increases the difficulty of designing MPC cost functions that reflect the desired long-term objective. This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC. It improves performance over alternative methods, such as approximate explicit MPC and automatic cost parameter tuning, in particular in terms of i) optimizing the long-term objective; ii) maintaining computational costs comparable to a short-horizon MPC; iii) ensuring constraint satisfaction; and iv) generalizing control behaviour to environments not observed during training. For this purpose, ZipMPC leverages the concept of differentiable MPC with neural networks to propagate gradients of the imitation loss through the MPC optimization. We validate our proposed method in simulation and real-world experiments on autonomous racing. ZipMPC consistently completes laps faster than selected baselines, achieving lap times close to the long-horizon MPC baseline. In challenging scenarios where the short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In particular, these performance gains are also observed on tracks unseen during training."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e7e513cc97aa3cc903e1d4951310230c7b6f412e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7e51401523ecf3a630c3d240ba8a0e9472fb9f8f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nrickenbach2025zipmpc,\ntitle={Zip{MPC}: Compressed Context-Dependent {MPC} Cost via Imitation Learning},\nauthor={Rahel Rickenbach and Alan Lahoud and Erik Schaffernicht and Melanie Zeilinger and Johannes A. Stork},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=R5oDzOZYwb}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/87b897e7e5e0a53171be664bd5c91a4ceef9b5fd.zip"
                    },
                    "paperhash": {
                        "value": "rickenbach|zipmpc_compressed_contextdependent_mpc_cost_via_imitation_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission591/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission591/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission591/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745410376463,
                "pdate": 1754680626118,
                "odate": 1758062774562,
                "mdate": 1758062818884,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission591/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "7XyO9Y1hI1",
        "title": "EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy",
        "abstract": "In endoscopic procedures, autonomous tracking of abnormal regions and following of circumferential cutting markers can significantly reduce the cognitive burden on endoscopists. However, conventional model-based pipelines are fragile—each component (e.g., detection, motion planning) requires manual tuning and struggles to incorporate high-level endoscopic intent, resulting in poor generalization across variable scenes. Vision–Language–Action (VLA) models, which integrate visual perception, language grounding, and motion planning within an end-to-end framework, offer a promising alternative to semantically adapt to surgeon prompts, without the need for manual recalibration. Despite their potential, applying VLA models to robotic endoscopy presents unique challenges due to the inherently complex and dynamic anatomical environments of the gastrointestinal (GI) tract. To this end, we introduce EndoVLA, designed specifically for continuum robots in GI interventions. Provided endoscopic images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1) polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3) adherence to predefined circular markers during circumferential cutting. To address the unique challenges posed by data scarcity and domain shifts, we propose a dual-phase strategy, with supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement fine-tuning using task-aware rewards. Our approach significantly enhances the tracking performance in endoscopy, and zero-shot generalization of tracking in general scenes and more challenging sequential tasks.",
        "keywords": [
            "Vision–Language–Action",
            "Continuum Robots",
            "Autonomous Endoscopic Tracking",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/683ef329160bf97140328f118a5968b7fa8378b1.pdf",
        "reviews": [
            {
                "id": "ozQbrtNAVV",
                "forum": "7XyO9Y1hI1",
                "replyto": "7XyO9Y1hI1",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission589/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068291184,
                "mdate": 1754869470545,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "7XyO9Y1hI1",
                "forum": "7XyO9Y1hI1",
                "content": {
                    "title": {
                        "value": "EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy"
                    },
                    "authors": {
                        "value": [
                            "CHI KIT NG",
                            "Long Bai",
                            "Guankun Wang",
                            "Yupeng Wang",
                            "Huxin Gao",
                            "Kun yuan",
                            "Chenhan Jin",
                            "Tieyong Zeng",
                            "Hongliang Ren"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~CHI_KIT_NG1",
                            "~Long_Bai2",
                            "~Guankun_Wang1",
                            "~Yupeng_Wang3",
                            "~Huxin_Gao1",
                            "~Kun_yuan6",
                            "~Chenhan_Jin2",
                            "~Tieyong_Zeng2",
                            "~Hongliang_Ren3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision–Language–Action",
                            "Continuum Robots",
                            "Autonomous Endoscopic Tracking",
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "In endoscopic procedures, autonomous tracking of abnormal regions and following of circumferential cutting markers can significantly reduce the cognitive burden on endoscopists. However, conventional model-based pipelines are fragile—each component (e.g., detection, motion planning) requires manual tuning and struggles to incorporate high-level endoscopic intent, resulting in poor generalization across variable scenes. Vision–Language–Action (VLA) models, which integrate visual perception, language grounding, and motion planning within an end-to-end framework, offer a promising alternative to semantically adapt to surgeon prompts, without the need for manual recalibration. Despite their potential, applying VLA models to robotic endoscopy presents unique challenges due to the inherently complex and dynamic anatomical environments of the gastrointestinal (GI) tract. To this end, we introduce EndoVLA, designed specifically for continuum robots in GI interventions. Provided endoscopic images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1) polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3) adherence to predefined circular markers during circumferential cutting. To address the unique challenges posed by data scarcity and domain shifts, we propose a dual-phase strategy, with supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement fine-tuning using task-aware rewards. Our approach significantly enhances the tracking performance in endoscopy, and zero-shot generalization of tracking in general scenes and more challenging sequential tasks."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/683ef329160bf97140328f118a5968b7fa8378b1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkit2025endovla,\ntitle={Endo{VLA}: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy},\nauthor={CHI KIT NG and Long Bai and Guankun Wang and Yupeng Wang and Huxin Gao and Kun yuan and Chenhan Jin and Tieyong Zeng and Hongliang Ren},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7XyO9Y1hI1}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/336fa415235d52518b79345bc6317e3cce14eaee.zip"
                    },
                    "paperhash": {
                        "value": "ng|endovla_dualphase_visionlanguageaction_for_precise_autonomous_tracking_in_endoscopy"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission589/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission589/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission589/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745410157744,
                "pdate": 1754680625971,
                "odate": 1758062774428,
                "mdate": 1759166346539,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission589/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "pukgxvcOwL",
        "title": "“Stack It Up!”: 3D Stable Structure Generation from 2D Hand-drawn Sketch",
        "abstract": "Imagine a child sketching the Eiffel Tower and asking a robot to bring it to life. Today’s robot manipulation systems can’t act on such sketches directly—they require precise 3D block poses as goals, which in turn demand structural analysis and expert tools like CAD. We present *StackItUp*, a system that enables non-experts to specify complex 3D structures using only 2D front-view hand-drawn sketches. *StackItUp* introduces an abstract relation graph to bridge the gap between rough sketches and accurate 3D block arrangements, capturing the symbolic geometric relations (e.g., *left-of*) and stability patterns (e.g.,*two-pillar-bridge*) while discarding noisy metric details from sketches. It then grounds this graph to 3D poses using compositional diffusion models and iteratively updates it by predicting hidden internal and rear supports—critical for stability but absent from the sketch. Evaluated on sketches of iconic landmarks and modern house designs, *StackItUp* consistently produces stable, multilevel 3D structures and outperforms all baselines in both stability and visual resemblance.",
        "keywords": [
            "Goal Specification",
            "Diffusion Models"
        ],
        "pdf_url": "https://openreview.net/pdf/f04d3646c7af0dd1bf96f2075371b1e78d7e9c79.pdf",
        "reviews": [
            {
                "id": "LDLUU0CQN4",
                "forum": "pukgxvcOwL",
                "replyto": "pukgxvcOwL",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission587/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068291132,
                "mdate": 1754869457212,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "pukgxvcOwL",
                "forum": "pukgxvcOwL",
                "content": {
                    "title": {
                        "value": "“Stack It Up!”: 3D Stable Structure Generation from 2D Hand-drawn Sketch"
                    },
                    "authors": {
                        "value": [
                            "Yiqing Xu",
                            "Linfeng Li",
                            "Cunjun Yu",
                            "David Hsu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yiqing_Xu1",
                            "~Linfeng_Li2",
                            "~Cunjun_Yu1",
                            "~David_Hsu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Goal Specification",
                            "Diffusion Models"
                        ]
                    },
                    "TLDR": {
                        "value": "We generate stable, multi-level 3D structure based on a simple rough 2D front-view sketch, even when the sketch is incomplete or imprecise."
                    },
                    "abstract": {
                        "value": "Imagine a child sketching the Eiffel Tower and asking a robot to bring it to life. Today’s robot manipulation systems can’t act on such sketches directly—they require precise 3D block poses as goals, which in turn demand structural analysis and expert tools like CAD. We present *StackItUp*, a system that enables non-experts to specify complex 3D structures using only 2D front-view hand-drawn sketches. *StackItUp* introduces an abstract relation graph to bridge the gap between rough sketches and accurate 3D block arrangements, capturing the symbolic geometric relations (e.g., *left-of*) and stability patterns (e.g.,*two-pillar-bridge*) while discarding noisy metric details from sketches. It then grounds this graph to 3D poses using compositional diffusion models and iteratively updates it by predicting hidden internal and rear supports—critical for stability but absent from the sketch. Evaluated on sketches of iconic landmarks and modern house designs, *StackItUp* consistently produces stable, multilevel 3D structures and outperforms all baselines in both stability and visual resemblance."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d754acd1def95a79d58ea7ce6f784d2661bb1d84.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f04d3646c7af0dd1bf96f2075371b1e78d7e9c79.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025stack,\ntitle={{\\textquotedblleft}Stack It Up!{\\textquotedblright}: 3D Stable Structure Generation from 2D Hand-drawn Sketch},\nauthor={Yiqing Xu and Linfeng Li and Cunjun Yu and David Hsu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=pukgxvcOwL}\n}"
                    },
                    "paperhash": {
                        "value": "xu|stack_it_up_3d_stable_structure_generation_from_2d_handdrawn_sketch"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission587/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission587/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission587/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745409490426,
                "pdate": 1754680625969,
                "odate": 1758062774426,
                "mdate": 1758062818599,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission587/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission587/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jZRz7Hvsyd",
        "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation",
        "abstract": "Humans naturally exhibit bilateral symmetry in their gross manipulation skills, effortlessly mirroring simple actions between left and right hands. Bimanual robots—which also feature bilateral symmetry—should similarly exploit this property to perform tasks with either hand. Unlike humans, who often favor a dominant hand for fine dexterous skills, robots should ideally execute ambidextrous manipulation with equal proficiency. To this end, we introduce SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for ambidextrous bi-manipulation that leverages the robot's inherent bilateral symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation tasks into per-hand subtasks and trains dedicated policies for each. By exploiting bilateral symmetry via equivariant neural networks, experience from one arm is inherently leveraged by the opposite arm. We then distill the subtask policies into a global ambidextrous policy that is independent of the hand-task assignment. We evaluate SYMDEX on six challenging simulated manipulation tasks and demonstrate successful real-world deployment on two of them. Our approach outperforms baselines on more complex, asymmetric tasks, where the left and right hands perform different roles. We further demonstrate SYMDEX’s scalability by extending it to a four-arm manipulation setup, where our symmetry-aware policies enable effective multi-arm collaboration and coordination. Our results highlight how structural symmetry as inductive bias in policy learning enhances sample efficiency, robustness, and generalization across diverse dexterous manipulation tasks.",
        "keywords": [
            "Bimanual Dexterous Manipulation",
            "Sim-to-Real Transfer",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/a0574ed90dad9fa5f9e19745115317ae17dcfa10.pdf",
        "reviews": [
            {
                "id": "JwJypxLwy1",
                "forum": "jZRz7Hvsyd",
                "replyto": "jZRz7Hvsyd",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission574/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068290841,
                "mdate": 1754869457139,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jZRz7Hvsyd",
                "forum": "jZRz7Hvsyd",
                "content": {
                    "title": {
                        "value": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Zechu Li",
                            "Yufeng Jin",
                            "Daniel Ordonez-Apraez",
                            "Claudio Semini",
                            "Puze Liu",
                            "Georgia Chalvatzaki"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zechu_Li1",
                            "~Yufeng_Jin2",
                            "~Daniel_Ordonez-Apraez1",
                            "~Claudio_Semini1",
                            "~Puze_Liu1",
                            "~Georgia_Chalvatzaki1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Bimanual Dexterous Manipulation",
                            "Sim-to-Real Transfer",
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Humans naturally exhibit bilateral symmetry in their gross manipulation skills, effortlessly mirroring simple actions between left and right hands. Bimanual robots—which also feature bilateral symmetry—should similarly exploit this property to perform tasks with either hand. Unlike humans, who often favor a dominant hand for fine dexterous skills, robots should ideally execute ambidextrous manipulation with equal proficiency. To this end, we introduce SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for ambidextrous bi-manipulation that leverages the robot's inherent bilateral symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation tasks into per-hand subtasks and trains dedicated policies for each. By exploiting bilateral symmetry via equivariant neural networks, experience from one arm is inherently leveraged by the opposite arm. We then distill the subtask policies into a global ambidextrous policy that is independent of the hand-task assignment. We evaluate SYMDEX on six challenging simulated manipulation tasks and demonstrate successful real-world deployment on two of them. Our approach outperforms baselines on more complex, asymmetric tasks, where the left and right hands perform different roles. We further demonstrate SYMDEX’s scalability by extending it to a four-arm manipulation setup, where our symmetry-aware policies enable effective multi-arm collaboration and coordination. Our results highlight how structural symmetry as inductive bias in policy learning enhances sample efficiency, robustness, and generalization across diverse dexterous manipulation tasks."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/a0574ed90dad9fa5f9e19745115317ae17dcfa10.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025morphologically,\ntitle={Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation},\nauthor={Zechu Li and Yufeng Jin and Daniel Ordonez-Apraez and Claudio Semini and Puze Liu and Georgia Chalvatzaki},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jZRz7Hvsyd}\n}"
                    },
                    "paperhash": {
                        "value": "li|morphologically_symmetric_reinforcement_learning_for_ambidextrous_bimanual_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission574/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission574/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission574/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745405268379,
                "pdate": 1754680625661,
                "odate": 1758062774139,
                "mdate": 1758062818655,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission574/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission574/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "wZUQq0JaL6",
        "title": "Robot Operating Home Appliances by Reading User Manuals",
        "abstract": "Operating home appliances, among the most common tools in every\nhousehold, is a critical capability for assistive home robots. This paper presents\nApBot, a robot system that operates novel household appliances by “reading” their\nuser manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual document,\n(ii) ground the policies to the appliance in the physical world, and (iii) execute\nthe policies reliably over potentially many steps, despite compounding errors. To\ntackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It\ngrounds the symbolic actions visually to control panel elements. Finally, ApBot\ncloses the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success\nrate, compared with state-of-the-art large VLMs used directly as control policies.\nThese results suggest that a structured internal representations plays an important\nrole in robust robot operation of home appliances, especially, complex ones.",
        "keywords": [
            "Home Appliance Operation",
            "Structured Model for Decision Making",
            "Foundation Models for Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/829e650d73458bea6ce5dffda692508763f494c4.pdf",
        "reviews": [
            {
                "id": "wMbUSHGqgP",
                "forum": "wZUQq0JaL6",
                "replyto": "wZUQq0JaL6",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission570/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068290588,
                "mdate": 1754869470272,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "wZUQq0JaL6",
                "forum": "wZUQq0JaL6",
                "content": {
                    "title": {
                        "value": "Robot Operating Home Appliances by Reading User Manuals"
                    },
                    "authors": {
                        "value": [
                            "Jian Zhang",
                            "Hanbo Zhang",
                            "Anxing Xiao",
                            "David Hsu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jian_Zhang34",
                            "~Hanbo_Zhang1",
                            "~Anxing_Xiao1",
                            "~David_Hsu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Home Appliance Operation",
                            "Structured Model for Decision Making",
                            "Foundation Models for Robotics"
                        ]
                    },
                    "TLDR": {
                        "value": "Teaching robots to accurately operate home appliances by reading user manuals"
                    },
                    "abstract": {
                        "value": "Operating home appliances, among the most common tools in every\nhousehold, is a critical capability for assistive home robots. This paper presents\nApBot, a robot system that operates novel household appliances by “reading” their\nuser manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual document,\n(ii) ground the policies to the appliance in the physical world, and (iii) execute\nthe policies reliably over potentially many steps, despite compounding errors. To\ntackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It\ngrounds the symbolic actions visually to control panel elements. Finally, ApBot\ncloses the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success\nrate, compared with state-of-the-art large VLMs used directly as control policies.\nThese results suggest that a structured internal representations plays an important\nrole in robust robot operation of home appliances, especially, complex ones."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7117445e58c27bfb57e118b77b5afb2331e33f52.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/829e650d73458bea6ce5dffda692508763f494c4.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025robot,\ntitle={Robot Operating Home Appliances by Reading User Manuals},\nauthor={Jian Zhang and Hanbo Zhang and Anxing Xiao and David Hsu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=wZUQq0JaL6}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/60652aa304193973705850b79d6fa11f9415808e.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|robot_operating_home_appliances_by_reading_user_manuals"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission570/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission570/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission570/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745404254689,
                "pdate": 1754680625598,
                "odate": 1758062774053,
                "mdate": 1758062818482,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission570/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission570/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JrhMGXZnja",
        "title": "CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation",
        "abstract": "We propose CARE (Collision Avoidance via Repulsive Estimation) for improving the robustness of learning-based visual navigation methods. Recently, visual navigation models, particularly foundation models, have demonstrated promising performance by generating viable trajectories using only RGB images. However, these policies can generalize poorly to environments containing out-of-distribution (OOD) scenes characterized by unseen objects or different camera setups (e.g., variations in field of view, camera pose, or focal length). Without fine-tuning, such models could produce trajectories that lead to collisions, necessitating substantial efforts in data collection and additional training.\nTo address this limitation, we introduce CARE, an attachable module that enhances the safety of visual navigation without requiring additional range sensors or fine-tuning of pretrained models. CARE can be integrated seamlessly into any RGB-based navigation model that generates local robot trajectories. It dynamically adjusts trajectories produced by a pretrained model using repulsive force vectors computed from depth images estimated directly from RGB inputs. \nWe evaluate CARE by integrating it with state-of-the-art visual navigation models across diverse robot platforms. Real-world experiments show that CARE significantly reduces collisions (up to 100%) without compromising navigation performance in goal-conditioned navigation, and further improves collision-free travel distance (up to 10.7×) in exploration tasks.",
        "keywords": [
            "Vision-based Navigation",
            "Reactive Planning",
            "Collision Avoidance"
        ],
        "pdf_url": "https://openreview.net/pdf/011d212b72847865b7973fc9d513647b64a77744.pdf",
        "reviews": [
            {
                "id": "wVD1hHr84Z",
                "forum": "JrhMGXZnja",
                "replyto": "JrhMGXZnja",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission568/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068290585,
                "mdate": 1754869470128,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JrhMGXZnja",
                "forum": "JrhMGXZnja",
                "content": {
                    "title": {
                        "value": "CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation"
                    },
                    "authors": {
                        "value": [
                            "Joonkyung Kim",
                            "Joonyeol Sim",
                            "Woojun Kim",
                            "Katia P. Sycara",
                            "Changjoo Nam"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Joonkyung_Kim1",
                            "~Joonyeol_Sim1",
                            "~Woojun_Kim1",
                            "~Katia_P._Sycara1",
                            "~Changjoo_Nam1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-based Navigation",
                            "Reactive Planning",
                            "Collision Avoidance"
                        ]
                    },
                    "TLDR": {
                        "value": "We present CARE, an attachable safety enhancement module that integrates repulsive force-based trajectory adjustment into RGB-based navigation models, reducing collisions by up to 50% in real-world OOD scenarios without requiring fine-tuning."
                    },
                    "abstract": {
                        "value": "We propose CARE (Collision Avoidance via Repulsive Estimation) for improving the robustness of learning-based visual navigation methods. Recently, visual navigation models, particularly foundation models, have demonstrated promising performance by generating viable trajectories using only RGB images. However, these policies can generalize poorly to environments containing out-of-distribution (OOD) scenes characterized by unseen objects or different camera setups (e.g., variations in field of view, camera pose, or focal length). Without fine-tuning, such models could produce trajectories that lead to collisions, necessitating substantial efforts in data collection and additional training.\nTo address this limitation, we introduce CARE, an attachable module that enhances the safety of visual navigation without requiring additional range sensors or fine-tuning of pretrained models. CARE can be integrated seamlessly into any RGB-based navigation model that generates local robot trajectories. It dynamically adjusts trajectories produced by a pretrained model using repulsive force vectors computed from depth images estimated directly from RGB inputs. \nWe evaluate CARE by integrating it with state-of-the-art visual navigation models across diverse robot platforms. Real-world experiments show that CARE significantly reduces collisions (up to 100%) without compromising navigation performance in goal-conditioned navigation, and further improves collision-free travel distance (up to 10.7×) in exploration tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/09f2c8ae6f890af5c4d34e93a4ad6fbe3f82e652.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/011d212b72847865b7973fc9d513647b64a77744.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkim2025care,\ntitle={{CARE}: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation},\nauthor={Joonkyung Kim and Joonyeol Sim and Woojun Kim and Katia P. Sycara and Changjoo Nam},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JrhMGXZnja}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/9703a0e0a3aa7c613792a5e7a7be9428fc692b80.zip"
                    },
                    "paperhash": {
                        "value": "kim|care_enhancing_safety_of_visual_navigation_through_collision_avoidance_via_repulsive_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission568/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission568/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission568/-/Spotlight",
                    "robot-learning.org/CoRL/2025/Conference/Submission568/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745403932934,
                "pdate": 1754680625468,
                "odate": 1758062774010,
                "mdate": 1758062818243,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission568/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "CBYGhryESq",
        "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence",
        "abstract": "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with 3D functional keypoints, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects.",
        "keywords": [
            "Tool Manipulation",
            "Imitation from Human Video"
        ],
        "pdf_url": "https://openreview.net/pdf/f559bf20f3a4a82d936697a3c431c15e7e15e72a.pdf",
        "reviews": [
            {
                "id": "1W76OuZcz7",
                "forum": "CBYGhryESq",
                "replyto": "CBYGhryESq",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission565/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068290489,
                "mdate": 1754869470024,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "CBYGhryESq",
                "forum": "CBYGhryESq",
                "content": {
                    "title": {
                        "value": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence"
                    },
                    "authors": {
                        "value": [
                            "Chao Tang",
                            "Anxing Xiao",
                            "Yuhong Deng",
                            "Tianrun Hu",
                            "Wenlong Dong",
                            "Hanbo Zhang",
                            "David Hsu",
                            "Hong Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chao_Tang6",
                            "~Anxing_Xiao1",
                            "~Yuhong_Deng1",
                            "~Tianrun_Hu1",
                            "~Wenlong_Dong1",
                            "~Hanbo_Zhang1",
                            "~David_Hsu1",
                            "~Hong_Zhang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tool Manipulation",
                            "Imitation from Human Video"
                        ]
                    },
                    "abstract": {
                        "value": "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with 3D functional keypoints, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects."
                    },
                    "supplementary_material": {
                        "value": "/attachment/12adad065cfd32a2d6ff552c9b50ecb4ac7a3d10.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f559bf20f3a4a82d936697a3c431c15e7e15e72a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntang2025mimicfunc,\ntitle={MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence},\nauthor={Chao Tang and Anxing Xiao and Yuhong Deng and Tianrun Hu and Wenlong Dong and Hanbo Zhang and David Hsu and Hong Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CBYGhryESq}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/713a8b484a201c5537adbf1e8b01979acf857601.mp4"
                    },
                    "paperhash": {
                        "value": "tang|mimicfunc_imitating_tool_manipulation_from_a_single_human_video_via_functional_correspondence"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission565/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission565/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745403187258,
                "pdate": 1754680625403,
                "odate": 1758062773931,
                "mdate": 1758062818241,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission565/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission565/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "uA9GZEmGiT",
        "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction",
        "abstract": "Path planning in unknown environments is a crucial yet inherently challenging capability for mobile robots, which primarily encompasses two coupled tasks: autonomous exploration and point-goal navigation. In both cases, the robot must perceive the environment, update its belief, and accurately estimate potential information gain on-the-fly to guide planning. In this work, we propose CogniPlan, a novel path planning framework that leverages multiple plausible layouts predicted by a conditional generative inpainting model, mirroring how humans rely on cognitive maps during navigation. These predictions, based on the partially observed map and a set of layout conditioning vectors, enable our planner to reason effectively under uncertainty. We demonstrate strong synergy between generative image-based layout prediction and graph-attention-based path planning, allowing CogniPlan to combine the scalability of graph representations with the fidelity and predictiveness of occupancy maps, yielding notable performance gains in both exploration and navigation. We extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic floor plans), consistently outperforming state-of-the-art planners. We further deploy it in a high-fidelity simulator and on hardware, showcasing its high-quality path planning and real-world applicability.",
        "keywords": [
            "Path Planning under Uncertainty",
            "Map Prediction",
            "Graph Attention"
        ],
        "pdf_url": "https://openreview.net/pdf/4e7f31cd34ca62e095e29d9e31607b8994919a59.pdf",
        "reviews": [
            {
                "id": "amfXhJ9ZSs",
                "forum": "uA9GZEmGiT",
                "replyto": "uA9GZEmGiT",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission557/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068290313,
                "mdate": 1754869469994,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "uA9GZEmGiT",
                "forum": "uA9GZEmGiT",
                "content": {
                    "title": {
                        "value": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction"
                    },
                    "authors": {
                        "value": [
                            "Yizhuo Wang",
                            "Haodong He",
                            "Jingsong Liang",
                            "Yuhong Cao",
                            "Ritabrata Chakraborty",
                            "Guillaume Adrien Sartoretti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yizhuo_Wang1",
                            "~Haodong_He2",
                            "~Jingsong_Liang1",
                            "~Yuhong_Cao1",
                            "~Ritabrata_Chakraborty2",
                            "~Guillaume_Adrien_Sartoretti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Path Planning under Uncertainty",
                            "Map Prediction",
                            "Graph Attention"
                        ]
                    },
                    "abstract": {
                        "value": "Path planning in unknown environments is a crucial yet inherently challenging capability for mobile robots, which primarily encompasses two coupled tasks: autonomous exploration and point-goal navigation. In both cases, the robot must perceive the environment, update its belief, and accurately estimate potential information gain on-the-fly to guide planning. In this work, we propose CogniPlan, a novel path planning framework that leverages multiple plausible layouts predicted by a conditional generative inpainting model, mirroring how humans rely on cognitive maps during navigation. These predictions, based on the partially observed map and a set of layout conditioning vectors, enable our planner to reason effectively under uncertainty. We demonstrate strong synergy between generative image-based layout prediction and graph-attention-based path planning, allowing CogniPlan to combine the scalability of graph representations with the fidelity and predictiveness of occupancy maps, yielding notable performance gains in both exploration and navigation. We extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic floor plans), consistently outperforming state-of-the-art planners. We further deploy it in a high-fidelity simulator and on hardware, showcasing its high-quality path planning and real-world applicability."
                    },
                    "supplementary_material": {
                        "value": "/attachment/27fd7c0c3a7ca14f2c5d23eacac9063da2fbd1ec.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We introduce CogniPlan, a path planning framework that leverages the synergy between generative layout prediction and graph-attention-based planning to enable uncertainty-aware, efficient exploration and navigation in unknown environments."
                    },
                    "pdf": {
                        "value": "/pdf/4e7f31cd34ca62e095e29d9e31607b8994919a59.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025cogniplan,\ntitle={CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction},\nauthor={Yizhuo Wang and Haodong He and Jingsong Liang and Yuhong Cao and Ritabrata Chakraborty and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=uA9GZEmGiT}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0cfb33bea72f57dfb26babcd657a3ef9413375a5.mp4"
                    },
                    "paperhash": {
                        "value": "wang|cogniplan_uncertaintyguided_path_planning_with_conditional_generative_layout_prediction"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission557/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission557/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission557/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745401848808,
                "pdate": 1754680625275,
                "odate": 1758062773822,
                "mdate": 1758062818080,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission557/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission557/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Nt4LmgZ7v9",
        "title": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings",
        "abstract": "Diffusion and flow matching policies have recently demonstrated remarkable performance in robotic applications by accurately capturing multimodal robot trajectory distributions. However, their computationally expensive inference, due to the numerical integration of an ODE or SDE, limits their applicability as real-time controllers for robots. We introduce a methodology that utilizes conditional Optimal Transport couplings between noise and samples to enforce straight solutions in the flow ODE for robot action generation tasks. We show that naively coupling noise and samples fails in conditional tasks and propose incorporating condition variables into the coupling process to improve few-step performance. The proposed few-step policy achieves a 4\\% higher success rate with a 10$\\times$ speed-up compared to Diffusion Policy on a diverse set of simulation tasks. Moreover, it produces high-quality and diverse action trajectories within 1-2 steps on a set of real-world robot tasks. Our method also retains the same training complexity as Diffusion Policy and vanilla Flow Matching, in contrast to distillation-based approaches.",
        "keywords": [
            "Flow Matching",
            "Optimal Transport",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/74161a8e41026ae9af71ee30a47b036c475d467a.pdf",
        "reviews": [
            {
                "id": "VUyHmTMWHN",
                "forum": "Nt4LmgZ7v9",
                "replyto": "Nt4LmgZ7v9",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission547/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068289934,
                "mdate": 1754869469876,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Nt4LmgZ7v9",
                "forum": "Nt4LmgZ7v9",
                "content": {
                    "title": {
                        "value": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings"
                    },
                    "authors": {
                        "value": [
                            "Andreas Sochopoulos",
                            "Nikolay Malkin",
                            "Nikolaos Tsagkas",
                            "Joao Moura",
                            "Michael Gienger",
                            "Sethu Vijayakumar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Andreas_Sochopoulos1",
                            "~Nikolay_Malkin1",
                            "~Nikolaos_Tsagkas1",
                            "~Joao_Moura1",
                            "~Michael_Gienger1",
                            "~Sethu_Vijayakumar1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Flow Matching",
                            "Optimal Transport",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper presents a novel flow-based visuomotor policy with faster inference and same training complexity compared to Diffusion Policies."
                    },
                    "abstract": {
                        "value": "Diffusion and flow matching policies have recently demonstrated remarkable performance in robotic applications by accurately capturing multimodal robot trajectory distributions. However, their computationally expensive inference, due to the numerical integration of an ODE or SDE, limits their applicability as real-time controllers for robots. We introduce a methodology that utilizes conditional Optimal Transport couplings between noise and samples to enforce straight solutions in the flow ODE for robot action generation tasks. We show that naively coupling noise and samples fails in conditional tasks and propose incorporating condition variables into the coupling process to improve few-step performance. The proposed few-step policy achieves a 4\\% higher success rate with a 10$\\times$ speed-up compared to Diffusion Policy on a diverse set of simulation tasks. Moreover, it produces high-quality and diverse action trajectories within 1-2 steps on a set of real-world robot tasks. Our method also retains the same training complexity as Diffusion Policy and vanilla Flow Matching, in contrast to distillation-based approaches."
                    },
                    "supplementary_material": {
                        "value": "/attachment/854f2c235af031acffe0b816af15093ce2b1a711.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/74161a8e41026ae9af71ee30a47b036c475d467a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsochopoulos2025fast,\ntitle={Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings},\nauthor={Andreas Sochopoulos and Nikolay Malkin and Nikolaos Tsagkas and Joao Moura and Michael Gienger and Sethu Vijayakumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Nt4LmgZ7v9}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5f98ea533be6358adb426cb8035619ce03772e30.mp4"
                    },
                    "paperhash": {
                        "value": "sochopoulos|fast_flowbased_visuomotor_policies_via_conditional_optimal_transport_couplings"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission547/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission547/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission547/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745399071703,
                "pdate": 1754680625049,
                "odate": 1758062773650,
                "mdate": 1758062818013,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission547/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "fBRqCMqVyS",
        "title": "Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions",
        "abstract": "Imitation-based policy training for long-horizon manipulation tasks involving multi-step object interactions is often susceptible to compounding action errors. Contemporary methods discover semantic subgoals embedded within the overall task, decomposing the overall task into tractable shorter-horizon goal-conditioned policy learning. However, policy deployment requires iteratively estimating $\\textit{which}$ subgoal is being pursued and $\\textit{when}$ it is achieved. We observe the brittleness of conventional $\\textit{heuristic}$-based approaches (ad hoc threshold based), particularly for long-horizon imitation, since pursuing an incorrect subgoal can lead the robot policy to experience out of distribution states. In this work, we introduce two policy architectures for modeling subgoal transitions within a policy learning loop for long-horizon tasks. The first model autoregressively predicts the likelihood of the next subgoal transition, while the second uses cross-attention (via a transformer-based architecture) and implicitly models smooth and continuous transitions. We evaluate our models on $25$ simulated tasks on Franka Kitchen, $6$ real-world table-top tasks and $18$ simulated tasks on a new corpus (Franka-Long Horizon Tasks (LHT)) focused on tasks with rich object interactions over long episode lengths. Experimental results show significant improvements in learning efficacy, task success rates and generalization to out-of-distribution settings- extending horizon lengths for imitating manipulation tasks $\\textit{from long to long(er)}$.",
        "keywords": [
            "Long Horizon Task Execution",
            "Goal Conditioned Policy Learning",
            "Imitation Learning",
            "Learning from Demonstration",
            "Robot Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/645ee22b77b3e2fc53b0fd53851cf6adf5c2a6e5.pdf",
        "reviews": [
            {
                "id": "qnwkxJpqAN",
                "forum": "fBRqCMqVyS",
                "replyto": "fBRqCMqVyS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission543/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068289877,
                "mdate": 1754869469702,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "fBRqCMqVyS",
                "forum": "fBRqCMqVyS",
                "content": {
                    "title": {
                        "value": "Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions"
                    },
                    "authors": {
                        "value": [
                            "Shivam Jain",
                            "Sachit Sachdeva",
                            "Rohan Paul"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shivam_Jain1",
                            "~Sachit_Sachdeva1",
                            "~Rohan_Paul1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Long Horizon Task Execution",
                            "Goal Conditioned Policy Learning",
                            "Imitation Learning",
                            "Learning from Demonstration",
                            "Robot Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "Learning to predict subgoal attainment aids in imitation of long horizon tasks."
                    },
                    "abstract": {
                        "value": "Imitation-based policy training for long-horizon manipulation tasks involving multi-step object interactions is often susceptible to compounding action errors. Contemporary methods discover semantic subgoals embedded within the overall task, decomposing the overall task into tractable shorter-horizon goal-conditioned policy learning. However, policy deployment requires iteratively estimating $\\textit{which}$ subgoal is being pursued and $\\textit{when}$ it is achieved. We observe the brittleness of conventional $\\textit{heuristic}$-based approaches (ad hoc threshold based), particularly for long-horizon imitation, since pursuing an incorrect subgoal can lead the robot policy to experience out of distribution states. In this work, we introduce two policy architectures for modeling subgoal transitions within a policy learning loop for long-horizon tasks. The first model autoregressively predicts the likelihood of the next subgoal transition, while the second uses cross-attention (via a transformer-based architecture) and implicitly models smooth and continuous transitions. We evaluate our models on $25$ simulated tasks on Franka Kitchen, $6$ real-world table-top tasks and $18$ simulated tasks on a new corpus (Franka-Long Horizon Tasks (LHT)) focused on tasks with rich object interactions over long episode lengths. Experimental results show significant improvements in learning efficacy, task success rates and generalization to out-of-distribution settings- extending horizon lengths for imitating manipulation tasks $\\textit{from long to long(er)}$."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a8cb1a3396e0dcfda7c0cc535ff8623a0fe1468c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/645ee22b77b3e2fc53b0fd53851cf6adf5c2a6e5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njain2025enabling,\ntitle={Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions},\nauthor={Shivam Jain and Sachit Sachdeva and Rohan Paul},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=fBRqCMqVyS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/66b550e35cc38872b02eea4b807fa38e29a4d7d3.zip"
                    },
                    "paperhash": {
                        "value": "jain|enabling_longer_horizon_imitation_for_manipulation_tasks_by_modeling_subgoal_transitions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission543/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission543/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission543/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745398757160,
                "pdate": 1754680624885,
                "odate": 1758062773540,
                "mdate": 1758062817910,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission543/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "iVbCWUDyBF",
        "title": "Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild",
        "abstract": "To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented (compared to real life) in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP’s visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP’s predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate Search-TTA’s performance, we curate a visual search dataset based on internet-scale ecological data. We find that Search-TTA improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.",
        "keywords": [
            "Test-Time Adaptation",
            "Visual Search",
            "VLM",
            "Ecological Monitoring"
        ],
        "pdf_url": "https://openreview.net/pdf/0ccf26acf96b13657767c26a734e39470b648df3.pdf",
        "reviews": [
            {
                "id": "YiSWPCUfD3",
                "forum": "iVbCWUDyBF",
                "replyto": "iVbCWUDyBF",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission536/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068289312,
                "mdate": 1754869469703,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "iVbCWUDyBF",
                "forum": "iVbCWUDyBF",
                "content": {
                    "title": {
                        "value": "Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild"
                    },
                    "authors": {
                        "value": [
                            "Derek Ming Siang Tan",
                            "Shailesh",
                            "Boyang Liu",
                            "Alok Raj",
                            "Qi Xuan Ang",
                            "Weiheng Dai",
                            "Tanishq Duhan",
                            "Jimmy Chiun",
                            "Yuhong Cao",
                            "Florian Shkurti",
                            "Guillaume Adrien Sartoretti"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Derek_Ming_Siang_Tan1",
                            "21je0865@iitism.ac.in",
                            "boyang.liu@u.nus.edu",
                            "22je0091@iitism.ac.in",
                            "qixuan.ang@stengg.com",
                            "~Weiheng_Dai1",
                            "~Tanishq_Duhan1",
                            "~Jimmy_Chiun1",
                            "~Yuhong_Cao1",
                            "~Florian_Shkurti1",
                            "~Guillaume_Adrien_Sartoretti1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Test-Time Adaptation",
                            "Visual Search",
                            "VLM",
                            "Ecological Monitoring"
                        ]
                    },
                    "abstract": {
                        "value": "To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented (compared to real life) in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP’s visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP’s predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate Search-TTA’s performance, we curate a visual search dataset based on internet-scale ecological data. We find that Search-TTA improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9a80baccccc2046164700dbcb3548d1115b7d164.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0ccf26acf96b13657767c26a734e39470b648df3.pdf"
                    },
                    "TLDR": {
                        "value": "We present Search-TTA, a multimodal test-time adaptation framework which improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions, and achieves comparable performance to state-of-the-art VLMs."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntan2025searchtta,\ntitle={Search-{TTA}: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild},\nauthor={Derek Ming Siang Tan and Shailesh and Boyang Liu and Alok Raj and Qi Xuan Ang and Weiheng Dai and Tanishq Duhan and Jimmy Chiun and Yuhong Cao and Florian Shkurti and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iVbCWUDyBF}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/93a6add7f5a655ea823061cbfbb0aa8ef8acddcb.mp4"
                    },
                    "paperhash": {
                        "value": "tan|searchtta_a_multimodal_testtime_adaptation_framework_for_visual_search_in_the_wild"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission536/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission536/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission536/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745397067406,
                "pdate": 1754680624570,
                "odate": 1758062773303,
                "mdate": 1758062817782,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission536/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission536/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "23FdMTxEh7",
        "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning",
        "abstract": "In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme.",
        "keywords": [
            "Multi-Agent",
            "Reinforcement Learning",
            "Self-play",
            "Drone Volleyball"
        ],
        "pdf_url": "https://openreview.net/pdf/aff8994bb8b74e2b7e0fe4ccb8ec13217cad5629.pdf",
        "reviews": [
            {
                "id": "DFygmRIe0l",
                "forum": "23FdMTxEh7",
                "replyto": "23FdMTxEh7",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission523/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068288627,
                "mdate": 1754869469689,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "23FdMTxEh7",
                "forum": "23FdMTxEh7",
                "content": {
                    "title": {
                        "value": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Ruize Zhang",
                            "Sirui Xiang",
                            "Zelai Xu",
                            "Feng Gao",
                            "Shilong Ji",
                            "Wenhao Tang",
                            "Wenbo Ding",
                            "Chao Yu",
                            "Yu Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ruize_Zhang3",
                            "~Sirui_Xiang1",
                            "~Zelai_Xu1",
                            "~Feng_Gao5",
                            "~Shilong_Ji1",
                            "~Wenhao_Tang2",
                            "~Wenbo_Ding1",
                            "~Chao_Yu1",
                            "~Yu_Wang3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-Agent",
                            "Reinforcement Learning",
                            "Self-play",
                            "Drone Volleyball"
                        ]
                    },
                    "TLDR": {
                        "value": "We tackle the problem of learning to play 3v3 multi-drone volleyball by proposing a novel three-stage hierarchical RL framework that learns both strategic coordination and agile drone control."
                    },
                    "abstract": {
                        "value": "In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/aff8994bb8b74e2b7e0fe4ccb8ec13217cad5629.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025mastering,\ntitle={Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning},\nauthor={Ruize Zhang and Sirui Xiang and Zelai Xu and Feng Gao and Shilong Ji and Wenhao Tang and Wenbo Ding and Chao Yu and Yu Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=23FdMTxEh7}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5e0e65d9734b7b8c67a12cbd398a329bcaa30252.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|mastering_multidrone_volleyball_through_hierarchical_coselfplay_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission523/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission523/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission523/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745392139385,
                "pdate": 1754680624171,
                "odate": 1758062772986,
                "mdate": 1758062817673,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission523/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zxnUgr88rd",
        "title": "Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics",
        "abstract": "Simulation has been pivotal in recent robotics milestones and is poised to play a prominent role in the field's future.\nHowever, recent robotic advances often rely on expensive and high-maintenance platforms, limiting access to broader robotics audiences. This work introduces Wheeled Lab, a framework for integrating the low-cost, open-source wheeled platforms that are already widely established in education and research with Isaac Lab, an open-source, widely adopted, and rapidly growing simulation framework for robotics research. Wheeled Lab thus introduces to new user communities modern techniques in Sim2Real, such as domain randomization, sensor simulation, and end-to-end learning. To kickstart educational uses, we demonstrate three state-of-the-art policies for small-scale RC cars: controlled drifting, elevation traversal, and visual navigation, each trained and deployed through zero-shot reinforcement learning. By bridging the gap between advanced Sim2Real methods and affordable, available robotics, Wheeled Lab aims to democratize access to cutting-edge tools, fostering innovation and education in a broader robotics context. The full stack, from hardware to software, is low cost and open-source.",
        "keywords": [
            "Sim2Real",
            "Mobile Robots",
            "Education"
        ],
        "pdf_url": "https://openreview.net/pdf/7144975c62be370aad8a918ae407ac2571e8a8ab.pdf",
        "reviews": [
            {
                "id": "rQ2S3Z80ar",
                "forum": "zxnUgr88rd",
                "replyto": "zxnUgr88rd",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission522/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068288419,
                "mdate": 1754869469476,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zxnUgr88rd",
                "forum": "zxnUgr88rd",
                "content": {
                    "title": {
                        "value": "Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics"
                    },
                    "authors": {
                        "value": [
                            "Tyler Han",
                            "Preet Shah",
                            "Sidharth Rajagopal",
                            "Yanda Bao",
                            "Sanghun Jung",
                            "Sidharth Talia",
                            "Gabriel Guo",
                            "Bryan Xu",
                            "Bhaumik Mehta",
                            "Emma Romig",
                            "Rosario Scalise",
                            "Byron Boots"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tyler_Han1",
                            "~Preet_Shah1",
                            "~Sidharth_Rajagopal1",
                            "~Yanda_Bao1",
                            "~Sanghun_Jung1",
                            "~Sidharth_Talia1",
                            "~Gabriel_Guo3",
                            "~Bryan_Xu1",
                            "~Bhaumik_Mehta1",
                            "~Emma_Romig1",
                            "~Rosario_Scalise1",
                            "~Byron_Boots1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Sim2Real",
                            "Mobile Robots",
                            "Education"
                        ]
                    },
                    "TLDR": {
                        "value": "Wheeled Lab is a framework which integrates accessible wheeled robots with state-of-the-art robotics simulation to help introduce modern techniques in Sim2Real for education and research."
                    },
                    "abstract": {
                        "value": "Simulation has been pivotal in recent robotics milestones and is poised to play a prominent role in the field's future.\nHowever, recent robotic advances often rely on expensive and high-maintenance platforms, limiting access to broader robotics audiences. This work introduces Wheeled Lab, a framework for integrating the low-cost, open-source wheeled platforms that are already widely established in education and research with Isaac Lab, an open-source, widely adopted, and rapidly growing simulation framework for robotics research. Wheeled Lab thus introduces to new user communities modern techniques in Sim2Real, such as domain randomization, sensor simulation, and end-to-end learning. To kickstart educational uses, we demonstrate three state-of-the-art policies for small-scale RC cars: controlled drifting, elevation traversal, and visual navigation, each trained and deployed through zero-shot reinforcement learning. By bridging the gap between advanced Sim2Real methods and affordable, available robotics, Wheeled Lab aims to democratize access to cutting-edge tools, fostering innovation and education in a broader robotics context. The full stack, from hardware to software, is low cost and open-source."
                    },
                    "supplementary_material": {
                        "value": "/attachment/bcde78bbded3bd8b5dd5d924d095f92d018f766b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7144975c62be370aad8a918ae407ac2571e8a8ab.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhan2025wheeled,\ntitle={Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics},\nauthor={Tyler Han and Preet Shah and Sidharth Rajagopal and Yanda Bao and Sanghun Jung and Sidharth Talia and Gabriel Guo and Bryan Xu and Bhaumik Mehta and Emma Romig and Rosario Scalise and Byron Boots},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zxnUgr88rd}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d580455af1d1a0e9a00ab1644e3b90eadfc4a021.mp4"
                    },
                    "paperhash": {
                        "value": "han|wheeled_lab_modern_sim2real_for_lowcost_opensource_wheeled_robotics"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission522/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission522/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission522/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745392137855,
                "pdate": 1754680624112,
                "odate": 1758062772944,
                "mdate": 1758062817611,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission522/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission522/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "RFmezNsPWV",
        "title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
        "abstract": "Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks.  We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like OpenVLA and $\\pi_{0}$.",
        "keywords": [
            "Vision-Language-Action Models",
            "Robotics Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/c167776dabc0e29615153bbbc61d8d3d57e428e5.pdf",
        "reviews": [
            {
                "id": "OpSIW8wBFI",
                "forum": "RFmezNsPWV",
                "replyto": "RFmezNsPWV",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission517/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068288350,
                "mdate": 1754869469474,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "RFmezNsPWV",
                "forum": "RFmezNsPWV",
                "content": {
                    "title": {
                        "value": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control"
                    },
                    "authors": {
                        "value": [
                            "Junjie Wen",
                            "Yichen Zhu",
                            "Jinming Li",
                            "Zhibin Tang",
                            "Chaomin Shen",
                            "Feifei Feng"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Junjie_Wen2",
                            "~Yichen_Zhu1",
                            "~Jinming_Li1",
                            "~Zhibin_Tang1",
                            "~Chaomin_Shen1",
                            "~Feifei_Feng1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action Models",
                            "Robotics Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks.  We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like OpenVLA and $\\pi_{0}$."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ce918a6c0ab8711f3a85b73a59b38f2510125f4b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c167776dabc0e29615153bbbc61d8d3d57e428e5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwen2025dexvla,\ntitle={Dex{VLA}: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control},\nauthor={Junjie Wen and Yichen Zhu and Jinming Li and Zhibin Tang and Chaomin Shen and Feifei Feng},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RFmezNsPWV}\n}"
                    },
                    "paperhash": {
                        "value": "wen|dexvla_visionlanguage_model_with_plugin_diffusion_expert_for_general_robot_control"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission517/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission517/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission517/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745391278795,
                "pdate": 1754680624046,
                "odate": 1758062772839,
                "mdate": 1758062817525,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission517/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "CNPCSuwxJw",
        "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
        "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region. Replicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge. In this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries. We demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces. We empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots. Our results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation.",
        "keywords": [
            "tactile sensing",
            "contact-rich manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/c5a8c7095f0ee5b63242614cfc6e13993ab5c7d8.pdf",
        "reviews": [
            {
                "id": "CV5JwcQL6V",
                "forum": "CNPCSuwxJw",
                "replyto": "CNPCSuwxJw",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission510/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068287466,
                "mdate": 1754869457043,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "CNPCSuwxJw",
                "forum": "CNPCSuwxJw",
                "content": {
                    "title": {
                        "value": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Suzannah Wistreich",
                            "Baiyu Shi",
                            "Stephen Tian",
                            "Samuel Clarke",
                            "Michael Nath",
                            "Chengyi Xu",
                            "Zhenan Bao",
                            "Jiajun Wu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Suzannah_Wistreich1",
                            "~Baiyu_Shi1",
                            "~Stephen_Tian1",
                            "~Samuel_Clarke1",
                            "~Michael_Nath1",
                            "~Chengyi_Xu1",
                            "~Zhenan_Bao1",
                            "~Jiajun_Wu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "tactile sensing",
                            "contact-rich manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region. Replicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge. In this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries. We demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces. We empirically evaluate DexSkin’s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots. Our results highlight DexSkin’s suitability and practicality for learning real-world, contact-rich manipulation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b263baaa2cf6c2e1a0a3a3f2a8c5714e7e620917.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c5a8c7095f0ee5b63242614cfc6e13993ab5c7d8.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwistreich2025dexskin,\ntitle={DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation},\nauthor={Suzannah Wistreich and Baiyu Shi and Stephen Tian and Samuel Clarke and Michael Nath and Chengyi Xu and Zhenan Bao and Jiajun Wu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CNPCSuwxJw}\n}"
                    },
                    "paperhash": {
                        "value": "wistreich|dexskin_highcoverage_conformable_robotic_skin_for_learning_contactrich_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission510/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission510/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission510/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745388423578,
                "pdate": 1754680623707,
                "odate": 1758062772552,
                "mdate": 1758062817401,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission510/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission510/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "GUgIpVTC1T",
        "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting",
        "abstract": "Safe operation is essential for autonomous systems in safety-critical environments such as urban air mobility. \nValue function-based safety filters provide formal guarantees on safety, wrapping learned or planning-based controllers with a layer of protection. \nRecent approaches leverage offline learned value functions to scale these safety filters to high-dimensional systems. \nYet these methods assume detailed prior knowledge of all possible sources of model mismatch, in the form of disturbances, in the environment -- information that is typically unavailable in real world settings. \nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors. \nWe introduce Space2Time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances. \nThe key idea is to reparameterize spatial disturbances as a time-varying formulation, allowing the use of temporally varying precomputed value functions during online operation. \nWe validate Space2Time through extensive simulations on diverse quadcopter models and real-world hardware experiments, demonstrating significantly improved safety performance over worst-case and naive baselines.",
        "keywords": [
            "Disturbance-aware Safety",
            "Reachability Analysis"
        ],
        "pdf_url": "https://openreview.net/pdf/635de90c904090c6ad80fa6e555f04ee40bec5ae.pdf",
        "reviews": [
            {
                "id": "QtsVOdY9Jq",
                "forum": "GUgIpVTC1T",
                "replyto": "GUgIpVTC1T",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission505/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068287037,
                "mdate": 1754869469384,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "GUgIpVTC1T",
                "forum": "GUgIpVTC1T",
                "content": {
                    "title": {
                        "value": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting"
                    },
                    "authors": {
                        "value": [
                            "Sander Tonkens",
                            "Nikhil Uday Shinde",
                            "Azra Begzadić",
                            "Michael C. Yip",
                            "Jorge Cortes",
                            "Sylvia Lee Herbert"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sander_Tonkens1",
                            "~Nikhil_Uday_Shinde1",
                            "abegzadic@ucsd.edu",
                            "~Michael_C._Yip1",
                            "~Jorge_Cortes1",
                            "~Sylvia_Lee_Herbert1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Disturbance-aware Safety",
                            "Reachability Analysis"
                        ]
                    },
                    "abstract": {
                        "value": "Safe operation is essential for autonomous systems in safety-critical environments such as urban air mobility. \nValue function-based safety filters provide formal guarantees on safety, wrapping learned or planning-based controllers with a layer of protection. \nRecent approaches leverage offline learned value functions to scale these safety filters to high-dimensional systems. \nYet these methods assume detailed prior knowledge of all possible sources of model mismatch, in the form of disturbances, in the environment -- information that is typically unavailable in real world settings. \nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors. \nWe introduce Space2Time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances. \nThe key idea is to reparameterize spatial disturbances as a time-varying formulation, allowing the use of temporally varying precomputed value functions during online operation. \nWe validate Space2Time through extensive simulations on diverse quadcopter models and real-world hardware experiments, demonstrating significantly improved safety performance over worst-case and naive baselines."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/635de90c904090c6ad80fa6e555f04ee40bec5ae.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntonkens2025from,\ntitle={From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting},\nauthor={Sander Tonkens and Nikhil Uday Shinde and Azra Begzadi{\\'c} and Michael C. Yip and Jorge Cortes and Sylvia Lee Herbert},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GUgIpVTC1T}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b109a5383d425002ea953478d455ba2a46b63dcb.mp4"
                    },
                    "paperhash": {
                        "value": "tonkens|from_space_to_time_enabling_adaptive_safety_with_learned_value_functions_via_disturbance_recasting"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission505/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission505/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission505/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745386424874,
                "pdate": 1754680623405,
                "odate": 1758062772309,
                "mdate": 1758062817371,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission505/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "LnryWopsfJ",
        "title": "Mobi-$\\pi$: Mobilizing Your Robot Learning Policy",
        "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the \"policy mobilization\" problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. With that, our formulation is still compatible with any approach that improves manipulation policy robustness. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes a 3D Gaussian Splatting model for novel viewpoint synthesis, a score function to evaluate pose suitability, as well as sampling-based optimization to identify optimal robot poses. We show that our approach on average outperforms the best baseline by 7.65$\\times$ in simulation and 2.38$\\times$ in the real world, demonstrating its effectiveness for policy mobilization.",
        "keywords": [
            "Policy Mobilization",
            "Mobile Manipulation",
            "Robot Learning",
            "Robot Perception"
        ],
        "pdf_url": "https://openreview.net/pdf/cdcbb8dcac8d65f19b93a77f8e3209941b4406a5.pdf",
        "reviews": [
            {
                "id": "J0ZeN9Fat8",
                "forum": "LnryWopsfJ",
                "replyto": "LnryWopsfJ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission501/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068286867,
                "mdate": 1754869468951,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "LnryWopsfJ",
                "forum": "LnryWopsfJ",
                "content": {
                    "title": {
                        "value": "Mobi-$\\pi$: Mobilizing Your Robot Learning Policy"
                    },
                    "authors": {
                        "value": [
                            "Jingyun Yang",
                            "Isabella Huang",
                            "Brandon Vu",
                            "Max Bajracharya",
                            "Rika Antonova",
                            "Jeannette Bohg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jingyun_Yang1",
                            "~Isabella_Huang2",
                            "~Brandon_Vu1",
                            "max.bajracharya@tri.global",
                            "~Rika_Antonova1",
                            "~Jeannette_Bohg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Policy Mobilization",
                            "Mobile Manipulation",
                            "Robot Learning",
                            "Robot Perception"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose novel metrics, tasks, visualization tools and methods for \"policy mobilization\", the problem of taking a non-mobile manipulation policy and finding a proper initial robot pose from which to execute it on a mobile platform."
                    },
                    "abstract": {
                        "value": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the \"policy mobilization\" problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. With that, our formulation is still compatible with any approach that improves manipulation policy robustness. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes a 3D Gaussian Splatting model for novel viewpoint synthesis, a score function to evaluate pose suitability, as well as sampling-based optimization to identify optimal robot poses. We show that our approach on average outperforms the best baseline by 7.65$\\times$ in simulation and 2.38$\\times$ in the real world, demonstrating its effectiveness for policy mobilization."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/cdcbb8dcac8d65f19b93a77f8e3209941b4406a5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyang2025mobipi,\ntitle={Mobi-\\${\\textbackslash}pi\\$: Mobilizing Your Robot Learning Policy},\nauthor={Jingyun Yang and Isabella Huang and Brandon Vu and Max Bajracharya and Rika Antonova and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LnryWopsfJ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/37a0a82ef75480aee051f68d2cd42435dc435570.mp4"
                    },
                    "paperhash": {
                        "value": "yang|mobi\\pi_mobilizing_your_robot_learning_policy"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission501/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission501/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission501/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745385010738,
                "pdate": 1754680623320,
                "odate": 1758062772210,
                "mdate": 1758062817261,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission501/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission501/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "VmCkEvRULX",
        "title": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation",
        "abstract": "Robotic manipulation in unstructured environments requires systems that can generalize across diverse tasks while maintaining robust and reliable performance. We introduce GVF-TAPE, a closed-loop framework that combines generative visual foresight with task-agnostic pose estimation to enable scalable robotic manipulation. GVF-TAPE employs a generative video model to predict future RGB-D frames from a single RGB side-view image and a task description, offering visual plans that guide robot actions. A decoupled pose estimation model then extracts end-effector poses from the predicted frames, translating them into executable commands via low-level controllers. By iteratively integrating video foresight and pose estimation in a closed loop, GVF-TAPE achieves real-time, adaptive manipulation across a broad range of tasks. Extensive experiments in both simulation and real-world settings demonstrate that our approach reduces reliance on task-specific action data and generalizes effectively, providing a practical and scalable solution for intelligent robotic systems",
        "keywords": [
            "robotic manipulation",
            "action-label-free learning",
            "generative visual foresight"
        ],
        "pdf_url": "https://openreview.net/pdf/76d1fb2fc9b6001a9471382c906f9ff025fe1630.pdf",
        "reviews": [
            {
                "id": "frBS0vT6CB",
                "forum": "VmCkEvRULX",
                "replyto": "VmCkEvRULX",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission498/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068286637,
                "mdate": 1754869468930,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "VmCkEvRULX",
                "forum": "VmCkEvRULX",
                "content": {
                    "title": {
                        "value": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Chuye Zhang",
                            "Xiaoxiong Zhang",
                            "Linfang Zheng",
                            "Wei Pan",
                            "Wei Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chuye_Zhang1",
                            "~Xiaoxiong_Zhang1",
                            "~Linfang_Zheng1",
                            "~Wei_Pan12",
                            "~Wei_Zhang40"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "robotic manipulation",
                            "action-label-free learning",
                            "generative visual foresight"
                        ]
                    },
                    "TLDR": {
                        "value": "we propose a novel closed-loop framework that integrates generative visual foresight with task-agnostic pose estimation"
                    },
                    "abstract": {
                        "value": "Robotic manipulation in unstructured environments requires systems that can generalize across diverse tasks while maintaining robust and reliable performance. We introduce GVF-TAPE, a closed-loop framework that combines generative visual foresight with task-agnostic pose estimation to enable scalable robotic manipulation. GVF-TAPE employs a generative video model to predict future RGB-D frames from a single RGB side-view image and a task description, offering visual plans that guide robot actions. A decoupled pose estimation model then extracts end-effector poses from the predicted frames, translating them into executable commands via low-level controllers. By iteratively integrating video foresight and pose estimation in a closed loop, GVF-TAPE achieves real-time, adaptive manipulation across a broad range of tasks. Extensive experiments in both simulation and real-world settings demonstrate that our approach reduces reliance on task-specific action data and generalizes effectively, providing a practical and scalable solution for intelligent robotic systems"
                    },
                    "supplementary_material": {
                        "value": "/attachment/1d03cec94bbf1a9c877c867f20444494d9c648e0.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/76d1fb2fc9b6001a9471382c906f9ff025fe1630.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025generative,\ntitle={Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation},\nauthor={Chuye Zhang and Xiaoxiong Zhang and Linfang Zheng and Wei Pan and Wei Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VmCkEvRULX}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2a6ddd6d19bae93951d0d78a3316c2f1689c8327.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|generative_visual_foresight_meets_taskagnostic_pose_estimation_in_robotic_tabletop_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission498/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission498/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission498/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745383469142,
                "pdate": 1754680623113,
                "odate": 1758062772101,
                "mdate": 1758062817110,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission498/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission498/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "oDUbsdc0Ru",
        "title": "Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation",
        "abstract": "Cloth manipulation is challenging due to its highly complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate both state estimation and dynamics modeling. Inspired by recent advances in generative models, we hypothesize that these expressive models can effectively capture intricate cloth configurations and deformation patterns from data. Therefore, we propose a diffusion-based generative approach for both perception and dynamics modeling. Specifically, we formulate state estimation as reconstructing full cloth states from partial observations and dynamics modeling as predicting future states given the current state and robot actions. Leveraging a transformer-based diffusion model, our method achieves accurate state reconstruction and reduces long-horizon dynamics prediction errors by an order of magnitude compared to prior approaches. We integrate our dynamics models with model-predictive control and show that our framework enables effective cloth folding on real robotic systems, demonstrating the potential of generative models for deformable object manipulation under partial observability and complex dynamics.",
        "keywords": [
            "Model Learning",
            "Deformable Object Manipulation",
            "Perception"
        ],
        "pdf_url": "https://openreview.net/pdf/4e80ab8d3d59de4fe83a308813c104264e7882cd.pdf",
        "reviews": [
            {
                "id": "mfcDRL5hAF",
                "forum": "oDUbsdc0Ru",
                "replyto": "oDUbsdc0Ru",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission497/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068286576,
                "mdate": 1754869468782,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "oDUbsdc0Ru",
                "forum": "oDUbsdc0Ru",
                "content": {
                    "title": {
                        "value": "Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Tongxuan Tian",
                            "Haoyang Li",
                            "Bo Ai",
                            "Xiaodi Yuan",
                            "Zhiao Huang",
                            "Hao Su"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tongxuan_Tian1",
                            "~Haoyang_Li6",
                            "~Bo_Ai1",
                            "~Xiaodi_Yuan1",
                            "~Zhiao_Huang1",
                            "~Hao_Su1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Model Learning",
                            "Deformable Object Manipulation",
                            "Perception"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a novel approach for estimating and learning cloth state and dynamics with diffusion models for cloth manipulation."
                    },
                    "abstract": {
                        "value": "Cloth manipulation is challenging due to its highly complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate both state estimation and dynamics modeling. Inspired by recent advances in generative models, we hypothesize that these expressive models can effectively capture intricate cloth configurations and deformation patterns from data. Therefore, we propose a diffusion-based generative approach for both perception and dynamics modeling. Specifically, we formulate state estimation as reconstructing full cloth states from partial observations and dynamics modeling as predicting future states given the current state and robot actions. Leveraging a transformer-based diffusion model, our method achieves accurate state reconstruction and reduces long-horizon dynamics prediction errors by an order of magnitude compared to prior approaches. We integrate our dynamics models with model-predictive control and show that our framework enables effective cloth folding on real robotic systems, demonstrating the potential of generative models for deformable object manipulation under partial observability and complex dynamics."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1470b67a19a8bbee2426e8e8c8df120c1e2a9bf5.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4e80ab8d3d59de4fe83a308813c104264e7882cd.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntian2025diffusion,\ntitle={Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation},\nauthor={Tongxuan Tian and Haoyang Li and Bo Ai and Xiaodi Yuan and Zhiao Huang and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oDUbsdc0Ru}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/790b83cbda336415c913674d0a69392dda77e605.mp4"
                    },
                    "paperhash": {
                        "value": "tian|diffusion_dynamics_models_with_generative_state_estimation_for_cloth_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission497/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission497/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission497/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745383208781,
                "pdate": 1754680623066,
                "odate": 1758062771988,
                "mdate": 1758062817043,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission497/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "P0uqo7CpL8",
        "title": "Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots",
        "abstract": "In complex environments, adaptive and fault-tolerant capabilities are essential for legged robot locomotion. To address this challenge, this study proposes a reinforcement learning framework that integrates contrastive learning with forward prediction to achieve fault-tolerant locomotion for legged robots. This framework constructs a forward prediction model with contrastive learning, incorporating a comparator and a forward model. The forward model predicts the robot's subsequent state, and the comparator compares these predictions with actual states to generate critical prediction errors. These errors are systematically integrated into the controller, facilitating the continuous adjustment and refinement of control signals.Experiments on quadruped robots across different terrains and various joint damage scenarios have verified the effectiveness of our method, especially the functions of the comparator and the forward model. Furthermore, robots can adapt to locked joints without prior training, demonstrating zero-shot transfer capability. Finally, the proposed method demonstrates universal applicability to both quadruped and hexapod robots, highlighting its potential for broader applications in legged robotics.",
        "keywords": [
            "Legged robot locomotion",
            "Fault tolerance control",
            "Deep Reinforcement learning"
        ],
        "pdf_url": "https://openreview.net/pdf/58960442a8bb8a9b35ae33388c72ee5ff9171f76.pdf",
        "reviews": [
            {
                "id": "1vlviUoZPK",
                "forum": "P0uqo7CpL8",
                "replyto": "P0uqo7CpL8",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission495/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068286518,
                "mdate": 1754869468628,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "P0uqo7CpL8",
                "forum": "P0uqo7CpL8",
                "content": {
                    "title": {
                        "value": "Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots"
                    },
                    "authors": {
                        "value": [
                            "Yangqing Fu",
                            "Yang Zhang",
                            "Qiyue Yang",
                            "Liyun Yan",
                            "Zhanxiang Cao",
                            "Yue Gao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yangqing_Fu1",
                            "zhangyang-sjtu-2022@sjtu.edu.cn",
                            "yangqiyue@sjtu.edu.cn",
                            "ylyem9x@sjtu.edu.cn",
                            "caozx1110@sjtu.edu.cn",
                            "~Yue_Gao8"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Legged robot locomotion",
                            "Fault tolerance control",
                            "Deep Reinforcement learning"
                        ]
                    },
                    "TLDR": {
                        "value": "This work proposes a reinforcement learning framework that enhances fault-tolerant control in legged robots by integrating forward prediction and error-based feedback, enabling robust and adaptive locomotion under joint damage."
                    },
                    "abstract": {
                        "value": "In complex environments, adaptive and fault-tolerant capabilities are essential for legged robot locomotion. To address this challenge, this study proposes a reinforcement learning framework that integrates contrastive learning with forward prediction to achieve fault-tolerant locomotion for legged robots. This framework constructs a forward prediction model with contrastive learning, incorporating a comparator and a forward model. The forward model predicts the robot's subsequent state, and the comparator compares these predictions with actual states to generate critical prediction errors. These errors are systematically integrated into the controller, facilitating the continuous adjustment and refinement of control signals.Experiments on quadruped robots across different terrains and various joint damage scenarios have verified the effectiveness of our method, especially the functions of the comparator and the forward model. Furthermore, robots can adapt to locked joints without prior training, demonstrating zero-shot transfer capability. Finally, the proposed method demonstrates universal applicability to both quadruped and hexapod robots, highlighting its potential for broader applications in legged robotics."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f72d839da5387ae2986d9101e308adf3a3732ae1.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/58960442a8bb8a9b35ae33388c72ee5ff9171f76.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfu2025contrastive,\ntitle={Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots},\nauthor={Yangqing Fu and Yang Zhang and Qiyue Yang and Liyun Yan and Zhanxiang Cao and Yue Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=P0uqo7CpL8}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/3190f9f7d7e23dafd466a8bed6c5a3f879b381de.mp4"
                    },
                    "paperhash": {
                        "value": "fu|contrastive_forward_prediction_reinforcement_learning_for_adaptive_faulttolerant_legged_robots"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission495/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission495/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745382804447,
                "pdate": 1754680623044,
                "odate": 1758062771914,
                "mdate": 1758062816965,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission495/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "DzRNBBCP4R",
        "title": "Action-Free Reasoning for Policy Generalization",
        "abstract": "End-to-end imitation learning offers a promising approach for training robot policies. However, generalizing to new settings—such as unseen scenes, tasks, and object instances—remains a significant challenge. Although large-scale robot demonstration datasets have shown potential for inducing generalization, they are resource-intensive to scale. In contrast, human video data is abundant and diverse, presenting an attractive alternative. Yet, these human-video datasets lack action labels, complicating their use in imitation learning. Existing methods attempt to extract grounded action representations (e.g., hand poses), but resulting policies struggle to bridge the embodiment gap between human and robot actions. We propose an alternative approach: leveraging language-based reasoning from human videos—essential for guiding robot actions—to train generalizable robot policies. Building on recent advances in reasoning-based policy architectures, we introduce Reasoning through Action-free Data (RAD). RAD learns from both robot demonstration data (with reasoning and action labels) and action-free human video data (with only reasoning labels). The robot data teaches the model to map reasoning to low-level actions, while the action-free data enhances reasoning capabilities. Additionally, we release a new dataset of 3,377 human-hand demonstrations compatible with the Bridge V2 benchmark. This dataset includes chain-of-thought reasoning annotations and hand-tracking data, and is aimed at facilitating future research on reasoning-driven robot learning. Our experiments demonstrate that RAD enables effective transfer across the embodiment gap, allowing robots to perform tasks seen only in action-free data. Furthermore, scaling up action-free reasoning data significantly improves policy performance and generalization to novel tasks. These results highlight the promise of reasoning-driven learning from action-free datasets for advancing generalizable robot control. See website with videos: https://rad-generalization-s.github.io/",
        "keywords": [
            "Foundation Models",
            "Robot Learning",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/9bfed659962586ae5513d09950d4498394185196.pdf",
        "reviews": [
            {
                "id": "fd2qDWJdAP",
                "forum": "DzRNBBCP4R",
                "replyto": "DzRNBBCP4R",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission493/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068286329,
                "mdate": 1754869468569,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "DzRNBBCP4R",
                "forum": "DzRNBBCP4R",
                "content": {
                    "title": {
                        "value": "Action-Free Reasoning for Policy Generalization"
                    },
                    "authors": {
                        "value": [
                            "Jaden Clark",
                            "Suvir Mirchandani",
                            "Dorsa Sadigh",
                            "Suneel Belkhale"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jaden_Clark1",
                            "~Suvir_Mirchandani1",
                            "~Dorsa_Sadigh1",
                            "~Suneel_Belkhale1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Foundation Models",
                            "Robot Learning",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a new way to train VLAs on human video data by learning from chain-of-thought reasoning, and show that this enables models to learn new tasks and improve their generalization performance."
                    },
                    "abstract": {
                        "value": "End-to-end imitation learning offers a promising approach for training robot policies. However, generalizing to new settings—such as unseen scenes, tasks, and object instances—remains a significant challenge. Although large-scale robot demonstration datasets have shown potential for inducing generalization, they are resource-intensive to scale. In contrast, human video data is abundant and diverse, presenting an attractive alternative. Yet, these human-video datasets lack action labels, complicating their use in imitation learning. Existing methods attempt to extract grounded action representations (e.g., hand poses), but resulting policies struggle to bridge the embodiment gap between human and robot actions. We propose an alternative approach: leveraging language-based reasoning from human videos—essential for guiding robot actions—to train generalizable robot policies. Building on recent advances in reasoning-based policy architectures, we introduce Reasoning through Action-free Data (RAD). RAD learns from both robot demonstration data (with reasoning and action labels) and action-free human video data (with only reasoning labels). The robot data teaches the model to map reasoning to low-level actions, while the action-free data enhances reasoning capabilities. Additionally, we release a new dataset of 3,377 human-hand demonstrations compatible with the Bridge V2 benchmark. This dataset includes chain-of-thought reasoning annotations and hand-tracking data, and is aimed at facilitating future research on reasoning-driven robot learning. Our experiments demonstrate that RAD enables effective transfer across the embodiment gap, allowing robots to perform tasks seen only in action-free data. Furthermore, scaling up action-free reasoning data significantly improves policy performance and generalization to novel tasks. These results highlight the promise of reasoning-driven learning from action-free datasets for advancing generalizable robot control. See website with videos: https://rad-generalization-s.github.io/"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9bfed659962586ae5513d09950d4498394185196.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nclark2025actionfree,\ntitle={Action-Free Reasoning for Policy Generalization},\nauthor={Jaden Clark and Suvir Mirchandani and Dorsa Sadigh and Suneel Belkhale},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DzRNBBCP4R}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/29800258fa184de86e813785aa295b83f8831724.mp4"
                    },
                    "paperhash": {
                        "value": "clark|actionfree_reasoning_for_policy_generalization"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission493/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission493/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745381765567,
                "pdate": 1754680622885,
                "odate": 1758062771802,
                "mdate": 1758062816843,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission493/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "VrNSv02Xfu",
        "title": "SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition",
        "abstract": "Robot-assisted feeding requires reliable bite acquisition, a challenging task due to the complex interactions between utensils and food with diverse physical properties. These interactions are further complicated by the temporal variability of food properties—for example, steak becomes firm as it cools even during a meal. To address this, we propose SAVOR, a novel approach for learning skill affordances for bite acquisition—how suitable a manipulation skill (e.g., skewering, scooping) is for a given utensil-food interaction. In our formulation, skill affordances arise from the combination of tool affordances (what a utensil can do) and food affordances (what the food allows). Tool affordances are learned offline through calibration, where different utensils interact with a variety of foods to model their functional capabilities. Food affordances are characterized by physical properties such as softness, moisture, and viscosity, initially inferred through commonsense reasoning using a visually-conditioned language model and then dynamically refined through online multi-modal visuo-haptic perception using SAVOR-Net during interaction. Our method integrates these offline and online estimates to predict skill affordances in real time, enabling the robot to select the most appropriate skill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild meals, our approach improves bite acquisition success by 13\\% over state-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits). These results highlight the importance of modeling interaction-driven skill affordances for generalizable and effective robot-assisted bite acquisition.",
        "keywords": [
            "Assistive Robotics",
            "Visuo-Haptic Perception",
            "Affordance Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/55b8628b7ad50e1fc455b8e9b260b4b64c04a1c7.pdf",
        "reviews": [
            {
                "id": "a3xhOqjVxr",
                "forum": "VrNSv02Xfu",
                "replyto": "VrNSv02Xfu",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission484/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068285757,
                "mdate": 1754869456978,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "VrNSv02Xfu",
                "forum": "VrNSv02Xfu",
                "content": {
                    "title": {
                        "value": "SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition"
                    },
                    "authors": {
                        "value": [
                            "Zhanxin Wu",
                            "Bo Ai",
                            "Tom Silver",
                            "Tapomayukh Bhattacharjee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhanxin_Wu1",
                            "~Bo_Ai1",
                            "~Tom_Silver1",
                            "~Tapomayukh_Bhattacharjee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Assistive Robotics",
                            "Visuo-Haptic Perception",
                            "Affordance Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Robot-assisted feeding requires reliable bite acquisition, a challenging task due to the complex interactions between utensils and food with diverse physical properties. These interactions are further complicated by the temporal variability of food properties—for example, steak becomes firm as it cools even during a meal. To address this, we propose SAVOR, a novel approach for learning skill affordances for bite acquisition—how suitable a manipulation skill (e.g., skewering, scooping) is for a given utensil-food interaction. In our formulation, skill affordances arise from the combination of tool affordances (what a utensil can do) and food affordances (what the food allows). Tool affordances are learned offline through calibration, where different utensils interact with a variety of foods to model their functional capabilities. Food affordances are characterized by physical properties such as softness, moisture, and viscosity, initially inferred through commonsense reasoning using a visually-conditioned language model and then dynamically refined through online multi-modal visuo-haptic perception using SAVOR-Net during interaction. Our method integrates these offline and online estimates to predict skill affordances in real time, enabling the robot to select the most appropriate skill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild meals, our approach improves bite acquisition success by 13\\% over state-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits). These results highlight the importance of modeling interaction-driven skill affordances for generalizable and effective robot-assisted bite acquisition."
                    },
                    "supplementary_material": {
                        "value": "/attachment/18238e74d9e975cc86eaade7598fd624d02610c6.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/55b8628b7ad50e1fc455b8e9b260b4b64c04a1c7.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwu2025savor,\ntitle={{SAVOR}: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition},\nauthor={Zhanxin Wu and Bo Ai and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VrNSv02Xfu}\n}"
                    },
                    "paperhash": {
                        "value": "wu|savor_skill_affordance_learning_from_visuohaptic_perception_for_robotassisted_bite_acquisition"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission484/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission484/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission484/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745380509868,
                "pdate": 1754680622581,
                "odate": 1758062771499,
                "mdate": 1758062816795,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission484/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission484/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "EGSSHukI05",
        "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
        "abstract": "Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. \n     However, existing IL-trained policies are confined to execute the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. \n      Experiments on 12 tasks across simulation and two real, distinct robot platforms shows that SAIL achieves up to a {4$\\times$ speedup} over demonstration speed in simulation and up to {3.2$\\times$ speedup} in the real world. Additional detail is available at https://sail-robot.github.io",
        "keywords": [
            "Visuomotor Imitation",
            "Robot Learning Systems",
            "Robotic Manipulation",
            "Speed Adaptive Execution"
        ],
        "pdf_url": "https://openreview.net/pdf/b418e37db73a8caedf44d4662bd53e8577749328.pdf",
        "reviews": [
            {
                "id": "SFIHaqGz1H",
                "forum": "EGSSHukI05",
                "replyto": "EGSSHukI05",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission483/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068285644,
                "mdate": 1754869456914,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "EGSSHukI05",
                "forum": "EGSSHukI05",
                "content": {
                    "title": {
                        "value": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies"
                    },
                    "authors": {
                        "value": [
                            "Nadun Ranawaka Arachchige",
                            "Zhenyang Chen",
                            "Wonsuhk Jung",
                            "Woo Chul Shin",
                            "Rohan Bansal",
                            "Pierre Barroso",
                            "Yu Hang He",
                            "Yingyan Celine Lin",
                            "Benjamin Joffe",
                            "Shreyas Kousik",
                            "Danfei Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Nadun_Ranawaka_Arachchige1",
                            "~Zhenyang_Chen1",
                            "~Wonsuhk_Jung1",
                            "~Woo_Chul_Shin1",
                            "~Rohan_Bansal1",
                            "~Pierre_Barroso1",
                            "~Yu_Hang_He1",
                            "~Yingyan_Celine_Lin1",
                            "~Benjamin_Joffe1",
                            "~Shreyas_Kousik1",
                            "~Danfei_Xu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Visuomotor Imitation",
                            "Robot Learning Systems",
                            "Robotic Manipulation",
                            "Speed Adaptive Execution"
                        ]
                    },
                    "abstract": {
                        "value": "Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. \n     However, existing IL-trained policies are confined to execute the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. \n      Experiments on 12 tasks across simulation and two real, distinct robot platforms shows that SAIL achieves up to a {4$\\times$ speedup} over demonstration speed in simulation and up to {3.2$\\times$ speedup} in the real world. Additional detail is available at https://sail-robot.github.io"
                    },
                    "supplementary_material": {
                        "value": "/attachment/52fadd6511316bb043174b8b4371bfed74c6dde5.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A system to execute imitation learning policies faster than demonstrations."
                    },
                    "pdf": {
                        "value": "/pdf/b418e37db73a8caedf44d4662bd53e8577749328.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\narachchige2025sail,\ntitle={{SAIL}: Faster-than-Demonstration Execution of Imitation Learning Policies},\nauthor={Nadun Ranawaka Arachchige and Zhenyang Chen and Wonsuhk Jung and Woo Chul Shin and Rohan Bansal and Pierre Barroso and Yu Hang He and Yingyan Celine Lin and Benjamin Joffe and Shreyas Kousik and Danfei Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EGSSHukI05}\n}"
                    },
                    "paperhash": {
                        "value": "arachchige|sail_fasterthandemonstration_execution_of_imitation_learning_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission483/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission483/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission483/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745380141967,
                "pdate": 1754680622513,
                "odate": 1758062771464,
                "mdate": 1758062816722,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission483/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission483/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "iWMM4oxMBu",
        "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online",
        "abstract": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain scenarios online by leveraging past interactions. Robots frequently encounter visually ambiguous objects whose manipulation outcomes remain uncertain until physically interacted with. While generative models alone could theoretically adapt to such ambiguity, in practice they obtain suboptimal performance in ambiguous cases, even when conditioned on action history.  To address this, we propose explicitly decoupling action generation from verification: we use an unconditional diffusion-based generator to propose multiple candidate actions and employ our history-aware verifier to select the most promising action by reasoning about past interactions. Through theoretical analysis, we demonstrate that employing a verifier significantly improves expected action quality. Empirical evaluations and analysis across multiple simulated and real-world environments including articulated objects, multi-modal doors, and uneven object pick-up confirm the effectiveness of our method and improvements over baselines.",
        "keywords": [
            "Ambiguities",
            "Multi-modality",
            "Generation-Verification"
        ],
        "pdf_url": "https://openreview.net/pdf/0d4dfc14075fadf610b2ae3f0efe233ab7d93455.pdf",
        "reviews": [
            {
                "id": "racPd6VdQ5",
                "forum": "iWMM4oxMBu",
                "replyto": "iWMM4oxMBu",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission479/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068285413,
                "mdate": 1754869468485,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "iWMM4oxMBu",
                "forum": "iWMM4oxMBu",
                "content": {
                    "title": {
                        "value": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online"
                    },
                    "authors": {
                        "value": [
                            "Yishu Li",
                            "Xinyi Mao",
                            "Ying Yuan",
                            "Kyutae Sim",
                            "Ben Eisner",
                            "David Held"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yishu_Li1",
                            "~Xinyi_Mao1",
                            "~Ying_Yuan2",
                            "~Kyutae_Sim1",
                            "~Ben_Eisner1",
                            "~David_Held1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Ambiguities",
                            "Multi-modality",
                            "Generation-Verification"
                        ]
                    },
                    "abstract": {
                        "value": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain scenarios online by leveraging past interactions. Robots frequently encounter visually ambiguous objects whose manipulation outcomes remain uncertain until physically interacted with. While generative models alone could theoretically adapt to such ambiguity, in practice they obtain suboptimal performance in ambiguous cases, even when conditioned on action history.  To address this, we propose explicitly decoupling action generation from verification: we use an unconditional diffusion-based generator to propose multiple candidate actions and employ our history-aware verifier to select the most promising action by reasoning about past interactions. Through theoretical analysis, we demonstrate that employing a verifier significantly improves expected action quality. Empirical evaluations and analysis across multiple simulated and real-world environments including articulated objects, multi-modal doors, and uneven object pick-up confirm the effectiveness of our method and improvements over baselines."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0d4dfc14075fadf610b2ae3f0efe233ab7d93455.pdf"
                    },
                    "TLDR": {
                        "value": "This paper introduces HAVE, a history-aware verifier that guides action selection in ambiguous robotic tasks based on past interactions, demonstrating improved performance in multiple simulation and real-world settings."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025learn,\ntitle={Learn from What We {HAVE}: History-Aware {VE}rifier that Reasons about Past Interactions Online},\nauthor={Yishu Li and Xinyi Mao and Ying Yuan and Kyutae Sim and Ben Eisner and David Held},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iWMM4oxMBu}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/cd5e042dabfdd63a3c810491581c7a13703226a2.mp4"
                    },
                    "paperhash": {
                        "value": "li|learn_from_what_we_have_historyaware_verifier_that_reasons_about_past_interactions_online"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission479/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission479/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission479/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745378885275,
                "pdate": 1754680622383,
                "odate": 1758062771306,
                "mdate": 1758062816608,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission479/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission479/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "GKueYvjqSS",
        "title": "KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation",
        "abstract": "Collecting demonstrations enriched with fine-grained tactile information is critical for dexterous manipulation, particularly in contact-rich tasks that require precise force control and physical interaction. While prior works primarily focus on teleoperation or video-based retargeting, they often suffer from kinematic mismatches and the absence of real-time tactile feedback, hindering the acquisition of high-fidelity tactile data. To mitigate this issue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in which the operator’s motion is directly transferred to the dexterous hand, enabling the collection of physically grounded demonstrations enriched with accurate tactile feedback. To resolve occlusions from human hand, we apply inpainting technique to preprocess the visual observations. Based on these demonstrations, we then train a visuomotor policy using tactile-augmented inputs and implement force control during deployment for precise contact-rich manipulation. We evaluate KineDex on a suite of challenging contact-rich manipulation tasks, including particularly difficult scenarios such as squeezing toothpaste onto a toothbrush, which require precise multi-finger coordination and stable force regulation. Across these tasks, KineDex achieves an average success rate of 74.4%, representing a 57.7% improvement over the variant without force control. Comparative experiments with teleoperation and user studies further validate the advantages of KineDex in data collection efficiency and operability. Specifically, KineDex collects data over twice as fast as teleoperation across two tasks of varying difficulty, while maintaining a near-100% success rate, compared to under 50% for teleoperation.",
        "keywords": [
            "Dexterous Manipulation; Kinesthetic Teaching;Tactile Sensing"
        ],
        "pdf_url": "https://openreview.net/pdf/c59076af44565b4b30f93f49702686dd026b3997.pdf",
        "reviews": [
            {
                "id": "P9L6UANNfA",
                "forum": "GKueYvjqSS",
                "replyto": "GKueYvjqSS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission478/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068285116,
                "mdate": 1754869468292,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "GKueYvjqSS",
                "forum": "GKueYvjqSS",
                "content": {
                    "title": {
                        "value": "KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Di Zhang",
                            "Chengbo Yuan",
                            "Chuan Wen",
                            "Hai Zhang",
                            "Junqiao Zhao",
                            "Yang Gao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Di_Zhang5",
                            "~Chengbo_Yuan2",
                            "~Chuan_Wen1",
                            "~Hai_Zhang2",
                            "~Junqiao_Zhao1",
                            "~Yang_Gao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Manipulation; Kinesthetic Teaching;Tactile Sensing"
                        ]
                    },
                    "abstract": {
                        "value": "Collecting demonstrations enriched with fine-grained tactile information is critical for dexterous manipulation, particularly in contact-rich tasks that require precise force control and physical interaction. While prior works primarily focus on teleoperation or video-based retargeting, they often suffer from kinematic mismatches and the absence of real-time tactile feedback, hindering the acquisition of high-fidelity tactile data. To mitigate this issue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in which the operator’s motion is directly transferred to the dexterous hand, enabling the collection of physically grounded demonstrations enriched with accurate tactile feedback. To resolve occlusions from human hand, we apply inpainting technique to preprocess the visual observations. Based on these demonstrations, we then train a visuomotor policy using tactile-augmented inputs and implement force control during deployment for precise contact-rich manipulation. We evaluate KineDex on a suite of challenging contact-rich manipulation tasks, including particularly difficult scenarios such as squeezing toothpaste onto a toothbrush, which require precise multi-finger coordination and stable force regulation. Across these tasks, KineDex achieves an average success rate of 74.4%, representing a 57.7% improvement over the variant without force control. Comparative experiments with teleoperation and user studies further validate the advantages of KineDex in data collection efficiency and operability. Specifically, KineDex collects data over twice as fast as teleoperation across two tasks of varying difficulty, while maintaining a near-100% success rate, compared to under 50% for teleoperation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/6b1e7891c78fd3fbc7b8626fc77fc55438edbe9e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We present KineDex, a framework for collecting tactile-enriched demonstrations via kinesthetic teaching and training tactile-informed visuomotor policies for dexterous manipulation."
                    },
                    "pdf": {
                        "value": "/pdf/c59076af44565b4b30f93f49702686dd026b3997.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025kinedex,\ntitle={KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation},\nauthor={Di Zhang and Chengbo Yuan and Chuan Wen and Hai Zhang and Junqiao Zhao and Yang Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GKueYvjqSS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5de30b9c779b63fd47a9fdb8adeefdc5e68fbfea.zip"
                    },
                    "paperhash": {
                        "value": "zhang|kinedex_learning_tactileinformed_visuomotor_policies_via_kinesthetic_teaching_for_dexterous_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission478/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission478/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745378743853,
                "pdate": 1754680622314,
                "odate": 1758062771260,
                "mdate": 1758062816540,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission478/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ux5EptB7xZ",
        "title": "Geometric Red-Teaming for Robotic Manipulation",
        "abstract": "Standard evaluation protocols in robotic manipulation typically assess policy performance over curated, in-distribution test sets, offering limited insight into how systems fail under plausible variation. \n    We introduce a red-teaming framework that probes robustness through object-centric geometric perturbations, automatically generating CrashShapes---structurally valid, user-constrained mesh deformations that trigger catastrophic failures in pre-trained manipulation policies. \n    The method integrates a Jacobian field–based deformation model with a gradient-free, simulator-in-the-loop optimization strategy.\n    Across insertion, articulation, and grasping tasks, our approach consistently discovers deformations that collapse policy performance, revealing brittle failure modes missed by static benchmarks. \n    By combining task-level policy rollouts with constraint-aware shape exploration, we aim to build a general purpose framework for structured, object-centric robustness evaluation in robotic manipulation.\n    We additionally show that fine-tuning on individual CrashShapes, a process we refer to as blue-teaming, improves task success by up to 60 percentage points on those shapes, while preserving performance on the original object, demonstrating the utility of red-teamed geometries for targeted policy refinement.\n    Finally, we validate both red-teaming and blue-teaming results with a real robotic arm, observing that simulated CrashShapes reduce task success from 90\\% to as low as 22.5\\%, and that blue-teaming recovers performance to up to 90\\% on the corresponding real-world geometry---closely matching simulation outcomes.",
        "keywords": [
            "Red-Teaming",
            "Manipulation",
            "Geometry Perturbation"
        ],
        "pdf_url": "https://openreview.net/pdf/40881d2bfd69d2638d84a0d1987b6e083c508958.pdf",
        "reviews": [
            {
                "id": "DCmzNx8ezY",
                "forum": "ux5EptB7xZ",
                "replyto": "ux5EptB7xZ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission477/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068285188,
                "mdate": 1754869456847,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ux5EptB7xZ",
                "forum": "ux5EptB7xZ",
                "content": {
                    "title": {
                        "value": "Geometric Red-Teaming for Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Divyam Goel",
                            "Yufei Wang",
                            "Tiancheng Wu",
                            "Guixiu Qiao",
                            "Pavel Piliptchak",
                            "David Held",
                            "Zackory Erickson"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Divyam_Goel1",
                            "~Yufei_Wang4",
                            "~Tiancheng_Wu1",
                            "~Guixiu_Qiao1",
                            "pavel.piliptchak@nist.gov",
                            "~David_Held1",
                            "~Zackory_Erickson1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Red-Teaming",
                            "Manipulation",
                            "Geometry Perturbation"
                        ]
                    },
                    "abstract": {
                        "value": "Standard evaluation protocols in robotic manipulation typically assess policy performance over curated, in-distribution test sets, offering limited insight into how systems fail under plausible variation. \n    We introduce a red-teaming framework that probes robustness through object-centric geometric perturbations, automatically generating CrashShapes---structurally valid, user-constrained mesh deformations that trigger catastrophic failures in pre-trained manipulation policies. \n    The method integrates a Jacobian field–based deformation model with a gradient-free, simulator-in-the-loop optimization strategy.\n    Across insertion, articulation, and grasping tasks, our approach consistently discovers deformations that collapse policy performance, revealing brittle failure modes missed by static benchmarks. \n    By combining task-level policy rollouts with constraint-aware shape exploration, we aim to build a general purpose framework for structured, object-centric robustness evaluation in robotic manipulation.\n    We additionally show that fine-tuning on individual CrashShapes, a process we refer to as blue-teaming, improves task success by up to 60 percentage points on those shapes, while preserving performance on the original object, demonstrating the utility of red-teamed geometries for targeted policy refinement.\n    Finally, we validate both red-teaming and blue-teaming results with a real robotic arm, observing that simulated CrashShapes reduce task success from 90\\% to as low as 22.5\\%, and that blue-teaming recovers performance to up to 90\\% on the corresponding real-world geometry---closely matching simulation outcomes."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/40881d2bfd69d2638d84a0d1987b6e083c508958.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngoel2025geometric,\ntitle={Geometric Red-Teaming for Robotic Manipulation},\nauthor={Divyam Goel and Yufei Wang and Tiancheng Wu and Guixiu Qiao and Pavel Piliptchak and David Held and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ux5EptB7xZ}\n}"
                    },
                    "paperhash": {
                        "value": "goel|geometric_redteaming_for_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission477/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission477/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission477/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745378334655,
                "pdate": 1754680622240,
                "odate": 1758062771203,
                "mdate": 1758062816465,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission477/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zk4fRmHF0Q",
        "title": "FlashBack: Consistency Model-Accelerated Shared Autonomy",
        "abstract": "Abstract: Shared autonomy is an enabling technology that provides users with control authority over robots that would otherwise be difficult if not impossible to directly control. Yet, standard methods make assumptions that limit their adoption in practice—for example, prior knowledge of the user’s goals or the objective (i.e., reward) function that they wish to optimize, knowledge of the user’s policy, or query-level access to the user during training. Diffusion-based approaches to shared autonomy do not make such assumptions and instead only require access to demonstrations of desired behaviors, while allowing the user to maintain control authority. However, these advantages have come at the expense of high computational complexity, which has made real-time shared autonomy all but impossible. To overcome this limitation, we propose Consistency Shared Autonomy (CSA), a shared autonomy framework that employs a consistency model-based formulation of diffusion. Key to CSA is that it employs the distilled probability flow of ordinary differential equations (PF ODE) to generate high-fidelity samples in a single step. This results in inference speeds significantly than what is possible with previous diffusion-based approaches to shared autonomy, enabling real-time assistance in complex domains with only a single function evaluation. Further, by intervening on flawed actions at intermediate states of the PF ODE, CSA enables varying levels of assistance. We evaluate CSA on a variety of challenging simulated and real-world robot control problems, demonstrating significant improvements over state-of-the-art methods both in terms of task performance and computational efficiency.",
        "keywords": [
            "Consistency Model",
            "Shared Autonomy",
            "ODE Distillation"
        ],
        "pdf_url": "https://openreview.net/pdf/12d154bcfcd150f984c729cfc73792f8e5ff9c31.pdf",
        "reviews": [
            {
                "id": "0myoZhUxYr",
                "forum": "zk4fRmHF0Q",
                "replyto": "zk4fRmHF0Q",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission476/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068284883,
                "mdate": 1754869468315,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zk4fRmHF0Q",
                "forum": "zk4fRmHF0Q",
                "content": {
                    "title": {
                        "value": "FlashBack: Consistency Model-Accelerated Shared Autonomy"
                    },
                    "authors": {
                        "value": [
                            "Luzhe Sun",
                            "Jingtian Ji",
                            "Xiangshan Tan",
                            "Matthew Walter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Luzhe_Sun2",
                            "~Jingtian_Ji2",
                            "~Xiangshan_Tan1",
                            "~Matthew_Walter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Consistency Model",
                            "Shared Autonomy",
                            "ODE Distillation"
                        ]
                    },
                    "abstract": {
                        "value": "Abstract: Shared autonomy is an enabling technology that provides users with control authority over robots that would otherwise be difficult if not impossible to directly control. Yet, standard methods make assumptions that limit their adoption in practice—for example, prior knowledge of the user’s goals or the objective (i.e., reward) function that they wish to optimize, knowledge of the user’s policy, or query-level access to the user during training. Diffusion-based approaches to shared autonomy do not make such assumptions and instead only require access to demonstrations of desired behaviors, while allowing the user to maintain control authority. However, these advantages have come at the expense of high computational complexity, which has made real-time shared autonomy all but impossible. To overcome this limitation, we propose Consistency Shared Autonomy (CSA), a shared autonomy framework that employs a consistency model-based formulation of diffusion. Key to CSA is that it employs the distilled probability flow of ordinary differential equations (PF ODE) to generate high-fidelity samples in a single step. This results in inference speeds significantly than what is possible with previous diffusion-based approaches to shared autonomy, enabling real-time assistance in complex domains with only a single function evaluation. Further, by intervening on flawed actions at intermediate states of the PF ODE, CSA enables varying levels of assistance. We evaluate CSA on a variety of challenging simulated and real-world robot control problems, demonstrating significant improvements over state-of-the-art methods both in terms of task performance and computational efficiency."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1bb974fa3a496a97226a3732e9a1b09b98a94d55.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/12d154bcfcd150f984c729cfc73792f8e5ff9c31.pdf"
                    },
                    "TLDR": {
                        "value": "Our CSA model leverages partial diffusion on consistency models to enable efficient, one-step inference for shared-autonomy tasks."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsun2025flashback,\ntitle={FlashBack: Consistency Model-Accelerated Shared Autonomy},\nauthor={Luzhe Sun and Jingtian Ji and Xiangshan Tan and Matthew Walter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zk4fRmHF0Q}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/61cb8773d093b6f74c25423f9c48157c77bf23e2.mp4"
                    },
                    "paperhash": {
                        "value": "sun|flashback_consistency_modelaccelerated_shared_autonomy"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission476/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission476/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission476/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745378202833,
                "pdate": 1754680622163,
                "odate": 1758062771172,
                "mdate": 1758062816325,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission476/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "BNdgT6GeC6",
        "title": "Granular loco-manipulation: Repositioning rocks through strategic sand avalanche",
        "abstract": "Legged robots have the potential to leverage obstacles to climb steep sand slopes. However, efficiently repositioning these obstacles to desired locations is challenging. Here we present DiffusiveGRAIN, a learning-based method that enables a multi-legged robot to strategically induce localized sand avalanches during locomotion and indirectly manipulate obstacles. We conducted 375 trials, systematically varying obstacle spacing, robot orientation, and leg actions in 75 of them. Results show that movement of closely-spaced obstacles exhibit significant interference, requiring joint modeling. In addition, different multi-leg excavation actions could cause distinct robot state changes, necessitating integrated planning of manipulation and locomotion. To address these challenges, DiffusiveGRAIN includes a diffusion-based environment predictor to capture multi-obstacle movements under granular flow interferences and a robot state predictor to estimate changes in robot state from multi-leg action patterns. Deployment experiments (90 trials) demonstrate that by integrating the environment and robot state predictors, the robot can autonomously plan its movements based on loco-manipulation goals, successfully shifting closely located rocks to desired locations in over 65% of trials. Our study showcases the potential for a locomoting robot to strategically manipulate obstacles to achieve improved mobility on challenging terrains.",
        "keywords": [
            "Granular media",
            "avalanche dynamics",
            "diffusion models",
            "legged robots"
        ],
        "pdf_url": "https://openreview.net/pdf/b008c6aa62c922b49fd13adf352420627dcf1250.pdf",
        "reviews": [
            {
                "id": "53F4JI8GwG",
                "forum": "BNdgT6GeC6",
                "replyto": "BNdgT6GeC6",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission474/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068284858,
                "mdate": 1754869468227,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "BNdgT6GeC6",
                "forum": "BNdgT6GeC6",
                "content": {
                    "title": {
                        "value": "Granular loco-manipulation: Repositioning rocks through strategic sand avalanche"
                    },
                    "authors": {
                        "value": [
                            "Haodi Hu",
                            "Yue Wu",
                            "Daniel Seita",
                            "Feifei Qian"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haodi_Hu1",
                            "~Yue_Wu46",
                            "~Daniel_Seita1",
                            "~Feifei_Qian1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Granular media",
                            "avalanche dynamics",
                            "diffusion models",
                            "legged robots"
                        ]
                    },
                    "abstract": {
                        "value": "Legged robots have the potential to leverage obstacles to climb steep sand slopes. However, efficiently repositioning these obstacles to desired locations is challenging. Here we present DiffusiveGRAIN, a learning-based method that enables a multi-legged robot to strategically induce localized sand avalanches during locomotion and indirectly manipulate obstacles. We conducted 375 trials, systematically varying obstacle spacing, robot orientation, and leg actions in 75 of them. Results show that movement of closely-spaced obstacles exhibit significant interference, requiring joint modeling. In addition, different multi-leg excavation actions could cause distinct robot state changes, necessitating integrated planning of manipulation and locomotion. To address these challenges, DiffusiveGRAIN includes a diffusion-based environment predictor to capture multi-obstacle movements under granular flow interferences and a robot state predictor to estimate changes in robot state from multi-leg action patterns. Deployment experiments (90 trials) demonstrate that by integrating the environment and robot state predictors, the robot can autonomously plan its movements based on loco-manipulation goals, successfully shifting closely located rocks to desired locations in over 65% of trials. Our study showcases the potential for a locomoting robot to strategically manipulate obstacles to achieve improved mobility on challenging terrains."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d6d9cd5157faeb334fa850288e6b57a94f646c55.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b008c6aa62c922b49fd13adf352420627dcf1250.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhu2025granular,\ntitle={Granular loco-manipulation: Repositioning rocks through strategic sand avalanche},\nauthor={Haodi Hu and Yue Wu and Daniel Seita and Feifei Qian},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BNdgT6GeC6}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d45e3e63a08e87c5a16d657c623d9cb4f125ae9e.zip"
                    },
                    "paperhash": {
                        "value": "hu|granular_locomanipulation_repositioning_rocks_through_strategic_sand_avalanche"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission474/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission474/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission474/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745378060291,
                "pdate": 1754680622103,
                "odate": 1758062771051,
                "mdate": 1758062816198,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission474/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission474/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "FCpYuGtN4j",
        "title": "HuB: Learning Extreme Humanoid Balance",
        "abstract": "The human body demonstrates exceptional motor capabilities—such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters—both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose $\\textbf{HuB}$ ($\\textbf{Hu}$manoid $\\textbf{B}$alance), a unified framework that integrates $\\textit{reference motion refinement}$, $\\textit{balance-aware policy learning}$, and $\\textit{sim-to-real robustness training}$, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as $\\texttt{Swallow Balance}$ and $\\texttt{Bruce Lee’s Kick}$. Our policy remains stable even under strong physical disturbances—such as a forceful soccer strike—while baseline methods consistently fail to complete these tasks.",
        "keywords": [
            "Humanoid Whole-body Control",
            "Balance Control",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/b29a36a3a7d2906350ac4bc75bb3d4cf1d6cfb4c.pdf",
        "reviews": [
            {
                "id": "6V4rzLVpd3",
                "forum": "FCpYuGtN4j",
                "replyto": "FCpYuGtN4j",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission470/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068284635,
                "mdate": 1754869456777,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "FCpYuGtN4j",
                "forum": "FCpYuGtN4j",
                "content": {
                    "title": {
                        "value": "HuB: Learning Extreme Humanoid Balance"
                    },
                    "authors": {
                        "value": [
                            "Tong Zhang",
                            "Boyuan Zheng",
                            "Ruiqian Nai",
                            "Yingdong Hu",
                            "Yen-Jen Wang",
                            "Geng Chen",
                            "Fanqi Lin",
                            "Jiongye Li",
                            "Chuye Hong",
                            "Koushil Sreenath",
                            "Yang Gao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tong_Zhang23",
                            "~Boyuan_Zheng3",
                            "~Ruiqian_Nai1",
                            "~Yingdong_Hu1",
                            "~Yen-Jen_Wang1",
                            "~Geng_Chen3",
                            "~Fanqi_Lin2",
                            "~Jiongye_Li1",
                            "~Chuye_Hong1",
                            "~Koushil_Sreenath1",
                            "~Yang_Gao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoid Whole-body Control",
                            "Balance Control",
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "The human body demonstrates exceptional motor capabilities—such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters—both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose $\\textbf{HuB}$ ($\\textbf{Hu}$manoid $\\textbf{B}$alance), a unified framework that integrates $\\textit{reference motion refinement}$, $\\textit{balance-aware policy learning}$, and $\\textit{sim-to-real robustness training}$, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as $\\texttt{Swallow Balance}$ and $\\texttt{Bruce Lee’s Kick}$. Our policy remains stable even under strong physical disturbances—such as a forceful soccer strike—while baseline methods consistently fail to complete these tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2b04c4462e801c80bb6162c407a4112fe60cf489.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b29a36a3a7d2906350ac4bc75bb3d4cf1d6cfb4c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025hub,\ntitle={HuB: Learning Extreme Humanoid Balance},\nauthor={Tong Zhang and Boyuan Zheng and Ruiqian Nai and Yingdong Hu and Yen-Jen Wang and Geng Chen and Fanqi Lin and Jiongye Li and Chuye Hong and Koushil Sreenath and Yang Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FCpYuGtN4j}\n}"
                    },
                    "paperhash": {
                        "value": "zhang|hub_learning_extreme_humanoid_balance"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission470/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission470/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission470/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745376403701,
                "pdate": 1754680621896,
                "odate": 1758062770920,
                "mdate": 1758062816139,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission470/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission470/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "LRG1xvtiwL",
        "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation",
        "abstract": "Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 180 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our anonymous website is at: https://dcodaaug.github.io/D-CODA/.",
        "keywords": [
            "Data Augmentation",
            "Bimanual Manipulation",
            "Diffusion Models"
        ],
        "pdf_url": "https://openreview.net/pdf/727b455ab2253bcb2a07133838b1d766e73cd402.pdf",
        "reviews": [
            {
                "id": "gueRhEAGqB",
                "forum": "LRG1xvtiwL",
                "replyto": "LRG1xvtiwL",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission465/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068284289,
                "mdate": 1754869468087,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "LRG1xvtiwL",
                "forum": "LRG1xvtiwL",
                "content": {
                    "title": {
                        "value": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation"
                    },
                    "authors": {
                        "value": [
                            "I-Chun Arthur Liu",
                            "Jason Chen",
                            "Gaurav S. Sukhatme",
                            "Daniel Seita"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~I-Chun_Arthur_Liu1",
                            "~Jason_Chen7",
                            "~Gaurav_S._Sukhatme1",
                            "~Daniel_Seita1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Data Augmentation",
                            "Bimanual Manipulation",
                            "Diffusion Models"
                        ]
                    },
                    "abstract": {
                        "value": "Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 180 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our anonymous website is at: https://dcodaaug.github.io/D-CODA/."
                    },
                    "supplementary_material": {
                        "value": "/attachment/3b1b9a68ac0df259190b2b97d4e4dd8f86e60d57.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "D-CODA is an offline data augmentation method tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while generating joint-space action labels."
                    },
                    "pdf": {
                        "value": "/pdf/727b455ab2253bcb2a07133838b1d766e73cd402.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025dcoda,\ntitle={D-{CODA}: Diffusion for Coordinated Dual-Arm Data Augmentation},\nauthor={I-Chun Arthur Liu and Jason Chen and Gaurav S. Sukhatme and Daniel Seita},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LRG1xvtiwL}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d5a50f14204389929aed59efd5253950e5a6b79c.zip"
                    },
                    "paperhash": {
                        "value": "liu|dcoda_diffusion_for_coordinated_dualarm_data_augmentation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission465/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission465/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission465/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745374834243,
                "pdate": 1754680621686,
                "odate": 1758062770816,
                "mdate": 1758062816061,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission465/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission465/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jPHhft5tNo",
        "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes",
        "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a  Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation.",
        "keywords": [
            "Multi-Robot Learning",
            "Sim2Real Deployment",
            "Benchmarking",
            "Jax"
        ],
        "pdf_url": "https://openreview.net/pdf/0536eca7f2719a57b45d89c6821a829e0fdfd83d.pdf",
        "reviews": [
            {
                "id": "p7kaImKmeC",
                "forum": "jPHhft5tNo",
                "replyto": "jPHhft5tNo",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission461/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068283954,
                "mdate": 1754869468091,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jPHhft5tNo",
                "forum": "jPHhft5tNo",
                "content": {
                    "title": {
                        "value": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes"
                    },
                    "authors": {
                        "value": [
                            "Shalin Jain",
                            "Jiazhen Liu",
                            "Siva Kailas",
                            "Harish Ravichandar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shalin_Jain1",
                            "~Jiazhen_Liu4",
                            "~Siva_Kailas1",
                            "~Harish_Ravichandar1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-Robot Learning",
                            "Sim2Real Deployment",
                            "Benchmarking",
                            "Jax"
                        ]
                    },
                    "abstract": {
                        "value": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a  Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/5513f940992695e34cf11b76af5e9ff38521c7a9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0536eca7f2719a57b45d89c6821a829e0fdfd83d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njain2025jaxrobotarium,\ntitle={JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes},\nauthor={Shalin Jain and Jiazhen Liu and Siva Kailas and Harish Ravichandar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jPHhft5tNo}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f4ecfb6ce1035c9550dae0f173f5fc8a4e154146.mp4"
                    },
                    "paperhash": {
                        "value": "jain|jaxrobotarium_training_and_deploying_multirobot_policies_in_10_minutes"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission461/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission461/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission461/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745374465193,
                "pdate": 1754680621493,
                "odate": 1758062770625,
                "mdate": 1758062815938,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission461/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "CQKxhmLobo",
        "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures",
        "abstract": "Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model’s epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space—spanning both the latent representation and the epistemic uncertainty—we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions.",
        "keywords": [
            "safe control",
            "uncertainty quantification",
            "world models"
        ],
        "pdf_url": "https://openreview.net/pdf/a8fee2ef2a28282a5d664d065373d5a00874a57b.pdf",
        "reviews": [
            {
                "id": "FQRJ4XsPyQ",
                "forum": "CQKxhmLobo",
                "replyto": "CQKxhmLobo",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission458/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068283302,
                "mdate": 1754869468032,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "CQKxhmLobo",
                "forum": "CQKxhmLobo",
                "content": {
                    "title": {
                        "value": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures"
                    },
                    "authors": {
                        "value": [
                            "Junwon Seo",
                            "Kensuke Nakamura",
                            "Andrea Bajcsy"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Junwon_Seo2",
                            "~Kensuke_Nakamura1",
                            "~Andrea_Bajcsy1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "safe control",
                            "uncertainty quantification",
                            "world models"
                        ]
                    },
                    "TLDR": {
                        "value": "Unifying reachability analysis in a latent world model with OOD detection to prevent both known and unseen safety hazards in hard-to-model vision-based tasks."
                    },
                    "abstract": {
                        "value": "Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model’s epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space—spanning both the latent representation and the epistemic uncertainty—we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ca413d58a8b4c6c9c18d703ee8cc9648c0420e51.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/a8fee2ef2a28282a5d664d065373d5a00874a57b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nseo2025uncertaintyaware,\ntitle={Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures},\nauthor={Junwon Seo and Kensuke Nakamura and Andrea Bajcsy},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CQKxhmLobo}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/4bc5c649dee4b11d49866534e00c4e2045e1b106.mp4"
                    },
                    "paperhash": {
                        "value": "seo|uncertaintyaware_latent_safety_filters_for_avoiding_outofdistribution_failures"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission458/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission458/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission458/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745373426616,
                "pdate": 1754680621327,
                "odate": 1758062770376,
                "mdate": 1758062815884,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission458/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission458/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "a5LFUOlkIj",
        "title": "ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning",
        "abstract": "Learning visuamotor policy through imitation learning often suffers from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances. In this work, we leverage 2D keypoints — spatially consistent features in the image frame — as a state representation for robust policy learning, and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method -ATK, to automatically select keypoints in a task-driven manner, such that the chosen keypoints are \nthat are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of task-relevant keypoints that preserve policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively tracks keypoints and transfers policies to the real-world evaluation scenario, even given perceptual challenges like transparent objects or fine-grained manipulation, or widely varying scene appearance. We validate our approach on various robotic tasks, demonstrating that these minimal keypoint representations improve robustness to visual disturbances and environmental variations.",
        "keywords": [
            "Imitation learning",
            "Sim2real",
            "Representation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/404e4302b170c4623bba3ab509c69308b7da7027.pdf",
        "reviews": [
            {
                "id": "dJIzNBBSAH",
                "forum": "a5LFUOlkIj",
                "replyto": "a5LFUOlkIj",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission450/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068282595,
                "mdate": 1754869467919,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "a5LFUOlkIj",
                "forum": "a5LFUOlkIj",
                "content": {
                    "title": {
                        "value": "ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning"
                    },
                    "authors": {
                        "value": [
                            "Yunchu Zhang",
                            "Shubham Mittal",
                            "Zhengyu Zhang",
                            "Liyiming Ke",
                            "Siddhartha Srinivasa",
                            "Abhishek Gupta"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yunchu_Zhang1",
                            "~Shubham_Mittal3",
                            "~Zhengyu_Zhang3",
                            "~Liyiming_Ke1",
                            "~Siddhartha_Srinivasa1",
                            "~Abhishek_Gupta1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation learning",
                            "Sim2real",
                            "Representation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Learning visuamotor policy through imitation learning often suffers from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances. In this work, we leverage 2D keypoints — spatially consistent features in the image frame — as a state representation for robust policy learning, and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method -ATK, to automatically select keypoints in a task-driven manner, such that the chosen keypoints are \nthat are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of task-relevant keypoints that preserve policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively tracks keypoints and transfers policies to the real-world evaluation scenario, even given perceptual challenges like transparent objects or fine-grained manipulation, or widely varying scene appearance. We validate our approach on various robotic tasks, demonstrating that these minimal keypoint representations improve robustness to visual disturbances and environmental variations."
                    },
                    "supplementary_material": {
                        "value": "/attachment/1f35c29d7de00cfe3bbc895cbfc14cd54108ea6b.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/404e4302b170c4623bba3ab509c69308b7da7027.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025atk,\ntitle={{ATK}: Automatic Task-driven Keypoint Selection for Robust Policy Learning},\nauthor={Yunchu Zhang and Shubham Mittal and Zhengyu Zhang and Liyiming Ke and Siddhartha Srinivasa and Abhishek Gupta},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a5LFUOlkIj}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a9e91a5f1868809302eee60e7f2ff95c5cae85c5.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|atk_automatic_taskdriven_keypoint_selection_for_robust_policy_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission450/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission450/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission450/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745370147318,
                "pdate": 1754680620930,
                "odate": 1758062770067,
                "mdate": 1758062815770,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission450/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "MCC3MG2aRH",
        "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation",
        "abstract": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 35 common and state-of-the-art VLM families on our benchmark, including variants to test different model sizes. The performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding.",
        "keywords": [
            "Vision-Language Models",
            "Robotics Benchmark",
            "Robot Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/75c36e1fe9a2c1c7d554df86cc7a2da0ba0f9fa9.pdf",
        "reviews": [
            {
                "id": "MJSSsXlUm2",
                "forum": "MCC3MG2aRH",
                "replyto": "MCC3MG2aRH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission448/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068282385,
                "mdate": 1754869467887,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "MCC3MG2aRH",
                "forum": "MCC3MG2aRH",
                "content": {
                    "title": {
                        "value": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Enyu Zhao",
                            "Vedant Raval",
                            "Hejia Zhang",
                            "Jiageng Mao",
                            "Zeyu Shangguan",
                            "Stefanos Nikolaidis",
                            "Yue Wang",
                            "Daniel Seita"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Enyu_Zhao1",
                            "~Vedant_Raval1",
                            "~Hejia_Zhang1",
                            "~Jiageng_Mao1",
                            "~Zeyu_Shangguan1",
                            "~Stefanos_Nikolaidis1",
                            "~Yue_Wang2",
                            "~Daniel_Seita1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language Models",
                            "Robotics Benchmark",
                            "Robot Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a benchmark of multiple choice questions to test VLMs' reasoning ability of low-level robot manipulation."
                    },
                    "abstract": {
                        "value": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 35 common and state-of-the-art VLM families on our benchmark, including variants to test different model sizes. The performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7a438c5c1577cc2ad75e3f591fe9280449a03539.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/75c36e1fe9a2c1c7d554df86cc7a2da0ba0f9fa9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhao2025manipbench,\ntitle={ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation},\nauthor={Enyu Zhao and Vedant Raval and Hejia Zhang and Jiageng Mao and Zeyu Shangguan and Stefanos Nikolaidis and Yue Wang and Daniel Seita},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MCC3MG2aRH}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c32e93084d0ed7afb7a60d0a442d9c122ea86c8a.mp4"
                    },
                    "paperhash": {
                        "value": "zhao|manipbench_benchmarking_visionlanguage_models_for_lowlevel_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission448/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission448/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission448/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745369128399,
                "pdate": 1754680620741,
                "odate": 1758062769975,
                "mdate": 1758062815687,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission448/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission448/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "vlhoswksBO",
        "title": "$\\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization",
        "abstract": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $\\pi_{0.5}$, a new model based on $\\pi_0$ that uses co-training on heterogeneous tasks to enable broad generalization. $\\pi_{0.5}$ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.",
        "keywords": [
            "vla",
            "generalization",
            "mobile manipulator"
        ],
        "pdf_url": "https://openreview.net/pdf/993cc214c7ac487ad1950d9d541e7a784d2130d0.pdf",
        "reviews": [
            {
                "id": "iq71gM7fy1",
                "forum": "vlhoswksBO",
                "replyto": "vlhoswksBO",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission439/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281895,
                "mdate": 1754869456695,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "vlhoswksBO",
                "forum": "vlhoswksBO",
                "content": {
                    "title": {
                        "value": "$\\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization"
                    },
                    "authors": {
                        "value": [
                            "Kevin Black",
                            "Noah Brown",
                            "James Darpinian",
                            "Karan Dhabalia",
                            "Danny Driess",
                            "Adnan Esmail",
                            "Michael Robert Equi",
                            "Chelsea Finn",
                            "Niccolo Fusai",
                            "Manuel Y. Galliker",
                            "Dibya Ghosh",
                            "Lachy Groom",
                            "Karol Hausman",
                            "brian ichter",
                            "Szymon Jakubczak",
                            "Tim Jones",
                            "Liyiming Ke",
                            "Devin LeBlanc",
                            "Sergey Levine",
                            "Adrian Li-Bell",
                            "Mohith Mothukuri",
                            "Suraj Nair",
                            "Karl Pertsch",
                            "Allen Z. Ren",
                            "Lucy Xiaoyang Shi",
                            "Laura Smith",
                            "Jost Tobias Springenberg",
                            "Kyle Stachowicz",
                            "James Tanner",
                            "Quan Vuong",
                            "Homer Walke",
                            "Anna Walling",
                            "Haohuan Wang",
                            "Lili Yu",
                            "Ury Zhilinsky"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kevin_Black2",
                            "~Noah_Brown1",
                            "james.darpinian@physicalintelligence.company",
                            "karan@physicalintelligence.company",
                            "~Danny_Driess1",
                            "adnan@physicalintelligence.company",
                            "~Michael_Robert_Equi1",
                            "~Chelsea_Finn1",
                            "~Niccolo_Fusai1",
                            "manuel@physicalintelligence.company",
                            "~Dibya_Ghosh1",
                            "~Lachy_Groom1",
                            "~Karol_Hausman2",
                            "~brian_ichter1",
                            "szymon@physicalintelligence.company",
                            "tim@physicalintelligence.company",
                            "~Liyiming_Ke1",
                            "devin@physicalintelligence.company",
                            "~Sergey_Levine1",
                            "~Adrian_Li-Bell1",
                            "mohith@physicalintelligence.company",
                            "~Suraj_Nair1",
                            "~Karl_Pertsch1",
                            "~Allen_Z._Ren1",
                            "~Lucy_Xiaoyang_Shi1",
                            "~Laura_Smith1",
                            "~Jost_Tobias_Springenberg1",
                            "~Kyle_Stachowicz1",
                            "~James_Tanner1",
                            "~Quan_Vuong2",
                            "~Homer_Walke1",
                            "~Anna_Walling1",
                            "~Haohuan_Wang1",
                            "~Lili_Yu2",
                            "ury@physicalintelligence.company"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "vla",
                            "generalization",
                            "mobile manipulator"
                        ]
                    },
                    "TLDR": {
                        "value": "We trained a vision-language-action (VLA) model that can perform long-horizon tasks and generalize to new homes."
                    },
                    "abstract": {
                        "value": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $\\pi_{0.5}$, a new model based on $\\pi_0$ that uses co-training on heterogeneous tasks to enable broad generalization. $\\pi_{0.5}$ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/993cc214c7ac487ad1950d9d541e7a784d2130d0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nblack2025pi,\ntitle={\\${\\textbackslash}pi\\_\\{0.5\\}\\$: a Vision-Language-Action Model with Open-World Generalization},\nauthor={Kevin Black and Noah Brown and James Darpinian and Karan Dhabalia and Danny Driess and Adnan Esmail and Michael Robert Equi and Chelsea Finn and Niccolo Fusai and Manuel Y. Galliker and Dibya Ghosh and Lachy Groom and Karol Hausman and brian ichter and Szymon Jakubczak and Tim Jones and Liyiming Ke and Devin LeBlanc and Sergey Levine and Adrian Li-Bell and Mohith Mothukuri and Suraj Nair and Karl Pertsch and Allen Z. Ren and Lucy Xiaoyang Shi and Laura Smith and Jost Tobias Springenberg and Kyle Stachowicz and James Tanner and Quan Vuong and Homer Walke and Anna Walling and Haohuan Wang and Lili Yu and Ury Zhilinsky},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=vlhoswksBO}\n}"
                    },
                    "paperhash": {
                        "value": "black|\\pi_05_a_visionlanguageaction_model_with_openworld_generalization"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission439/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745361289652,
                "pdate": 1754680620292,
                "odate": 1758062769650,
                "mdate": 1758062769671,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission439/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "aSUNzvEJIf",
        "title": "Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning",
        "abstract": "Multi-part assembly poses significant challenges for robotic systems to execute long-horizon, contact-rich manipulation with generalization across complex geometries. We present a dual-arm robotic system capable of end-to-end planning and control for autonomous assembly of general multi-part objects. For planning over long horizons, we develop hierarchies of precedence, sequence, grasp, and motion planning with automated fixture generation, enabling general multi-step assembly on any dual-arm robots. The planner is made efficient through a parallelizable design and is optimized for downstream control stability. For contact-rich assembly steps, we propose a lightweight reinforcement learning framework that trains generalist policies across object geometries, assembly directions, and grasp poses, guided by equivaraiance and residual actions obtained from the plan. These policies transfer zero-shot to the real world and achieve 80% success rates. For systematic evaluation, we propose a benchmark suite of multi-part assemblies resembling industrial and daily objects across diverse categories and geometries. By integrating efficient global planning and robust local control, we demonstrate the first system to achieve complete and generalizable real-world multi-part assembly without domain knowledge or human demonstrations.",
        "keywords": [
            "Assembly",
            "Planning",
            "Reinforcement Learning",
            "Benchmark"
        ],
        "pdf_url": "https://openreview.net/pdf/f9cd59a6201d32402e75336cacb7fe380773df25.pdf",
        "reviews": [
            {
                "id": "XZIhJPYVvb",
                "forum": "aSUNzvEJIf",
                "replyto": "aSUNzvEJIf",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission437/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281886,
                "mdate": 1754869456616,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "aSUNzvEJIf",
                "forum": "aSUNzvEJIf",
                "content": {
                    "title": {
                        "value": "Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning"
                    },
                    "authors": {
                        "value": [
                            "Yunsheng Tian",
                            "Joshua Jacob",
                            "Yijiang Huang",
                            "Jialiang Zhao",
                            "Edward Li Gu",
                            "Pingchuan Ma",
                            "Annan Zhang",
                            "Farhad Javid",
                            "Branden Romero",
                            "Sachin Chitta",
                            "Shinjiro Sueda",
                            "Hui Li",
                            "Wojciech Matusik"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yunsheng_Tian1",
                            "~Joshua_Jacob1",
                            "~Yijiang_Huang1",
                            "~Jialiang_Zhao1",
                            "~Edward_Li_Gu1",
                            "~Pingchuan_Ma3",
                            "~Annan_Zhang1",
                            "~Farhad_Javid1",
                            "~Branden_Romero1",
                            "~Sachin_Chitta1",
                            "~Shinjiro_Sueda1",
                            "~Hui_Li6",
                            "~Wojciech_Matusik2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Assembly",
                            "Planning",
                            "Reinforcement Learning",
                            "Benchmark"
                        ]
                    },
                    "abstract": {
                        "value": "Multi-part assembly poses significant challenges for robotic systems to execute long-horizon, contact-rich manipulation with generalization across complex geometries. We present a dual-arm robotic system capable of end-to-end planning and control for autonomous assembly of general multi-part objects. For planning over long horizons, we develop hierarchies of precedence, sequence, grasp, and motion planning with automated fixture generation, enabling general multi-step assembly on any dual-arm robots. The planner is made efficient through a parallelizable design and is optimized for downstream control stability. For contact-rich assembly steps, we propose a lightweight reinforcement learning framework that trains generalist policies across object geometries, assembly directions, and grasp poses, guided by equivaraiance and residual actions obtained from the plan. These policies transfer zero-shot to the real world and achieve 80% success rates. For systematic evaluation, we propose a benchmark suite of multi-part assemblies resembling industrial and daily objects across diverse categories and geometries. By integrating efficient global planning and robust local control, we demonstrate the first system to achieve complete and generalizable real-world multi-part assembly without domain knowledge or human demonstrations."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f02d4b11a8c287296f180f4077b4ddcca74c33a5.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A general planning and control system for flexible, dual-arm assembly of multi-part objects."
                    },
                    "pdf": {
                        "value": "/pdf/f9cd59a6201d32402e75336cacb7fe380773df25.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntian2025fabrica,\ntitle={Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning},\nauthor={Yunsheng Tian and Joshua Jacob and Yijiang Huang and Jialiang Zhao and Edward Li Gu and Pingchuan Ma and Annan Zhang and Farhad Javid and Branden Romero and Sachin Chitta and Shinjiro Sueda and Hui Li and Wojciech Matusik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=aSUNzvEJIf}\n}"
                    },
                    "paperhash": {
                        "value": "tian|fabrica_dualarm_assembly_of_general_multipart_objects_via_integrated_planning_and_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission437/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission437/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission437/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745360786233,
                "pdate": 1754680620280,
                "odate": 1758062769646,
                "mdate": 1758062815584,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission437/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "b2mXmmGX8E",
        "title": "IRIS: An Immersive Robot Interaction System",
        "abstract": "This paper introduces IRIS, an Immersive Robot Interaction System leveraging Extended Reality (XR). Existing XR-based systems enable efficient data collection but are often challenging to reproduce and reuse due to their specificity to particular robots, objects, simulators, and environments. IRIS addresses these issues by supporting immersive interaction and data collection across diverse simulators and real-world scenarios. It visualizes arbitrary rigid and deformable objects, robots from simulation, and integrates real-time sensor-generated point clouds for real-world applications. Additionally, IRIS enhances collaborative capabilities by enabling multiple users to simultaneously interact within the same virtual scene. Extensive experiments demonstrate that IRIS offers efficient and intuitive data collection in both simulated and real-world settings.",
        "keywords": [
            "Human-Robot Interaction",
            "Extended Reality",
            "Robot Learning: Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/67fdb26a1bd192294df8bb4ac3fa76a695dd21f4.pdf",
        "reviews": [
            {
                "id": "EgUz2Tnxvf",
                "forum": "b2mXmmGX8E",
                "replyto": "b2mXmmGX8E",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission433/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281739,
                "mdate": 1754869467826,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "b2mXmmGX8E",
                "forum": "b2mXmmGX8E",
                "content": {
                    "title": {
                        "value": "IRIS: An Immersive Robot Interaction System"
                    },
                    "authors": {
                        "value": [
                            "Xinkai Jiang",
                            "Qihao Yuan",
                            "Enes Ulas Dincer",
                            "Hongyi Zhou",
                            "Ge Li",
                            "Xueyin Li",
                            "Xiaogang Jia",
                            "Timo Schnizer",
                            "Nicolas Schreiber",
                            "Weiran Liao",
                            "Julius Haag",
                            "Kailai Li",
                            "Gerhard Neumann",
                            "Rudolf Lioutikov"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xinkai_Jiang1",
                            "~Qihao_Yuan1",
                            "~Enes_Ulas_Dincer1",
                            "~Hongyi_Zhou1",
                            "~Ge_Li3",
                            "~Xueyin_Li1",
                            "~Xiaogang_Jia1",
                            "~Timo_Schnizer1",
                            "~Nicolas_Schreiber1",
                            "~Weiran_Liao3",
                            "~Julius_Haag1",
                            "~Kailai_Li2",
                            "~Gerhard_Neumann2",
                            "~Rudolf_Lioutikov1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Human-Robot Interaction",
                            "Extended Reality",
                            "Robot Learning: Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "This paper introduces IRIS, an Immersive Robot Interaction System leveraging Extended Reality (XR). Existing XR-based systems enable efficient data collection but are often challenging to reproduce and reuse due to their specificity to particular robots, objects, simulators, and environments. IRIS addresses these issues by supporting immersive interaction and data collection across diverse simulators and real-world scenarios. It visualizes arbitrary rigid and deformable objects, robots from simulation, and integrates real-time sensor-generated point clouds for real-world applications. Additionally, IRIS enhances collaborative capabilities by enabling multiple users to simultaneously interact within the same virtual scene. Extensive experiments demonstrate that IRIS offers efficient and intuitive data collection in both simulated and real-world settings."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/67fdb26a1bd192294df8bb4ac3fa76a695dd21f4.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njiang2025iris,\ntitle={{IRIS}: An Immersive Robot Interaction System},\nauthor={Xinkai Jiang and Qihao Yuan and Enes Ulas Dincer and Hongyi Zhou and Ge Li and Xueyin Li and Xiaogang Jia and Timo Schnizer and Nicolas Schreiber and Weiran Liao and Julius Haag and Kailai Li and Gerhard Neumann and Rudolf Lioutikov},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b2mXmmGX8E}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8c604b2b4e5c1ea3d42bb939a38ea1edc7e02fe4.mp4"
                    },
                    "paperhash": {
                        "value": "jiang|iris_an_immersive_robot_interaction_system"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission433/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission433/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission433/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745356944096,
                "pdate": 1754680620050,
                "odate": 1758062769495,
                "mdate": 1758062815404,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission433/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission433/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "vLtS0ZDL73",
        "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection",
        "abstract": "Reliable localization is critical for robot navigation, yet many existing systems assume that all viewpoints along a trajectory are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At the core of ActLoc is an attention-based model trained at scale for viewpoint selection. This model encodes a metric map of the scene, along with camera poses used during map construction, and estimates localization accuracy over camera pitch and yaw directions at arbitrary 3D waypoint in space. This per-point accuracy distribution is integrated into the path planning process, allowing the robot to actively choose camera orientation that maximize localization robustness while respecting task and motion constraints. ActLoc achieves state-of-the-art performance in single-viewpoint selection task, and generalizes effectively to full-trajectory planning. It provides a modular enhancement to a wide range of navigation and inspection tasks in structured environments.",
        "keywords": [
            "Active Vision; Robot Navigation"
        ],
        "pdf_url": "https://openreview.net/pdf/09d63263c990ebcd3e44d64f1a51f975481a04b8.pdf",
        "reviews": [
            {
                "id": "AQWU9Vzngp",
                "forum": "vLtS0ZDL73",
                "replyto": "vLtS0ZDL73",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission432/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281572,
                "mdate": 1754869467711,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "vLtS0ZDL73",
                "forum": "vLtS0ZDL73",
                "content": {
                    "title": {
                        "value": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection"
                    },
                    "authors": {
                        "value": [
                            "Jiajie Li",
                            "Boyang Sun",
                            "Luca Di Giammarino",
                            "Hermann Blum",
                            "Marc Pollefeys"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiajie_Li4",
                            "~Boyang_Sun2",
                            "~Luca_Di_Giammarino1",
                            "~Hermann_Blum1",
                            "~Marc_Pollefeys2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Active Vision; Robot Navigation"
                        ]
                    },
                    "abstract": {
                        "value": "Reliable localization is critical for robot navigation, yet many existing systems assume that all viewpoints along a trajectory are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At the core of ActLoc is an attention-based model trained at scale for viewpoint selection. This model encodes a metric map of the scene, along with camera poses used during map construction, and estimates localization accuracy over camera pitch and yaw directions at arbitrary 3D waypoint in space. This per-point accuracy distribution is integrated into the path planning process, allowing the robot to actively choose camera orientation that maximize localization robustness while respecting task and motion constraints. ActLoc achieves state-of-the-art performance in single-viewpoint selection task, and generalizes effectively to full-trajectory planning. It provides a modular enhancement to a wide range of navigation and inspection tasks in structured environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2a335b6927e0c0a4f13b64cbe35db4e8ab0ab137.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A learning-based active viewpoint selection pipeline for robot navigation"
                    },
                    "pdf": {
                        "value": "/pdf/09d63263c990ebcd3e44d64f1a51f975481a04b8.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025actloc,\ntitle={ActLoc: Learning to Localize on the Move via Active Viewpoint Selection},\nauthor={Jiajie Li and Boyang Sun and Luca Di Giammarino and Hermann Blum and Marc Pollefeys},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=vLtS0ZDL73}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/39a04c2727e3927f5bb51f6cbb31616fc9266c7b.mp4"
                    },
                    "paperhash": {
                        "value": "li|actloc_learning_to_localize_on_the_move_via_active_viewpoint_selection"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission432/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission432/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission432/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745356696008,
                "pdate": 1754680620048,
                "odate": 1758062769425,
                "mdate": 1758062815383,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission432/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission432/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "H0zFqW6QM0",
        "title": "AnyPlace: Learning Generalizable Object Placement for Robot Manipulation",
        "abstract": "Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. We address this with AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify approximate placement locations, we can focus only on the relevant regions for precise local placement, which enables us to train the low-level placement-pose-prediction model to capture multimodal placements efficiently. For training, we generate a fully synthetic dataset comprising 13 categories of randomly generated objects in 5370 different placement poses across three configurations (insertion, stacking, hanging) and train local placement-prediction models. We extensively evaluate our method in high-fidelity simulation and show that it consistently outperforms baseline approaches across all three tasks in terms of success rate, coverage of placement modes, and precision. In real-world experiments, our method achieves an average success and coverage rate of 76% across three tasks, where most baseline methods fail completely. We further validate the generalization of our approach on 16 real-world placement tasks, demonstrating that models trained purely on synthetic data can be directly transferred to the real world in a zero-shot setting. More at: https://anyplace-pnp.github.io.",
        "keywords": [
            "Pick and Place",
            "Robot Manipulation",
            "Synthetic Dataset"
        ],
        "pdf_url": "https://openreview.net/pdf/ce7dbf014cf03bc399cce8f44299506b200eae03.pdf",
        "reviews": [
            {
                "id": "JgdHoPTGuO",
                "forum": "H0zFqW6QM0",
                "replyto": "H0zFqW6QM0",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission425/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281311,
                "mdate": 1754869467672,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "H0zFqW6QM0",
                "forum": "H0zFqW6QM0",
                "content": {
                    "title": {
                        "value": "AnyPlace: Learning Generalizable Object Placement for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yuchi Zhao",
                            "Miroslav Bogdanovic",
                            "Chengyuan Luo",
                            "Steven Tohme",
                            "Kourosh Darvish",
                            "Alan Aspuru-Guzik",
                            "Florian Shkurti",
                            "Animesh Garg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuchi_Zhao2",
                            "~Miroslav_Bogdanovic1",
                            "~Chengyuan_Luo1",
                            "~Steven_Tohme1",
                            "~Kourosh_Darvish1",
                            "~Alan_Aspuru-Guzik2",
                            "~Florian_Shkurti1",
                            "~Animesh_Garg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Pick and Place",
                            "Robot Manipulation",
                            "Synthetic Dataset"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks."
                    },
                    "abstract": {
                        "value": "Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. We address this with AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify approximate placement locations, we can focus only on the relevant regions for precise local placement, which enables us to train the low-level placement-pose-prediction model to capture multimodal placements efficiently. For training, we generate a fully synthetic dataset comprising 13 categories of randomly generated objects in 5370 different placement poses across three configurations (insertion, stacking, hanging) and train local placement-prediction models. We extensively evaluate our method in high-fidelity simulation and show that it consistently outperforms baseline approaches across all three tasks in terms of success rate, coverage of placement modes, and precision. In real-world experiments, our method achieves an average success and coverage rate of 76% across three tasks, where most baseline methods fail completely. We further validate the generalization of our approach on 16 real-world placement tasks, demonstrating that models trained purely on synthetic data can be directly transferred to the real world in a zero-shot setting. More at: https://anyplace-pnp.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/42d7f353dcbe788df07198f74056ac90598d8acc.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/ce7dbf014cf03bc399cce8f44299506b200eae03.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhao2025anyplace,\ntitle={AnyPlace: Learning Generalizable Object Placement for Robot Manipulation},\nauthor={Yuchi Zhao and Miroslav Bogdanovic and Chengyuan Luo and Steven Tohme and Kourosh Darvish and Alan Aspuru-Guzik and Florian Shkurti and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=H0zFqW6QM0}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/93534597ee7f9aee7bc814858add4b82d5ff5d76.mp4"
                    },
                    "paperhash": {
                        "value": "zhao|anyplace_learning_generalizable_object_placement_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission425/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission425/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745354865443,
                "pdate": 1754680619729,
                "odate": 1758062769126,
                "mdate": 1758062815319,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission425/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission425/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "isrcFrgwZp",
        "title": "AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World",
        "abstract": "Scalable and reproducible policy evaluation has been a long-standing challenge in robot learning: evaluations are critical to assess progress and build better policies, but evaluation in the real world, especially at a scale that would provide statistically reliable results, is costly in terms of human time and hard to obtain. Evaluation of increasingly generalist robot policies requires an increasingly diverse repertoire of evaluation environments, making the evaluation bottleneck even more pronounced. To make real-world evaluation of robotic policies more practical, we propose AutoEval, a system to autonomously evaluate generalist robot policies around the clock with minimal human intervention. Users interact with AutoEval by submitting evaluation jobs to the AutoEval queue, much like how software jobs are submitted with a cluster scheduling system, and AutoEval will schedule the policies for evaluation within a framework supplying automatic success detection and automatic scene resets. We show that AutoEval can nearly fully eliminate human involvement in the evaluation process, permitting around the clock evaluations, and the evaluation results correspond closely to ground truth evaluations conducted by hand. To facilitate the evaluation of generalist policies in the robotics community, we provide public access to multiple AutoEval scenes in the popular BridgeData robot setup with WidowX robot arms. In the future, we hope that AutoEval scenes can be set up across institutions to form a diverse and distributed evaluation network.",
        "keywords": [
            "Robot evaluation",
            "benchmark",
            "generalist robot policy",
            "manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/9065b02ff17da4e92d0a75e831389b646979d9fb.pdf",
        "reviews": [
            {
                "id": "yyMpSOj7C8",
                "forum": "isrcFrgwZp",
                "replyto": "isrcFrgwZp",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission422/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281035,
                "mdate": 1754869467634,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "isrcFrgwZp",
                "forum": "isrcFrgwZp",
                "content": {
                    "title": {
                        "value": "AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World"
                    },
                    "authors": {
                        "value": [
                            "Zhiyuan Zhou",
                            "Pranav Atreya",
                            "You Liang Tan",
                            "Karl Pertsch",
                            "Sergey Levine"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhiyuan_Zhou2",
                            "~Pranav_Atreya1",
                            "~You_Liang_Tan1",
                            "~Karl_Pertsch1",
                            "~Sergey_Levine1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot evaluation",
                            "benchmark",
                            "generalist robot policy",
                            "manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "We build a system, AutoEval, that autonomously evaluates generalist robot policies around the clock that correlates closely with ground truth human evaluation results while saving >99% of human time."
                    },
                    "abstract": {
                        "value": "Scalable and reproducible policy evaluation has been a long-standing challenge in robot learning: evaluations are critical to assess progress and build better policies, but evaluation in the real world, especially at a scale that would provide statistically reliable results, is costly in terms of human time and hard to obtain. Evaluation of increasingly generalist robot policies requires an increasingly diverse repertoire of evaluation environments, making the evaluation bottleneck even more pronounced. To make real-world evaluation of robotic policies more practical, we propose AutoEval, a system to autonomously evaluate generalist robot policies around the clock with minimal human intervention. Users interact with AutoEval by submitting evaluation jobs to the AutoEval queue, much like how software jobs are submitted with a cluster scheduling system, and AutoEval will schedule the policies for evaluation within a framework supplying automatic success detection and automatic scene resets. We show that AutoEval can nearly fully eliminate human involvement in the evaluation process, permitting around the clock evaluations, and the evaluation results correspond closely to ground truth evaluations conducted by hand. To facilitate the evaluation of generalist policies in the robotics community, we provide public access to multiple AutoEval scenes in the popular BridgeData robot setup with WidowX robot arms. In the future, we hope that AutoEval scenes can be set up across institutions to form a diverse and distributed evaluation network."
                    },
                    "supplementary_material": {
                        "value": "/attachment/82e52753f169e76d8870af4cc7fe5ebfd5c6ed96.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9065b02ff17da4e92d0a75e831389b646979d9fb.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhou2025autoeval,\ntitle={AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World},\nauthor={Zhiyuan Zhou and Pranav Atreya and You Liang Tan and Karl Pertsch and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=isrcFrgwZp}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/04b22413cca656c73a5f95d3a99846b26a18398b.mp4"
                    },
                    "paperhash": {
                        "value": "zhou|autoeval_autonomous_evaluation_of_generalist_robot_manipulation_policies_in_the_real_world"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission422/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission422/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745353616233,
                "pdate": 1754680619551,
                "odate": 1758062768983,
                "mdate": 1758062815165,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission422/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "SUqMCzslNH",
        "title": "Poke and Strike: Learning Task-Informed Exploration Policies",
        "abstract": "In many dynamic robotic tasks, such as striking pucks into a goal outside the reachable workspace, the robot must first identify the relevant physical properties of the object for successful task execution, as it is unable to recover from failure or retry without human intervention. To address this challenge, we propose a task-informed exploration approach, based on reinforcement learning, that trains an exploration policy using rewards automatically generated from the sensitivity of a privileged task policy to errors in estimated properties. We also introduce an uncertainty-based mechanism to determine when to transition from exploration to task execution, ensuring sufficient property estimation accuracy with minimal exploration time. Our method achieves a 90% success rate on the striking task with an average exploration time under 1.2 seconds—significantly outperforming baselines that achieve at most 40% success or require inefficient querying and retraining in a simulator at test time. Additionally, we demonstrate that our task-informed rewards capture the relative importance of physical properties in both the striking task and the classical CartPole example. Finally, we validate our approach by demonstrating its ability to identify object properties and adjust task execution in a physical setup using the KUKA iiwa robot arm.",
        "keywords": [
            "Interactive Perception",
            "Manipulation",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/cd8aecc02677c315fe41095465eef25b6ff59cee.pdf",
        "reviews": [
            {
                "id": "ychQegJWCP",
                "forum": "SUqMCzslNH",
                "replyto": "SUqMCzslNH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission421/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068281031,
                "mdate": 1754869456542,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "SUqMCzslNH",
                "forum": "SUqMCzslNH",
                "content": {
                    "title": {
                        "value": "Poke and Strike: Learning Task-Informed Exploration Policies"
                    },
                    "authors": {
                        "value": [
                            "Marina Y. Aoyama",
                            "Joao Moura",
                            "Juan Del Aguila Ferrandis",
                            "Sethu Vijayakumar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Marina_Y._Aoyama1",
                            "~Joao_Moura1",
                            "~Juan_Del_Aguila_Ferrandis1",
                            "~Sethu_Vijayakumar1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Interactive Perception",
                            "Manipulation",
                            "Reinforcement Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a task-informed exploration approach, based on reinforcement learning, that learns to explore and perform dynamic tasks with objects of unknown physical properties, and validate it in both simulation and on a physical KUKA iiwa robot arm."
                    },
                    "abstract": {
                        "value": "In many dynamic robotic tasks, such as striking pucks into a goal outside the reachable workspace, the robot must first identify the relevant physical properties of the object for successful task execution, as it is unable to recover from failure or retry without human intervention. To address this challenge, we propose a task-informed exploration approach, based on reinforcement learning, that trains an exploration policy using rewards automatically generated from the sensitivity of a privileged task policy to errors in estimated properties. We also introduce an uncertainty-based mechanism to determine when to transition from exploration to task execution, ensuring sufficient property estimation accuracy with minimal exploration time. Our method achieves a 90% success rate on the striking task with an average exploration time under 1.2 seconds—significantly outperforming baselines that achieve at most 40% success or require inefficient querying and retraining in a simulator at test time. Additionally, we demonstrate that our task-informed rewards capture the relative importance of physical properties in both the striking task and the classical CartPole example. Finally, we validate our approach by demonstrating its ability to identify object properties and adjust task execution in a physical setup using the KUKA iiwa robot arm."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7ded0652417010dfd76c7602b51f8a3d82f20833.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/cd8aecc02677c315fe41095465eef25b6ff59cee.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\naoyama2025poke,\ntitle={Poke and Strike: Learning Task-Informed Exploration Policies},\nauthor={Marina Y. Aoyama and Joao Moura and Juan Del Aguila Ferrandis and Sethu Vijayakumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SUqMCzslNH}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/55104b0ab0ab2342dac419eadb4ffb0a8196a5a7.mp4"
                    },
                    "paperhash": {
                        "value": "aoyama|poke_and_strike_learning_taskinformed_exploration_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission421/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission421/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission421/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745353266464,
                "pdate": 1754680619546,
                "odate": 1758062768963,
                "mdate": 1758062815148,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission421/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission421/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "yeuA6M8JIX",
        "title": "Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration",
        "abstract": "In unstructured environments, robotic manipulation tasks involving objects with constrained motion trajectories—such as door opening—often experience discrepancies between the robot's vision-guided end-effector trajectory and the object's constrained motion path. \nSuch discrepancies generate unintended harmful forces, which, if exacerbated, may lead to task failure and potential damage to the manipulated objects or the robot itself. To address this issue, this paper introduces a novel diffusion framework, termed SafeDiff. Unlike conventional methods that sequentially fuse visual and tactile data to predict future robot states, our approach generates a prospective state sequence based on the current robot state and visual context observations, using real-time force feedback as a calibration signal. \nThis implicitly adjusts the robot’s state within the state space, enhancing operational success rates and significantly reducing harmful forces during manipulation, thus ensuring manipulation force safety. Additionally, we develop a large-scale simulation dataset named SafeDoorManip50k, offering extensive multimodal data to train and evaluate the proposed method. Extensive experiments show that our visual-tactile model substantially mitigates the risk of harmful forces in the door opening task, across both simulated and real-world settings.",
        "keywords": [
            "Imitation Learning",
            "Trajectory Constrainted Task",
            "Multi-modality",
            "Force Safety"
        ],
        "pdf_url": "https://openreview.net/pdf/6ddd3ebcd0c4c8527fadd8ce4d77781cbfe18077.pdf",
        "reviews": [
            {
                "id": "L7YZr06TR1",
                "forum": "yeuA6M8JIX",
                "replyto": "yeuA6M8JIX",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission413/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068280676,
                "mdate": 1754869467504,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "yeuA6M8JIX",
                "forum": "yeuA6M8JIX",
                "content": {
                    "title": {
                        "value": "Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration"
                    },
                    "authors": {
                        "value": [
                            "Lai Wei",
                            "Jiahua Ma",
                            "Yibo Hu",
                            "Ruimao Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Lai_Wei4",
                            "~Jiahua_Ma2",
                            "~Yibo_Hu4",
                            "~Ruimao_Zhang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Trajectory Constrainted Task",
                            "Multi-modality",
                            "Force Safety"
                        ]
                    },
                    "abstract": {
                        "value": "In unstructured environments, robotic manipulation tasks involving objects with constrained motion trajectories—such as door opening—often experience discrepancies between the robot's vision-guided end-effector trajectory and the object's constrained motion path. \nSuch discrepancies generate unintended harmful forces, which, if exacerbated, may lead to task failure and potential damage to the manipulated objects or the robot itself. To address this issue, this paper introduces a novel diffusion framework, termed SafeDiff. Unlike conventional methods that sequentially fuse visual and tactile data to predict future robot states, our approach generates a prospective state sequence based on the current robot state and visual context observations, using real-time force feedback as a calibration signal. \nThis implicitly adjusts the robot’s state within the state space, enhancing operational success rates and significantly reducing harmful forces during manipulation, thus ensuring manipulation force safety. Additionally, we develop a large-scale simulation dataset named SafeDoorManip50k, offering extensive multimodal data to train and evaluate the proposed method. Extensive experiments show that our visual-tactile model substantially mitigates the risk of harmful forces in the door opening task, across both simulated and real-world settings."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6ddd3ebcd0c4c8527fadd8ce4d77781cbfe18077.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwei2025ensuring,\ntitle={Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration},\nauthor={Lai Wei and Jiahua Ma and Yibo Hu and Ruimao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=yeuA6M8JIX}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/260c16dbbb13d079d74656f6f7ea27b5a4559182.mp4"
                    },
                    "paperhash": {
                        "value": "wei|ensuring_force_safety_in_visionguided_robotic_manipulation_via_implicit_tactile_calibration"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission413/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission413/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission413/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745347882630,
                "pdate": 1754680619289,
                "odate": 1758062768759,
                "mdate": 1758062815037,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission413/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "GfEHkeEU73",
        "title": "Off Policy Lyapunov Stability in Reinforcement Learning",
        "abstract": "Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function.",
        "keywords": [
            "Reinforcement Learning",
            "Control",
            "Stability",
            "Lyapunov"
        ],
        "pdf_url": "https://openreview.net/pdf/d918f89654124b82462303c9f45a6935e7f09f84.pdf",
        "reviews": [
            {
                "id": "ipjpKAA35j",
                "forum": "GfEHkeEU73",
                "replyto": "GfEHkeEU73",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission412/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068280570,
                "mdate": 1754869467444,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "GfEHkeEU73",
                "forum": "GfEHkeEU73",
                "content": {
                    "title": {
                        "value": "Off Policy Lyapunov Stability in Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Sarvan Gill",
                            "Daniela Constantinescu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sarvan_Gill1",
                            "~Daniela_Constantinescu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement Learning",
                            "Control",
                            "Stability",
                            "Lyapunov"
                        ]
                    },
                    "TLDR": {
                        "value": "A novel method to compute Lyapunov functions using off-policy data that can provide stability certificates to reinforcement learning algorihtms."
                    },
                    "abstract": {
                        "value": "Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d918f89654124b82462303c9f45a6935e7f09f84.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngill2025off,\ntitle={Off Policy Lyapunov Stability in Reinforcement Learning},\nauthor={Sarvan Gill and Daniela Constantinescu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GfEHkeEU73}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a935b54c54a02cf65671d7542b142f10e77a888c.zip"
                    },
                    "paperhash": {
                        "value": "gill|off_policy_lyapunov_stability_in_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission412/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission412/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745347661608,
                "pdate": 1754680619287,
                "odate": 1758062768718,
                "mdate": 1758062814930,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission412/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission412/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "qTYD21Wpxb",
        "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination",
        "abstract": "Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe—Reinforcement-based Constraint Design—a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.",
        "keywords": [
            "multi-agent reinforcement learning",
            "multi-robot systems"
        ],
        "pdf_url": "https://openreview.net/pdf/4e35f77ec3c578006f9db1bd2e18e09d6e4d8bda.pdf",
        "reviews": [
            {
                "id": "0ysKiXupDX",
                "forum": "qTYD21Wpxb",
                "replyto": "qTYD21Wpxb",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission407/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068280312,
                "mdate": 1754869467374,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "qTYD21Wpxb",
                "forum": "qTYD21Wpxb",
                "content": {
                    "title": {
                        "value": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination"
                    },
                    "authors": {
                        "value": [
                            "Michael Amir",
                            "Guang Yang",
                            "Zhan Gao",
                            "Keisuke Okumura",
                            "Heedo Woo",
                            "Amanda Prorok"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Michael_Amir1",
                            "~Guang_Yang16",
                            "~Zhan_Gao1",
                            "~Keisuke_Okumura1",
                            "~Heedo_Woo1",
                            "~Amanda_Prorok1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "multi-agent reinforcement learning",
                            "multi-robot systems"
                        ]
                    },
                    "abstract": {
                        "value": "Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe—Reinforcement-based Constraint Design—a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller."
                    },
                    "supplementary_material": {
                        "value": "/attachment/dabd426449651a0b1c203c7631aac93a5586b1da.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We present a method for improving multi-agent coordination by augmenting handcrafted controllers with additional, learned constraints."
                    },
                    "pdf": {
                        "value": "/pdf/4e35f77ec3c578006f9db1bd2e18e09d6e4d8bda.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\namir2025recode,\ntitle={ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination},\nauthor={Michael Amir and Guang Yang and Zhan Gao and Keisuke Okumura and Heedo Woo and Amanda Prorok},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qTYD21Wpxb}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/4aaadd4332a338a105c4d3b21024025cca08919c.zip"
                    },
                    "paperhash": {
                        "value": "amir|recode_reinforcement_learningbased_dynamic_constraint_design_for_multiagent_coordination"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission407/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission407/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission407/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745344280692,
                "pdate": 1754680619111,
                "odate": 1758062768485,
                "mdate": 1758062814890,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission407/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "nMiyWyNhQx",
        "title": "Human-like Navigation in a World Built for Humans",
        "abstract": "When navigating in a man-made environment they haven’t visited before—like an office building—humans employ behaviors such as reading signs and asking others for directions. These behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilites of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings.",
        "keywords": [
            "navigation",
            "reasoning",
            "vison-language model"
        ],
        "pdf_url": "https://openreview.net/pdf/c7b0582720d5a7364258b38d625fc1ffc59e13b1.pdf",
        "reviews": [
            {
                "id": "c4CSlUckbD",
                "forum": "nMiyWyNhQx",
                "replyto": "nMiyWyNhQx",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission398/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068279876,
                "mdate": 1754869467313,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "nMiyWyNhQx",
                "forum": "nMiyWyNhQx",
                "content": {
                    "title": {
                        "value": "Human-like Navigation in a World Built for Humans"
                    },
                    "authors": {
                        "value": [
                            "Bhargav Chandaka",
                            "Gloria Xinyue Wang",
                            "Haozhe Chen",
                            "Henry Che",
                            "Albert J. Zhai",
                            "Shenlong Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Bhargav_Chandaka1",
                            "~Gloria_Xinyue_Wang1",
                            "~Haozhe_Chen5",
                            "~Henry_Che1",
                            "~Albert_J._Zhai1",
                            "~Shenlong_Wang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "navigation",
                            "reasoning",
                            "vison-language model"
                        ]
                    },
                    "abstract": {
                        "value": "When navigating in a man-made environment they haven’t visited before—like an office building—humans employ behaviors such as reading signs and asking others for directions. These behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilites of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ee6f3207c1cf04ae4618328650300e1caa1b197c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c7b0582720d5a7364258b38d625fc1ffc59e13b1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchandaka2025humanlike,\ntitle={Human-like Navigation in a World Built for Humans},\nauthor={Bhargav Chandaka and Gloria Xinyue Wang and Haozhe Chen and Henry Che and Albert J. Zhai and Shenlong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=nMiyWyNhQx}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/81f4abd720a198d8e7781f974e75f92b99f74c68.zip"
                    },
                    "paperhash": {
                        "value": "chandaka|humanlike_navigation_in_a_world_built_for_humans"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission398/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission398/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission398/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745339704201,
                "pdate": 1754680618706,
                "odate": 1758062768138,
                "mdate": 1758062814793,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission398/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission398/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jnpILGz9gQ",
        "title": "Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories",
        "abstract": "Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a *trajectory of trajectories*—a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by *treating action trajectories as flow trajectories*. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a *single* trajectory. This enables actions to be streamed to the robot on-the-fly *during* the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that *stabilize* around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.",
        "keywords": [
            "imitation learning",
            "diffusion policy",
            "flow matching"
        ],
        "pdf_url": "https://openreview.net/pdf/b0d0cb9e88ebf5b408354ca1329bd1c5a11f9c1b.pdf",
        "reviews": [
            {
                "id": "IpXPwIZPt6",
                "forum": "jnpILGz9gQ",
                "replyto": "jnpILGz9gQ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission397/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068279771,
                "mdate": 1754869456465,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jnpILGz9gQ",
                "forum": "jnpILGz9gQ",
                "content": {
                    "title": {
                        "value": "Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories"
                    },
                    "authors": {
                        "value": [
                            "Sunshine Jiang",
                            "Xiaolin Fang",
                            "Nicholas Roy",
                            "Tomás Lozano-Pérez",
                            "Leslie Pack Kaelbling",
                            "Siddharth Ancha"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sunshine_Jiang1",
                            "~Xiaolin_Fang1",
                            "~Nicholas_Roy1",
                            "~Tomás_Lozano-Pérez1",
                            "~Leslie_Pack_Kaelbling1",
                            "~Siddharth_Ancha1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "imitation learning",
                            "diffusion policy",
                            "flow matching"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a novel imitation learning framework based on flow matching that simplifies diffusion flow/policies by treating robot trajectories as flow trajectories, allowing us to stream actions during flow sampling."
                    },
                    "abstract": {
                        "value": "Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a *trajectory of trajectories*—a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by *treating action trajectories as flow trajectories*. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a *single* trajectory. This enables actions to be streamed to the robot on-the-fly *during* the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that *stabilize* around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e2973afa4f7edb9a5308cc877d10193b04e7590c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b0d0cb9e88ebf5b408354ca1329bd1c5a11f9c1b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njiang2025streaming,\ntitle={Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories},\nauthor={Sunshine Jiang and Xiaolin Fang and Nicholas Roy and Tom{\\'a}s Lozano-P{\\'e}rez and Leslie Pack Kaelbling and Siddharth Ancha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jnpILGz9gQ}\n}"
                    },
                    "paperhash": {
                        "value": "jiang|streaming_flow_policy_simplifying_diffusionflowmatching_policies_by_treating_action_trajectories_as_flow_trajectories"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission397/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission397/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission397/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745339502938,
                "pdate": 1754680618664,
                "odate": 1758062768107,
                "mdate": 1758062814686,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission397/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission397/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "pJ5FONkM9N",
        "title": "Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection",
        "abstract": "Evaluating learned robot control policies to determine their performance costs the experimenter time and effort. As robots become more capable in accomplishing diverse tasks, evaluating across all these tasks becomes more difficult as it is impractical to test every policy on every task multiple times. Rather than considering the average performance of a policy on a task, we consider the distribution of performance over time. In a multi-task policy evaluation setting, we actively model the distribution of robot performance across multiple tasks and policies as we sequentially execute experiments. We show that natural language is a useful prior in modeling relationships between tasks because they often share similarities that can reveal potential relationships in policy behavior. We leverage this formulation to reduce experimenter effort by using a cost-aware information gain heuristic to efficiently select informative trials. We conduct experiments on existing evaluation data from real robots and simulations and find a 50% reduction in estimates of the mean performance given a fixed cost budget. We encourage the use of our surrogate model as a scalable approach to track progress in evaluation.",
        "keywords": [
            "robot evaluation",
            "active testing",
            "manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/9190c7cfb32f5db1b087a6588a460556b83a5869.pdf",
        "reviews": [
            {
                "id": "2HaiME7XKL",
                "forum": "pJ5FONkM9N",
                "replyto": "pJ5FONkM9N",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission396/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070537454,
                "mdate": 1754869467231,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "pJ5FONkM9N",
                "forum": "pJ5FONkM9N",
                "content": {
                    "title": {
                        "value": "Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection"
                    },
                    "authors": {
                        "value": [
                            "Abrar Anwar",
                            "Rohan Gupta",
                            "Zain Merchant",
                            "Sayan Ghosh",
                            "Willie Neiswanger",
                            "Jesse Thomason"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Abrar_Anwar1",
                            "~Rohan_Gupta3",
                            "~Zain_Merchant1",
                            "~Sayan_Ghosh5",
                            "~Willie_Neiswanger2",
                            "~Jesse_Thomason1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "robot evaluation",
                            "active testing",
                            "manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "We introduce a framework to model the distribution of robot performance across tasks and policies during evaluation, which enables us to actively select informative experiments in a cost-aware manner."
                    },
                    "abstract": {
                        "value": "Evaluating learned robot control policies to determine their performance costs the experimenter time and effort. As robots become more capable in accomplishing diverse tasks, evaluating across all these tasks becomes more difficult as it is impractical to test every policy on every task multiple times. Rather than considering the average performance of a policy on a task, we consider the distribution of performance over time. In a multi-task policy evaluation setting, we actively model the distribution of robot performance across multiple tasks and policies as we sequentially execute experiments. We show that natural language is a useful prior in modeling relationships between tasks because they often share similarities that can reveal potential relationships in policy behavior. We leverage this formulation to reduce experimenter effort by using a cost-aware information gain heuristic to efficiently select informative trials. We conduct experiments on existing evaluation data from real robots and simulations and find a 50% reduction in estimates of the mean performance given a fixed cost budget. We encourage the use of our surrogate model as a scalable approach to track progress in evaluation."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9d6cda5e60fa9e0da15aff37d43a04fd20f3be8f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9190c7cfb32f5db1b087a6588a460556b83a5869.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nanwar2025efficient,\ntitle={Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection},\nauthor={Abrar Anwar and Rohan Gupta and Zain Merchant and Sayan Ghosh and Willie Neiswanger and Jesse Thomason},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=pJ5FONkM9N}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2d415d7ce2a46f28b1b6d9bbc29bdcd69f30074c.mp4"
                    },
                    "paperhash": {
                        "value": "anwar|efficient_evaluation_of_multitask_robot_policies_with_active_experiment_selection"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission396/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission396/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission396/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745339471284,
                "pdate": 1754680618649,
                "odate": 1758062768031,
                "mdate": 1758062814656,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission396/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "06YyNxzwae",
        "title": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference",
        "abstract": "Skill effect models for long-horizon manipulation tasks are prone to failures in conditions not covered by training data distributions. Therefore, enabling robots to reason about and learn from failures is necessary. We investigate the problem of efficiently generating a dataset targeted to observed failures. After fine-tuning a skill effect model on this dataset, we evaluate the extent to which the model can recover from failures and minimize future failures. We propose Fail2Progress, an approach that leverages Stein variational inference to generate multiple simulation environments in parallel, enabling efficient data sample generation similar to observed failures. Our method is capable of handling several challenging mobile manipulation tasks, including transporting multiple objects, organizing a constrained shelf, and tabletop organization. Through large-scale simulation and real-world experiments, we demonstrate that our approach excels at learning from failures across different numbers of objects. Furthermore, we show that Fail2Progress outperforms several baselines.",
        "keywords": [
            "Learning from failures",
            "Variational inference",
            "Skill effect models"
        ],
        "pdf_url": "https://openreview.net/pdf/9f6070edb3b86d81b688d55981467e6be8a14266.pdf",
        "reviews": [
            {
                "id": "YWLGDHaxtU",
                "forum": "06YyNxzwae",
                "replyto": "06YyNxzwae",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission395/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068279766,
                "mdate": 1754869467141,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "06YyNxzwae",
                "forum": "06YyNxzwae",
                "content": {
                    "title": {
                        "value": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference"
                    },
                    "authors": {
                        "value": [
                            "Yixuan Huang",
                            "Novella Alvina",
                            "Mohanraj Devendran Shanthi",
                            "Tucker Hermans"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yixuan_Huang1",
                            "~Novella_Alvina1",
                            "~Mohanraj_Devendran_Shanthi1",
                            "~Tucker_Hermans2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learning from failures",
                            "Variational inference",
                            "Skill effect models"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose learning from failures by using Stein variational inference to generate diverse, but focused datasets around the failure cases to solve diverse real-world manipulation tasks."
                    },
                    "abstract": {
                        "value": "Skill effect models for long-horizon manipulation tasks are prone to failures in conditions not covered by training data distributions. Therefore, enabling robots to reason about and learn from failures is necessary. We investigate the problem of efficiently generating a dataset targeted to observed failures. After fine-tuning a skill effect model on this dataset, we evaluate the extent to which the model can recover from failures and minimize future failures. We propose Fail2Progress, an approach that leverages Stein variational inference to generate multiple simulation environments in parallel, enabling efficient data sample generation similar to observed failures. Our method is capable of handling several challenging mobile manipulation tasks, including transporting multiple objects, organizing a constrained shelf, and tabletop organization. Through large-scale simulation and real-world experiments, we demonstrate that our approach excels at learning from failures across different numbers of objects. Furthermore, we show that Fail2Progress outperforms several baselines."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9199a57ac435822efe500ae2fc40af4e41be0480.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9f6070edb3b86d81b688d55981467e6be8a14266.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhuang2025failprogress,\ntitle={Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference},\nauthor={Yixuan Huang and Novella Alvina and Mohanraj Devendran Shanthi and Tucker Hermans},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=06YyNxzwae}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f323cfa8ab9e34ae198ba0edc12a047efeca5616.mp4"
                    },
                    "paperhash": {
                        "value": "huang|fail2progress_learning_from_realworld_robot_failures_with_stein_variational_inference"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission395/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission395/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission395/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745338285755,
                "pdate": 1754680618593,
                "odate": 1758062767985,
                "mdate": 1758062814549,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission395/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jU7AbGq3se",
        "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning",
        "abstract": "Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior---an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies---a state-of-the-art BC methodology---we propose *diffusion steering via reinforcement learning* (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.",
        "keywords": [
            "diffusion policies",
            "reinforcement learning",
            "finetuning"
        ],
        "pdf_url": "https://openreview.net/pdf/9dd5ba8ff737a825608f74b01066b4dd24579f83.pdf",
        "reviews": [
            {
                "id": "7u1WDbpbw8",
                "forum": "jU7AbGq3se",
                "replyto": "jU7AbGq3se",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission381/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068277439,
                "mdate": 1754869456384,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jU7AbGq3se",
                "forum": "jU7AbGq3se",
                "content": {
                    "title": {
                        "value": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Andrew Wagenmaker",
                            "Yunchu Zhang",
                            "Mitsuhiko Nakamoto",
                            "Seohong Park",
                            "Waleed Yagoub",
                            "Anusha Nagabandi",
                            "Abhishek Gupta",
                            "Sergey Levine"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Andrew_Wagenmaker1",
                            "~Yunchu_Zhang1",
                            "~Mitsuhiko_Nakamoto1",
                            "~Seohong_Park1",
                            "~Waleed_Yagoub1",
                            "~Anusha_Nagabandi1",
                            "~Abhishek_Gupta1",
                            "~Sergey_Levine1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "diffusion policies",
                            "reinforcement learning",
                            "finetuning"
                        ]
                    },
                    "TLDR": {
                        "value": "We show that diffusion policies can be steered in a highly efficient manner by running RL over the initial noise sampled for the reverse process."
                    },
                    "abstract": {
                        "value": "Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior---an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies---a state-of-the-art BC methodology---we propose *diffusion steering via reinforcement learning* (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/9dd5ba8ff737a825608f74b01066b4dd24579f83.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwagenmaker2025steering,\ntitle={Steering Your Diffusion Policy with Latent Space Reinforcement Learning},\nauthor={Andrew Wagenmaker and Yunchu Zhang and Mitsuhiko Nakamoto and Seohong Park and Waleed Yagoub and Anusha Nagabandi and Abhishek Gupta and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jU7AbGq3se}\n}"
                    },
                    "paperhash": {
                        "value": "wagenmaker|steering_your_diffusion_policy_with_latent_space_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission381/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission381/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission381/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745331319788,
                "pdate": 1754680617774,
                "odate": 1758062767209,
                "mdate": 1758062814426,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission381/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission381/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "kXhOmN3x18",
        "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models",
        "abstract": "Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies.\nIn real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7\\% success rate while requiring only 10-20 demonstrations --- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. \nAdditional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.",
        "keywords": [
            "Robotic manipulation",
            "Imitation learning",
            "Few-shot learning"
        ],
        "pdf_url": "https://openreview.net/pdf/83f10da483b0dd723ff44a709f36a572b7bfaca1.pdf",
        "reviews": [
            {
                "id": "btiUx81AP2",
                "forum": "kXhOmN3x18",
                "replyto": "kXhOmN3x18",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission379/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068277238,
                "mdate": 1754869467044,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "kXhOmN3x18",
                "forum": "kXhOmN3x18",
                "content": {
                    "title": {
                        "value": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models"
                    },
                    "authors": {
                        "value": [
                            "Puhao Li",
                            "Yingying Wu",
                            "Ziheng Xi",
                            "Wanlin Li",
                            "Yuzhe Huang",
                            "Zhiyuan Zhang",
                            "Yinghan Chen",
                            "Jianan Wang",
                            "Song-Chun Zhu",
                            "Tengyu Liu",
                            "Siyuan Huang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Puhao_Li1",
                            "~Yingying_Wu2",
                            "~Ziheng_Xi1",
                            "~Wanlin_Li1",
                            "~Yuzhe_Huang3",
                            "~Zhiyuan_Zhang5",
                            "~Yinghan_Chen2",
                            "~Jianan_Wang2",
                            "~Song-Chun_Zhu1",
                            "~Tengyu_Liu1",
                            "~Siyuan_Huang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robotic manipulation",
                            "Imitation learning",
                            "Few-shot learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose ControlVLA, a novel manipulation learning framework that fine-tunes tasks using only 10-20 demonstrations in real-world."
                    },
                    "abstract": {
                        "value": "Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies.\nIn real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7\\% success rate while requiring only 10-20 demonstrations --- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. \nAdditional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ae304f4d0854db1a3d5a574b5a00a43c873c92fb.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/83f10da483b0dd723ff44a709f36a572b7bfaca1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025controlvla,\ntitle={Control{VLA}: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models},\nauthor={Puhao Li and Yingying Wu and Ziheng Xi and Wanlin Li and Yuzhe Huang and Zhiyuan Zhang and Yinghan Chen and Jianan Wang and Song-Chun Zhu and Tengyu Liu and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kXhOmN3x18}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a1823bf625e00324c6f7f6e15d814d1b50ef4274.zip"
                    },
                    "paperhash": {
                        "value": "li|controlvla_fewshot_objectcentric_adaptation_for_pretrained_visionlanguageaction_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission379/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission379/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission379/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745330542696,
                "pdate": 1754680617578,
                "odate": 1758062767155,
                "mdate": 1758062814443,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission379/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Q0H9xlNdVm",
        "title": "SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation",
        "abstract": "Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects.  To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 11.1\\% increase in success rate and a 18.9\\% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5\\%.",
        "keywords": [
            "Bimanual manipulation",
            "diffusion policy",
            "trajectory optimization"
        ],
        "pdf_url": "https://openreview.net/pdf/610a05a497c095fad609967bbfcf021ca7787961.pdf",
        "reviews": [
            {
                "id": "7e7JNeV7L3",
                "forum": "Q0H9xlNdVm",
                "replyto": "Q0H9xlNdVm",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission372/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070537209,
                "mdate": 1754869466976,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Q0H9xlNdVm",
                "forum": "Q0H9xlNdVm",
                "content": {
                    "title": {
                        "value": "SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation"
                    },
                    "authors": {
                        "value": [
                            "Haoyuan Deng",
                            "Wenkai Guo",
                            "Qianzhun Wang",
                            "Zhenyu Wu",
                            "Ziwei Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haoyuan_Deng1",
                            "~Wenkai_Guo2",
                            "~Qianzhun_Wang1",
                            "~Zhenyu_Wu6",
                            "~Ziwei_Wang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Bimanual manipulation",
                            "diffusion policy",
                            "trajectory optimization"
                        ]
                    },
                    "abstract": {
                        "value": "Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects.  To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 11.1\\% increase in success rate and a 18.9\\% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5\\%."
                    },
                    "supplementary_material": {
                        "value": "/attachment/3760de4076abb4a60d07b8454a9209f5120b75f9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/610a05a497c095fad609967bbfcf021ca7787961.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndeng2025safebimanual,\ntitle={SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation},\nauthor={Haoyuan Deng and Wenkai Guo and Qianzhun Wang and Zhenyu Wu and Ziwei Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Q0H9xlNdVm}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b512bfef2579d9a712941b1465bd631b0fd7a913.zip"
                    },
                    "paperhash": {
                        "value": "deng|safebimanual_diffusionbased_trajectory_optimization_for_safe_bimanual_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission372/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission372/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/Submission372/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745328174136,
                "pdate": 1754680617194,
                "odate": 1758062766875,
                "mdate": 1758062814268,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission372/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "NtnPVwUCAH",
        "title": "$Door(s)$: Junction State Estimation for Efficient Exploration in Reinforcement Learning",
        "abstract": "Exploration is one of the important bottlenecks for efficient learning in reinforcement learning, especially in the presence of sparse rewards. One way to traverse the environment faster is by passing through junctions, or metaphorical doors, in the state space. We propose a novel heuristic, $Door(s)$, focused on such narrow passages that serve as pathways to a large number of other states. Our approach works by estimating the state occupancy distribution and allows computation of its entropy, which forms the basis for our measure. Its computation is more sample-efficient compared to other similar methods and robustly works over longer horizons. Our results highlight the detection of dead-end states, show increased exploration efficiency, and demonstrate that $Door(s)$ encodes specific behaviors useful for downstream learning of various robotic manipulation tasks.",
        "keywords": [
            "Reinforcement learning",
            "Intrinsic motivation",
            "Junction States",
            "Information theory",
            "Heuristic",
            "Exploration"
        ],
        "pdf_url": "https://openreview.net/pdf/75ff4b72b7deee1ac89955013d3c0defe8a76bb0.pdf",
        "reviews": [
            {
                "id": "0mGMwNyrq9",
                "forum": "NtnPVwUCAH",
                "replyto": "NtnPVwUCAH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission362/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068276471,
                "mdate": 1754869466801,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "NtnPVwUCAH",
                "forum": "NtnPVwUCAH",
                "content": {
                    "title": {
                        "value": "$Door(s)$: Junction State Estimation for Efficient Exploration in Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Benjamin Fele",
                            "Jan Babic"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Benjamin_Fele1",
                            "~Jan_Babic1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement learning",
                            "Intrinsic motivation",
                            "Junction States",
                            "Information theory",
                            "Heuristic",
                            "Exploration"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a junction state estimation heuristic that is computationally efficient, increases exploration efficiency and helps in learning robotic manipulation downstream tasks."
                    },
                    "abstract": {
                        "value": "Exploration is one of the important bottlenecks for efficient learning in reinforcement learning, especially in the presence of sparse rewards. One way to traverse the environment faster is by passing through junctions, or metaphorical doors, in the state space. We propose a novel heuristic, $Door(s)$, focused on such narrow passages that serve as pathways to a large number of other states. Our approach works by estimating the state occupancy distribution and allows computation of its entropy, which forms the basis for our measure. Its computation is more sample-efficient compared to other similar methods and robustly works over longer horizons. Our results highlight the detection of dead-end states, show increased exploration efficiency, and demonstrate that $Door(s)$ encodes specific behaviors useful for downstream learning of various robotic manipulation tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ccadef649d2b4bcd46d5252e084789a032752277.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/75ff4b72b7deee1ac89955013d3c0defe8a76bb0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfele2025doors,\ntitle={\\$Door(s)\\$: Junction State Estimation for Efficient Exploration in Reinforcement Learning},\nauthor={Benjamin Fele and Jan Babic},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=NtnPVwUCAH}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/aef043b9eefdf906db53fac4336c228768c5cf98.zip"
                    },
                    "paperhash": {
                        "value": "fele|doors_junction_state_estimation_for_efficient_exploration_in_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission362/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission362/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745325948813,
                "pdate": 1754680616795,
                "odate": 1758062766567,
                "mdate": 1758062814172,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission362/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "najxw1MlZH",
        "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
        "abstract": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data — caused by inherent kinematic constraints and complex sensor calibration challenges — fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer—the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE) – a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available.",
        "keywords": [
            "Panoramic Video Generation",
            "World Model",
            "Quadruped Robots"
        ],
        "pdf_url": "https://openreview.net/pdf/94614faf34bd0f14555dffce6aab801e0f275432.pdf",
        "reviews": [
            {
                "id": "WUM5rtRor0",
                "forum": "najxw1MlZH",
                "replyto": "najxw1MlZH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission361/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068276444,
                "mdate": 1754869456305,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "najxw1MlZH",
                "forum": "najxw1MlZH",
                "content": {
                    "title": {
                        "value": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots"
                    },
                    "authors": {
                        "value": [
                            "Sheng Wu",
                            "Fei Teng",
                            "Hao Shi",
                            "Qi Jiang",
                            "Kai Luo",
                            "Kaiwei Wang",
                            "Kailun Yang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sheng_Wu4",
                            "~Fei_Teng11",
                            "~Hao_Shi2",
                            "~Qi_Jiang2",
                            "~Kai_Luo3",
                            "~Kaiwei_Wang1",
                            "~Kailun_Yang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Panoramic Video Generation",
                            "World Model",
                            "Quadruped Robots"
                        ]
                    },
                    "abstract": {
                        "value": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data — caused by inherent kinematic constraints and complex sensor calibration challenges — fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer—the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE) – a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available."
                    },
                    "supplementary_material": {
                        "value": "/attachment/8b51841ea2ce6216a2b6f07190900f120464828e.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "QuaDreamer—the first panoramic data generation engine specifically designed for quadruped robots."
                    },
                    "pdf": {
                        "value": "/pdf/94614faf34bd0f14555dffce6aab801e0f275432.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwu2025quadreamer,\ntitle={QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots},\nauthor={Sheng Wu and Fei Teng and Hao Shi and Qi Jiang and Kai Luo and Kaiwei Wang and Kailun Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=najxw1MlZH}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/da5d746f5188eb97ea2eb2b1b8fccbdc0d8d1bfc.zip"
                    },
                    "paperhash": {
                        "value": "wu|quadreamer_controllable_panoramic_video_generation_for_quadruped_robots"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission361/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission361/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission361/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745325666959,
                "pdate": 1754680616722,
                "odate": 1758062766515,
                "mdate": 1758062814172,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission361/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "xpEjjGC82v",
        "title": "COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping",
        "abstract": "This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Existing RL methods struggle with task complexity, and collecting expert demonstrations is often impractical. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), an approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination, where gradients from a jointly trained value function refine the constraint policy during RL training to improve bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy vision-based policies in real-world environments. Experiments show that COMBO-Grasp significantly outperforms baselines and generalises to unseen objects in both simulation and real environments.",
        "keywords": [
            "Occluded Grasping",
            "Bimanual Manipulation",
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/674c044587ecec92de7e3a0005928e46714c890f.pdf",
        "reviews": [
            {
                "id": "Wg2JJXy3I0",
                "forum": "xpEjjGC82v",
                "replyto": "xpEjjGC82v",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission356/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068276105,
                "mdate": 1754869466740,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "xpEjjGC82v",
                "forum": "xpEjjGC82v",
                "content": {
                    "title": {
                        "value": "COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping"
                    },
                    "authors": {
                        "value": [
                            "Jun Yamada",
                            "Alexander Luis Mitchell",
                            "Jack Collins",
                            "Ingmar Posner"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jun_Yamada1",
                            "~Alexander_Luis_Mitchell1",
                            "~Jack_Collins2",
                            "~Ingmar_Posner1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Occluded Grasping",
                            "Bimanual Manipulation",
                            "Reinforcement Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose COMBO-Grasp, a bimanual learning-based framework combining self-supervised and RL policies to tackle occluded robotic grasping tasks."
                    },
                    "abstract": {
                        "value": "This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Existing RL methods struggle with task complexity, and collecting expert demonstrations is often impractical. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), an approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination, where gradients from a jointly trained value function refine the constraint policy during RL training to improve bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy vision-based policies in real-world environments. Experiments show that COMBO-Grasp significantly outperforms baselines and generalises to unseen objects in both simulation and real environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c598daa720fc8ea1a8033ade77bbff73f2bd5478.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/674c044587ecec92de7e3a0005928e46714c890f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyamada2025combograsp,\ntitle={{COMBO}-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping},\nauthor={Jun Yamada and Alexander Luis Mitchell and Jack Collins and Ingmar Posner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xpEjjGC82v}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b7ec24cc33c5fc475ce4cfd4d7b2c1fa75ed3ad5.zip"
                    },
                    "paperhash": {
                        "value": "yamada|combograsp_learning_constraintbased_manipulation_for_bimanual_occluded_grasping"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission356/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission356/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission356/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745322297177,
                "pdate": 1754680616444,
                "odate": 1758062766298,
                "mdate": 1758062813974,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission356/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission356/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "5htQM8jqOe",
        "title": "D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation",
        "abstract": "Mastering deformable object manipulation often necessitates the use of anthropomorphic, high-degree-of-freedom robot hands capable of precise, contact-rich control. However, current trajectory optimisation methods often struggle in these settings due to the large search space and the sparse task information available from shape-matching cost functions, particularly when contact is absent. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions from a play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a published benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin.",
        "keywords": [
            "Trajectory Optimisation",
            "Dexterous Deformable Object Manipulation",
            "Latent Diffusion Model",
            "Gradient-Free Guidance"
        ],
        "pdf_url": "https://openreview.net/pdf/671489ac676e6d3070c0ce28850f17397167a889.pdf",
        "reviews": [
            {
                "id": "pznICawgkm",
                "forum": "5htQM8jqOe",
                "replyto": "5htQM8jqOe",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission355/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068276105,
                "mdate": 1754869466662,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "5htQM8jqOe",
                "forum": "5htQM8jqOe",
                "content": {
                    "title": {
                        "value": "D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Jun Yamada",
                            "Shaohong Zhong",
                            "Jack Collins",
                            "Ingmar Posner"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jun_Yamada1",
                            "~Shaohong_Zhong1",
                            "~Jack_Collins2",
                            "~Ingmar_Posner1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Trajectory Optimisation",
                            "Dexterous Deformable Object Manipulation",
                            "Latent Diffusion Model",
                            "Gradient-Free Guidance"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a trajectory optimisation method using a gradient-free guided sampling within the reverse process of skill-latent diffusion models to solve dexterous deformable object manipulation tasks."
                    },
                    "abstract": {
                        "value": "Mastering deformable object manipulation often necessitates the use of anthropomorphic, high-degree-of-freedom robot hands capable of precise, contact-rich control. However, current trajectory optimisation methods often struggle in these settings due to the large search space and the sparse task information available from shape-matching cost functions, particularly when contact is absent. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions from a play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a published benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9ea2b5298e26bd3ea4abbbbc464a87d7aef35d82.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/671489ac676e6d3070c0ce28850f17397167a889.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyamada2025dcubed,\ntitle={D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation},\nauthor={Jun Yamada and Shaohong Zhong and Jack Collins and Ingmar Posner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=5htQM8jqOe}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d8d8678d606aea02cd68d9149070bfc8d50fab5f.zip"
                    },
                    "paperhash": {
                        "value": "yamada|dcubed_latent_diffusion_trajectory_optimisation_for_dexterous_deformable_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission355/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission355/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission355/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745321553966,
                "pdate": 1754680616391,
                "odate": 1758062766233,
                "mdate": 1758062813858,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission355/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission355/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "irh5o90Mj1",
        "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
        "abstract": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.",
        "keywords": [
            "VLA; Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/ee7cfcafd6f72e13184d9a54a39ba51344f125ad.pdf",
        "reviews": [
            {
                "id": "QVU3kgtdGA",
                "forum": "irh5o90Mj1",
                "replyto": "irh5o90Mj1",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission348/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068275740,
                "mdate": 1754869466548,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "irh5o90Mj1",
                "forum": "irh5o90Mj1",
                "content": {
                    "title": {
                        "value": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yiguo Fan",
                            "Shuanghao Bai",
                            "Xinyang Tong",
                            "Pengxiang Ding",
                            "Yuyang Zhu",
                            "Hongchao Lu",
                            "Fengqi Dai",
                            "Wei Zhao",
                            "Yang Liu",
                            "Siteng Huang",
                            "Zhaoxin Fan",
                            "Badong Chen",
                            "Donglin Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yiguo_Fan2",
                            "~Shuanghao_Bai1",
                            "~Xinyang_Tong1",
                            "~Pengxiang_Ding1",
                            "~Yuyang_Zhu2",
                            "~Hongchao_Lu2",
                            "~Fengqi_Dai1",
                            "~Wei_Zhao21",
                            "~Yang_Liu97",
                            "~Siteng_Huang1",
                            "~Zhaoxin_Fan1",
                            "~Badong_Chen1",
                            "~Donglin_Wang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "VLA; Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b94e0738e19a0865d994c989069bf3af79870935.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/ee7cfcafd6f72e13184d9a54a39ba51344f125ad.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfan2025longvla,\ntitle={Long-{VLA}: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation},\nauthor={Yiguo Fan and Shuanghao Bai and Xinyang Tong and Pengxiang Ding and Yuyang Zhu and Hongchao Lu and Fengqi Dai and Wei Zhao and Yang Liu and Siteng Huang and Zhaoxin Fan and Badong Chen and Donglin Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=irh5o90Mj1}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/3cbea862f57eebc63c91063c5ee22b57bf486e06.zip"
                    },
                    "paperhash": {
                        "value": "fan|longvla_unleashing_longhorizon_capability_of_vision_language_action_model_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission348/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission348/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission348/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745315590157,
                "pdate": 1754680616162,
                "odate": 1758062766019,
                "mdate": 1758062813855,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission348/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission348/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "O2RGlBfR9H",
        "title": "ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models",
        "abstract": "LiDAR Place Recognition (LPR) is a key component in robotic localization, enabling robots to align current scans with prior maps of their environment. While Visual Place Recognition (VPR) has embraced Vision Foundation Models (VFMs) to enhance descriptor robustness, LPR has relied on task-specific models with limited use of pre-trained foundation-level knowledge. This is due to the lack of 3D foundation models and the challenges of using VFM with LiDAR point clouds. To tackle this, we introduce ImLPR, a novel pipeline that employs a pre-trained DINOv2 VFM to generate rich descriptors for LPR. To our knowledge, ImLPR is the first method to leverage a VFM to support LPR. ImLPR converts raw point clouds into Range Image Views (RIV) to leverage VFM in the LiDAR domain. It employs MultiConv adapters and Patch-InfoNCE loss for effective feature learning. We validate ImLPR using public datasets where it outperforms state-of-the-art (SOTA) methods in intra-session and inter-session LPR with top Recall@1 and F1 scores across various LiDARs. We also demonstrate that RIV outperforms Bird’s-Eye-View (BEV) as a representation choice for adapting LiDAR for VFM. We release ImLPR as open source for the robotics community.",
        "keywords": [
            "LiDAR Place Recognition",
            "Deep Learning",
            "Vision Foundation Model"
        ],
        "pdf_url": "https://openreview.net/pdf/2dead8b23337a37cfadfed9c6440f1cf6efb7d42.pdf",
        "reviews": [
            {
                "id": "qUBol82mHJ",
                "forum": "O2RGlBfR9H",
                "replyto": "O2RGlBfR9H",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission340/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068275349,
                "mdate": 1754869466477,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "O2RGlBfR9H",
                "forum": "O2RGlBfR9H",
                "content": {
                    "title": {
                        "value": "ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models"
                    },
                    "authors": {
                        "value": [
                            "Minwoo Jung",
                            "Lanke Frank Tarimo Fu",
                            "Maurice Fallon",
                            "Ayoung Kim"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Minwoo_Jung1",
                            "~Lanke_Frank_Tarimo_Fu1",
                            "~Maurice_Fallon1",
                            "~Ayoung_Kim1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "LiDAR Place Recognition",
                            "Deep Learning",
                            "Vision Foundation Model"
                        ]
                    },
                    "TLDR": {
                        "value": "Range Image Views for LiDAR Place Recognition with a vision foundation model"
                    },
                    "abstract": {
                        "value": "LiDAR Place Recognition (LPR) is a key component in robotic localization, enabling robots to align current scans with prior maps of their environment. While Visual Place Recognition (VPR) has embraced Vision Foundation Models (VFMs) to enhance descriptor robustness, LPR has relied on task-specific models with limited use of pre-trained foundation-level knowledge. This is due to the lack of 3D foundation models and the challenges of using VFM with LiDAR point clouds. To tackle this, we introduce ImLPR, a novel pipeline that employs a pre-trained DINOv2 VFM to generate rich descriptors for LPR. To our knowledge, ImLPR is the first method to leverage a VFM to support LPR. ImLPR converts raw point clouds into Range Image Views (RIV) to leverage VFM in the LiDAR domain. It employs MultiConv adapters and Patch-InfoNCE loss for effective feature learning. We validate ImLPR using public datasets where it outperforms state-of-the-art (SOTA) methods in intra-session and inter-session LPR with top Recall@1 and F1 scores across various LiDARs. We also demonstrate that RIV outperforms Bird’s-Eye-View (BEV) as a representation choice for adapting LiDAR for VFM. We release ImLPR as open source for the robotics community."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2dead8b23337a37cfadfed9c6440f1cf6efb7d42.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njung2025imlpr,\ntitle={Im{LPR}: Image-based Li{DAR} Place Recognition using Vision Foundation Models},\nauthor={Minwoo Jung and Lanke Frank Tarimo Fu and Maurice Fallon and Ayoung Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=O2RGlBfR9H}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/bfcbcadaae5339846f6045e0329bbca74f81e2d3.zip"
                    },
                    "supplementary_material": {
                        "value": "/attachment/a9fd7a34fe7ec49c151ec29b70ece3cd6c9c2034.zip"
                    },
                    "paperhash": {
                        "value": "jung|imlpr_imagebased_lidar_place_recognition_using_vision_foundation_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission340/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission340/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission340/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745311004691,
                "pdate": 1754680615715,
                "odate": 1758062765615,
                "mdate": 1758062813696,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission340/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission340/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Th1kFSnjUW",
        "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation",
        "abstract": "Mobile manipulation is the fundamental challenge for robotics in assisting humans with diverse tasks and environments in everyday life.\nConventional mobile manipulation approaches often struggle to generalize across different tasks and environments due to the lack of large-scale training. However, recent advances in manipulation foundation models demonstrate impressive generalization capability on a wide range of fixed-base manipulation tasks, which are still limited to a fixed setting. Therefore, we devise a plug-in module named MoTo, which can be combined with any off-the-shelf manipulation foundation model to empower them with mobile manipulation ability. Specifically, we propose an interaction-aware navigation policy to generate agent docking points for generalized mobile manipulation.  To enable zero-shot ability, we propose an interaction keypoints framework via vision-language models (VLM) under multi-view consistency for both target object and robotic arm following instructions, where fixed-base manipulation foundation models can be employed. We further propose motion planning objectives for the mobile base and robot arm, which minimize the distance between the two keypoints and maintain the physical feasibility of trajectories. In this way, MoTo guides the agent to move to the docking points where fixed-base manipulation can be successfully performed, and leverages VLM generation and trajectory optimization to achieve mobile manipulation in a zero-shot manner, without any requirement on mobile manipulation expert data. Extensive experimental results on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68% and 16.67% higher than the state-of-the-art mobile manipulation methods, respectively, without requiring additional training data.",
        "keywords": [
            "Mobile Manipulation",
            "VLA",
            "VLM"
        ],
        "pdf_url": "https://openreview.net/pdf/e9dfbb4abd3a0fc395390128c2d0c1ae7cbbab1a.pdf",
        "reviews": [
            {
                "id": "t5Z2mbM8s6",
                "forum": "Th1kFSnjUW",
                "replyto": "Th1kFSnjUW",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission338/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068275137,
                "mdate": 1754869466416,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Th1kFSnjUW",
                "forum": "Th1kFSnjUW",
                "content": {
                    "title": {
                        "value": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Zhenyu Wu",
                            "Angyuan Ma",
                            "Xiuwei Xu",
                            "Hang Yin",
                            "Yinan Liang",
                            "Ziwei Wang",
                            "Jiwen Lu",
                            "Haibin Yan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhenyu_Wu6",
                            "~Angyuan_Ma1",
                            "~Xiuwei_Xu1",
                            "~Hang_Yin5",
                            "~Yinan_Liang1",
                            "~Ziwei_Wang2",
                            "~Jiwen_Lu1",
                            "~Haibin_Yan1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Mobile Manipulation",
                            "VLA",
                            "VLM"
                        ]
                    },
                    "abstract": {
                        "value": "Mobile manipulation is the fundamental challenge for robotics in assisting humans with diverse tasks and environments in everyday life.\nConventional mobile manipulation approaches often struggle to generalize across different tasks and environments due to the lack of large-scale training. However, recent advances in manipulation foundation models demonstrate impressive generalization capability on a wide range of fixed-base manipulation tasks, which are still limited to a fixed setting. Therefore, we devise a plug-in module named MoTo, which can be combined with any off-the-shelf manipulation foundation model to empower them with mobile manipulation ability. Specifically, we propose an interaction-aware navigation policy to generate agent docking points for generalized mobile manipulation.  To enable zero-shot ability, we propose an interaction keypoints framework via vision-language models (VLM) under multi-view consistency for both target object and robotic arm following instructions, where fixed-base manipulation foundation models can be employed. We further propose motion planning objectives for the mobile base and robot arm, which minimize the distance between the two keypoints and maintain the physical feasibility of trajectories. In this way, MoTo guides the agent to move to the docking points where fixed-base manipulation can be successfully performed, and leverages VLM generation and trajectory optimization to achieve mobile manipulation in a zero-shot manner, without any requirement on mobile manipulation expert data. Extensive experimental results on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68% and 16.67% higher than the state-of-the-art mobile manipulation methods, respectively, without requiring additional training data."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d5a9640c5a164ce32dbc5fa0c72675250f44a8f1.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/e9dfbb4abd3a0fc395390128c2d0c1ae7cbbab1a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwu2025moto,\ntitle={MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation},\nauthor={Zhenyu Wu and Angyuan Ma and Xiuwei Xu and Hang Yin and Yinan Liang and Ziwei Wang and Jiwen Lu and Haibin Yan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Th1kFSnjUW}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6d8f6b07036ca486def64424d90aa97c5255ebf6.mp4"
                    },
                    "paperhash": {
                        "value": "wu|moto_a_zeroshot_plugin_interactionaware_navigation_for_general_mobile_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission338/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission338/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission338/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745309789479,
                "pdate": 1754680615587,
                "odate": 1758062765527,
                "mdate": 1758062813549,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission338/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission338/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Ggu7Hh2xnn",
        "title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning",
        "abstract": "Future robotic systems operating in real-world environments require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of small\nparameter-count Large Language Models (LLMs) in the robotic domain. The\nR1-Zero approach was originally developed to enable mathematical reasoning in\nLLMs using static datasets. We extend it to the robotics domain through integration with a closed-loop Reinforcement Learning (RL) framework. This extension\nallows reasoning in Embodied Artificial Intelligence (EmbodiedAI) settings without relying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which\nenables tasks that previously required significantly larger models. A performance\ngain of 20.2% points over the SFT-based baseline is observed with a Qwen2.5-1.5B\nmodel. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3%\ncontrol adaptability score, surpassing the 58.5% obtained by the much larger,\ncloud-bound GPT-4o. These results highlight that practical, on-board deployment\nof small LLMs is not only feasible but can outperform larger models when trained\nthrough environmental interaction, underscoring the importance of an interactive,\nembodied learning framework for robotic EmbodiedAI — one grounded in practical experience rather than static supervision.",
        "keywords": [
            "Large Language Models",
            "Reinforcement Learning",
            "Embodied AI",
            "Constrained Hardware"
        ],
        "pdf_url": "https://openreview.net/pdf/b16ef4e381dd0434a584660729bbc04a02f6b7cd.pdf",
        "reviews": [
            {
                "id": "9ayRL3Bt6X",
                "forum": "Ggu7Hh2xnn",
                "replyto": "Ggu7Hh2xnn",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission325/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068274119,
                "mdate": 1754869466225,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Ggu7Hh2xnn",
                "forum": "Ggu7Hh2xnn",
                "content": {
                    "title": {
                        "value": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning"
                    },
                    "authors": {
                        "value": [
                            "Liam Boyle",
                            "Nicolas Baumann",
                            "Paviththiren Sivasothilingam",
                            "Michele Magno",
                            "Luca Benini"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "boylel@ethz.ch",
                            "~Nicolas_Baumann1",
                            "~Paviththiren_Sivasothilingam1",
                            "~Michele_Magno1",
                            "~Luca_Benini2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Large Language Models",
                            "Reinforcement Learning",
                            "Embodied AI",
                            "Constrained Hardware"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper extends R1-zero to embodied AI by training small LLMs via closed-loop RL for autonomous driving, enabling edge deployment with reasoning and adaptability once limited to significantly larger models."
                    },
                    "abstract": {
                        "value": "Future robotic systems operating in real-world environments require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of small\nparameter-count Large Language Models (LLMs) in the robotic domain. The\nR1-Zero approach was originally developed to enable mathematical reasoning in\nLLMs using static datasets. We extend it to the robotics domain through integration with a closed-loop Reinforcement Learning (RL) framework. This extension\nallows reasoning in Embodied Artificial Intelligence (EmbodiedAI) settings without relying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which\nenables tasks that previously required significantly larger models. A performance\ngain of 20.2% points over the SFT-based baseline is observed with a Qwen2.5-1.5B\nmodel. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3%\ncontrol adaptability score, surpassing the 58.5% obtained by the much larger,\ncloud-bound GPT-4o. These results highlight that practical, on-board deployment\nof small LLMs is not only feasible but can outperform larger models when trained\nthrough environmental interaction, underscoring the importance of an interactive,\nembodied learning framework for robotic EmbodiedAI — one grounded in practical experience rather than static supervision."
                    },
                    "supplementary_material": {
                        "value": "/attachment/21038dc6c3784a4b5ca7168760179434e5fc0e7c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b16ef4e381dd0434a584660729bbc04a02f6b7cd.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nboyle2025robotxr,\ntitle={RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning},\nauthor={Liam Boyle and Nicolas Baumann and Paviththiren Sivasothilingam and Michele Magno and Luca Benini},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ggu7Hh2xnn}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/487a26558285b672e4aeffe424dd37a06d198a6e.zip"
                    },
                    "paperhash": {
                        "value": "boyle|robotxr1_enabling_embodied_robotic_intelligence_on_large_language_models_through_closedloop_reinforcement_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission325/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission325/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission325/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745303837303,
                "pdate": 1754680614962,
                "odate": 1758062764801,
                "mdate": 1758062813541,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission325/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "zEC8TOXDkH",
        "title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data",
        "abstract": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action (VLA) models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and a flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA’s advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.",
        "keywords": [
            "Vision-Language-Action",
            "Large-scale Robot Learning",
            "Grasping"
        ],
        "pdf_url": "https://openreview.net/pdf/aafa403ec79ccec3abaefcd949429c6fe4488acc.pdf",
        "reviews": [
            {
                "id": "pHm8koGRkD",
                "forum": "zEC8TOXDkH",
                "replyto": "zEC8TOXDkH",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission317/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068273909,
                "mdate": 1754869466179,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "zEC8TOXDkH",
                "forum": "zEC8TOXDkH",
                "content": {
                    "title": {
                        "value": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data"
                    },
                    "authors": {
                        "value": [
                            "Shengliang Deng",
                            "Mi Yan",
                            "Songlin Wei",
                            "Haixin Ma",
                            "Yuxin Yang",
                            "Jiayi Chen",
                            "Zhiqi Zhang",
                            "Taoyu Yang",
                            "Xuheng Zhang",
                            "Heming Cui",
                            "Zhizheng Zhang",
                            "He Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shengliang_Deng1",
                            "~Mi_Yan1",
                            "~Songlin_Wei1",
                            "~Haixin_Ma2",
                            "~Yuxin_Yang10",
                            "~Jiayi_Chen5",
                            "~Zhiqi_Zhang3",
                            "~Taoyu_Yang1",
                            "~Xuheng_Zhang1",
                            "~Heming_Cui1",
                            "~Zhizheng_Zhang1",
                            "~He_Wang5"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action",
                            "Large-scale Robot Learning",
                            "Grasping"
                        ]
                    },
                    "abstract": {
                        "value": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action (VLA) models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and a flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA’s advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community."
                    },
                    "supplementary_material": {
                        "value": "/attachment/65e5ddd97209836ad8daa18e6b7139b727ae0d75.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/aafa403ec79ccec3abaefcd949429c6fe4488acc.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndeng2025graspvla,\ntitle={Grasp{VLA}: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data},\nauthor={Shengliang Deng and Mi Yan and Songlin Wei and Haixin Ma and Yuxin Yang and Jiayi Chen and Zhiqi Zhang and Taoyu Yang and Xuheng Zhang and Heming Cui and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zEC8TOXDkH}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/37e6beebe89b92662efc3b16e5e7e5d916788601.zip"
                    },
                    "paperhash": {
                        "value": "deng|graspvla_a_grasping_foundation_model_pretrained_on_billionscale_synthetic_action_data"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission317/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission317/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission317/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745292472906,
                "pdate": 1754680614704,
                "odate": 1758062764513,
                "mdate": 1758062813350,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission317/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1K3kjo91Q1",
        "title": "Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames",
        "abstract": "Imitation learning has unlocked the potential for robots to exhibit highly dexterous behaviours. However, it still struggles with long-horizon, multi-object tasks due to poor sample efficiency and limited generalisation. Existing methods require a substantial number of demonstrations to cover possible task variations, making them costly and often impractical for real-world deployment. \nWe address this challenge by introducing \\emph{oriented affordance frames}, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data. Video demonstration can be found on our anonymised project page: https://affordance-policy.github.io/.",
        "keywords": [
            "behaviour cloning",
            "spatial generalisation",
            "intra-category generalisation",
            "long-horizon tasks",
            "affordances"
        ],
        "pdf_url": "https://openreview.net/pdf/fa4394a62ef9b3e130788e120410bc55f6863c71.pdf",
        "reviews": [
            {
                "id": "u7SiYJJ7sy",
                "forum": "1K3kjo91Q1",
                "replyto": "1K3kjo91Q1",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission316/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068273922,
                "mdate": 1754869466135,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1K3kjo91Q1",
                "forum": "1K3kjo91Q1",
                "content": {
                    "title": {
                        "value": "Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames"
                    },
                    "authors": {
                        "value": [
                            "Krishan Rana",
                            "Jad Abou-Chakra",
                            "Sourav Garg",
                            "Robert Lee",
                            "Ian Reid",
                            "Niko Suenderhauf"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Krishan_Rana1",
                            "~Jad_Abou-Chakra1",
                            "~Sourav_Garg1",
                            "~Robert_Lee1",
                            "~Ian_Reid1",
                            "~Niko_Suenderhauf1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "behaviour cloning",
                            "spatial generalisation",
                            "intra-category generalisation",
                            "long-horizon tasks",
                            "affordances"
                        ]
                    },
                    "TLDR": {
                        "value": "Learning spatial and intra-category invariant diffusion policies using oriented affordance task frames for compositional generalisation in long-horizon, multi-object tasks using only 10 demonstrations."
                    },
                    "abstract": {
                        "value": "Imitation learning has unlocked the potential for robots to exhibit highly dexterous behaviours. However, it still struggles with long-horizon, multi-object tasks due to poor sample efficiency and limited generalisation. Existing methods require a substantial number of demonstrations to cover possible task variations, making them costly and often impractical for real-world deployment. \nWe address this challenge by introducing \\emph{oriented affordance frames}, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data. Video demonstration can be found on our anonymised project page: https://affordance-policy.github.io/."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/fa4394a62ef9b3e130788e120410bc55f6863c71.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nrana2025learning,\ntitle={Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames},\nauthor={Krishan Rana and Jad Abou-Chakra and Sourav Garg and Robert Lee and Ian Reid and Niko Suenderhauf},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1K3kjo91Q1}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1cfe6a4f517ce98cbdb0a47769f377f0b7e01b79.zip"
                    },
                    "paperhash": {
                        "value": "rana|learning_from_10_demos_generalisable_and_sampleefficient_policy_learning_with_oriented_affordance_frames"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission316/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission316/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745291875280,
                "pdate": 1754680614636,
                "odate": 1758062764434,
                "mdate": 1758062813270,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission316/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission316/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "o0LBjJxUeS",
        "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
        "abstract": "Reasoning over long sequences of observations and actions is essential for many robotic tasks. \nYet, learning effective long-context policies from demonstrations remains challenging. \nAs context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations.\nRecent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions.\nIn this paper, we propose an alternative approach that explicitly regularizes the retention of past information.\nWe first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions.\nTo address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones.\nThis regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations.\nBuilding on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. \nThis strategy preserves the benefits of PTP while greatly reducing memory and computational overhead.\nFinally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference.\nExperiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3× and accelerates policy training by more than 10×.\nVideos are available at https://ptp-robot.github.io.",
        "keywords": [
            "history-conditioned policy learning"
        ],
        "pdf_url": "https://openreview.net/pdf/44308fed7b0274bc3dcda83088e65c20509c0cf1.pdf",
        "reviews": [
            {
                "id": "ScEu3yAz6s",
                "forum": "o0LBjJxUeS",
                "replyto": "o0LBjJxUeS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission300/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068272503,
                "mdate": 1754869465921,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "o0LBjJxUeS",
                "forum": "o0LBjJxUeS",
                "content": {
                    "title": {
                        "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                    },
                    "authors": {
                        "value": [
                            "Marcel Torne Villasevil",
                            "Andy Tang",
                            "Yuejiang Liu",
                            "Chelsea Finn"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Marcel_Torne_Villasevil1",
                            "~Andy_Tang1",
                            "~Yuejiang_Liu1",
                            "~Chelsea_Finn1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "history-conditioned policy learning"
                        ]
                    },
                    "abstract": {
                        "value": "Reasoning over long sequences of observations and actions is essential for many robotic tasks. \nYet, learning effective long-context policies from demonstrations remains challenging. \nAs context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations.\nRecent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions.\nIn this paper, we propose an alternative approach that explicitly regularizes the retention of past information.\nWe first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions.\nTo address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones.\nThis regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations.\nBuilding on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. \nThis strategy preserves the benefits of PTP while greatly reducing memory and computational overhead.\nFinally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference.\nExperiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3× and accelerates policy training by more than 10×.\nVideos are available at https://ptp-robot.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/cf0be1b1b46ef688873e211d1e65194e8476b254.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/44308fed7b0274bc3dcda83088e65c20509c0cf1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nvillasevil2025learning,\ntitle={Learning Long-Context Diffusion Policies via Past-Token Prediction},\nauthor={Marcel Torne Villasevil and Andy Tang and Yuejiang Liu and Chelsea Finn},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=o0LBjJxUeS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d7033114047355c776612cd12168148fd7e1668c.mp4"
                    },
                    "paperhash": {
                        "value": "villasevil|learning_longcontext_diffusion_policies_via_pasttoken_prediction"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission300/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission300/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission300/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745274839050,
                "pdate": 1754680613984,
                "odate": 1758062763596,
                "mdate": 1758062813234,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission300/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HprBJupvvM",
        "title": "Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation",
        "abstract": "How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. \\textit{Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video.} To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data.",
        "keywords": [
            "Robot Manipulation",
            "Human Video Generation",
            "Human Videos"
        ],
        "pdf_url": "https://openreview.net/pdf/172a473976930b5cc2213c2ce93fda6325dc9d9a.pdf",
        "reviews": [
            {
                "id": "z86Gcz3Ios",
                "forum": "HprBJupvvM",
                "replyto": "HprBJupvvM",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission295/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068272201,
                "mdate": 1754869465848,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HprBJupvvM",
                "forum": "HprBJupvvM",
                "content": {
                    "title": {
                        "value": "Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Homanga Bharadhwaj",
                            "Debidatta Dwibedi",
                            "Abhinav Gupta",
                            "Shubham Tulsiani",
                            "Carl Doersch",
                            "Ted Xiao",
                            "Dhruv Shah",
                            "Fei Xia",
                            "Dorsa Sadigh",
                            "Sean Kirmani"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Homanga_Bharadhwaj1",
                            "~Debidatta_Dwibedi1",
                            "~Abhinav_Gupta1",
                            "~Shubham_Tulsiani1",
                            "~Carl_Doersch1",
                            "~Ted_Xiao1",
                            "~Dhruv_Shah1",
                            "~Fei_Xia1",
                            "~Dorsa_Sadigh1",
                            "~Sean_Kirmani1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Manipulation",
                            "Human Video Generation",
                            "Human Videos"
                        ]
                    },
                    "TLDR": {
                        "value": "Casting language-conditioned manipulation as human video generation followed by closed-loop policy execution conditioned on the generated video enables solving diverse real-world tasks involving object/motion types unseen in the robot dataset."
                    },
                    "abstract": {
                        "value": "How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. \\textit{Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video.} To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data."
                    },
                    "supplementary_material": {
                        "value": "/attachment/4d7425e111afc38d718b2d47c9ee8bc884f0b91e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/172a473976930b5cc2213c2ce93fda6325dc9d9a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nbharadhwaj2025genact,\ntitle={Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation},\nauthor={Homanga Bharadhwaj and Debidatta Dwibedi and Abhinav Gupta and Shubham Tulsiani and Carl Doersch and Ted Xiao and Dhruv Shah and Fei Xia and Dorsa Sadigh and Sean Kirmani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HprBJupvvM}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/48b6833edf2d2ba84215140b0d391157077b53ef.mp4"
                    },
                    "paperhash": {
                        "value": "bharadhwaj|gen2act_human_video_generation_in_novel_scenarios_enables_generalizable_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission295/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission295/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission295/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745269440345,
                "pdate": 1754680613777,
                "odate": 1758062763295,
                "mdate": 1758062813071,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission295/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission295/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "BNCh3SS1Yl",
        "title": "Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling",
        "abstract": "3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate AnyMesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate AnyMesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system.",
        "keywords": [
            "Articulated objects",
            "visual prompting",
            "URDF prediction"
        ],
        "pdf_url": "https://openreview.net/pdf/0e2002bb4d50e56f6e3775306bec59b268d2ba6c.pdf",
        "reviews": [
            {
                "id": "6zJWCkhZCs",
                "forum": "BNCh3SS1Yl",
                "replyto": "BNCh3SS1Yl",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission293/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068272123,
                "mdate": 1754869465847,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "BNCh3SS1Yl",
                "forum": "BNCh3SS1Yl",
                "content": {
                    "title": {
                        "value": "Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling"
                    },
                    "authors": {
                        "value": [
                            "Xiaowen Qiu",
                            "Jincheng Yang",
                            "Yian Wang",
                            "Zhehuan Chen",
                            "Yufei Wang",
                            "Tsun-Hsuan Wang",
                            "Zhou Xian",
                            "Chuang Gan"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xiaowen_Qiu1",
                            "~Jincheng_Yang2",
                            "~Yian_Wang1",
                            "~Zhehuan_Chen1",
                            "~Yufei_Wang4",
                            "~Tsun-Hsuan_Wang2",
                            "~Zhou_Xian1",
                            "~Chuang_Gan1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Articulated objects",
                            "visual prompting",
                            "URDF prediction"
                        ]
                    },
                    "abstract": {
                        "value": "3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate AnyMesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate AnyMesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0e2002bb4d50e56f6e3775306bec59b268d2ba6c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nqiu2025articulate,\ntitle={Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling},\nauthor={Xiaowen Qiu and Jincheng Yang and Yian Wang and Zhehuan Chen and Yufei Wang and Tsun-Hsuan Wang and Zhou Xian and Chuang Gan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BNCh3SS1Yl}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d496a16f49d0e8dcafab45752df9c66f8969a3de.zip"
                    },
                    "paperhash": {
                        "value": "qiu|articulate_anymesh_openvocabulary_3d_articulated_objects_modeling"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission293/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission293/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745267350637,
                "pdate": 1754680613656,
                "odate": 1758062763179,
                "mdate": 1758062813018,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission293/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission293/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "BTUioBmCWo",
        "title": "Phantom: Training Robots Without Robots Using Only Human Videos",
        "abstract": "Training general-purpose robots requires learning from large and diverse data sources. Current approaches rely heavily on teleoperated demonstrations which are difficult to scale. We present a scalable framework for training manipulation policies directly from human video demonstrations, requiring no robot data. Our method converts human demonstrations into robot-compatible observation-action pairs using hand pose estimation and visual data editing. We inpaint the human arm and overlay a rendered robot to align the visual domains. This enables zero-shot deployment on real hardware without any fine-tuning. We demonstrate strong success rates—up to 92%—on a range of tasks including deformable object manipulation, multi-object sweeping, and insertion. Our approach generalizes to novel environments and supports closed-loop execution. By demonstrating that effective policies can be trained using only human videos, our method broadens the path to scalable robot learning. Videos are available at https://phantom-training-robots.github.io.",
        "keywords": [
            "Learning from Human Videos",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/65d7515fa095a1688a9c1fdad9b3143a28c23877.pdf",
        "reviews": [
            {
                "id": "pbCe62EEpU",
                "forum": "BTUioBmCWo",
                "replyto": "BTUioBmCWo",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission289/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271835,
                "mdate": 1754869465720,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "BTUioBmCWo",
                "forum": "BTUioBmCWo",
                "content": {
                    "title": {
                        "value": "Phantom: Training Robots Without Robots Using Only Human Videos"
                    },
                    "authors": {
                        "value": [
                            "Marion Lepert",
                            "Jiaying Fang",
                            "Jeannette Bohg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Marion_Lepert1",
                            "~Jiaying_Fang1",
                            "~Jeannette_Bohg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learning from Human Videos",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a scalable method for training robot manipulation policies directly from human videos—without any robot data—achieving zero-shot deployment and up to 92% success on diverse tasks."
                    },
                    "abstract": {
                        "value": "Training general-purpose robots requires learning from large and diverse data sources. Current approaches rely heavily on teleoperated demonstrations which are difficult to scale. We present a scalable framework for training manipulation policies directly from human video demonstrations, requiring no robot data. Our method converts human demonstrations into robot-compatible observation-action pairs using hand pose estimation and visual data editing. We inpaint the human arm and overlay a rendered robot to align the visual domains. This enables zero-shot deployment on real hardware without any fine-tuning. We demonstrate strong success rates—up to 92%—on a range of tasks including deformable object manipulation, multi-object sweeping, and insertion. Our approach generalizes to novel environments and supports closed-loop execution. By demonstrating that effective policies can be trained using only human videos, our method broadens the path to scalable robot learning. Videos are available at https://phantom-training-robots.github.io."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/65d7515fa095a1688a9c1fdad9b3143a28c23877.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlepert2025phantom,\ntitle={Phantom: Training Robots Without Robots Using Only Human Videos},\nauthor={Marion Lepert and Jiaying Fang and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BTUioBmCWo}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/e0038a74e0e26eebe1d2f348b0250263728cf0b3.mp4"
                    },
                    "paperhash": {
                        "value": "lepert|phantom_training_robots_without_robots_using_only_human_videos"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission289/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission289/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745260967383,
                "pdate": 1754680613322,
                "odate": 1758062763013,
                "mdate": 1758062812911,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission289/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "seSw6ssEid",
        "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments",
        "abstract": "In this paper, we propose a hybrid MPC local planner that uses a learning-based approximation of a time-varying safe set, derived from local observations and applied as the MPC terminal constraint. This set can be represented as a zero-superlevel set of the value function computed via Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time. We exploit the property that the HJ value function can be expressed as a difference of the corresponding signed distance function (SDF) and a non-negative residual function. The residual component is modeled as a neural network with non-negative output and subtracted from the computed SDF, resulting in a real-time value function estimate that is at least as safe as the SDF by design. Additionally, we parametrize the neural residual by a hypernetwork to improve real-time performance and generalization properties. The proposed method is compared with three state-of-the-art methods in simulations and hardware experiments, achieving up to 30\\% higher success rates compared to the best baseline while requiring a similar computational effort and producing high-quality (low travel-time) solutions.",
        "keywords": [
            "MPC",
            "Obstacle Avoidance",
            "Learning for Control"
        ],
        "pdf_url": "https://openreview.net/pdf/e380498975a5687ad2cc2a4649500d5e3ef892d8.pdf",
        "reviews": [
            {
                "id": "46CBvv4Dfo",
                "forum": "seSw6ssEid",
                "replyto": "seSw6ssEid",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission286/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271737,
                "mdate": 1754869456235,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "seSw6ssEid",
                "forum": "seSw6ssEid",
                "content": {
                    "title": {
                        "value": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments"
                    },
                    "authors": {
                        "value": [
                            "Bojan Derajic",
                            "Mohamed-Khalil Bouzidi",
                            "Sebastian Bernhard",
                            "Wolfgang Hönig"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Bojan_Derajic2",
                            "~Mohamed-Khalil_Bouzidi1",
                            "~Sebastian_Bernhard1",
                            "~Wolfgang_Hönig1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "MPC",
                            "Obstacle Avoidance",
                            "Learning for Control"
                        ]
                    },
                    "TLDR": {
                        "value": "A hybrid MPC local planner that uses a learning-based approximation of a time-varying safe set, derived from local observations and applied as the MPC terminal constraint."
                    },
                    "abstract": {
                        "value": "In this paper, we propose a hybrid MPC local planner that uses a learning-based approximation of a time-varying safe set, derived from local observations and applied as the MPC terminal constraint. This set can be represented as a zero-superlevel set of the value function computed via Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time. We exploit the property that the HJ value function can be expressed as a difference of the corresponding signed distance function (SDF) and a non-negative residual function. The residual component is modeled as a neural network with non-negative output and subtracted from the computed SDF, resulting in a real-time value function estimate that is at least as safe as the SDF by design. Additionally, we parametrize the neural residual by a hypernetwork to improve real-time performance and generalization properties. The proposed method is compared with three state-of-the-art methods in simulations and hardware experiments, achieving up to 30\\% higher success rates compared to the best baseline while requiring a similar computational effort and producing high-quality (low travel-time) solutions."
                    },
                    "supplementary_material": {
                        "value": "/attachment/0ca6f7206b7517109a52e64e06312bb473b6eadf.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/e380498975a5687ad2cc2a4649500d5e3ef892d8.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nderajic2025residual,\ntitle={Residual Neural Terminal Constraint for {MPC}-based Collision Avoidance in Dynamic Environments},\nauthor={Bojan Derajic and Mohamed-Khalil Bouzidi and Sebastian Bernhard and Wolfgang H{\\\"o}nig},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=seSw6ssEid}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6643e8206e903ea31e0abe3156034c06b895a2ba.zip"
                    },
                    "paperhash": {
                        "value": "derajic|residual_neural_terminal_constraint_for_mpcbased_collision_avoidance_in_dynamic_environments"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission286/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission286/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission286/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745255102847,
                "pdate": 1754680613255,
                "odate": 1758062762877,
                "mdate": 1758062812823,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission286/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission286/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Bl2VfU9NhF",
        "title": "Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control",
        "abstract": "Can your humanoid walk up and hand you a full cup of beer—without spilling a drop? While humanoids are increasingly featured in flashy demos—dancing, delivering packages, traversing rough terrain—fine-grained control during locomotion remains a significant challenge. In particular, stabilizing a filled end-effector (EE) while walking is far from solved, due to a fundamental mismatch in task dynamics: locomotion demands slow-timescale, robust control, whereas EE stabilization requires rapid, high-precision corrections. To address this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples upper-body and lower-body control into separate agents operating at different frequencies and with distinct rewards. This temporal and objective separation mitigates policy interference mitagates objective conflict and enables coordinated whole-body behavior. SoFTA executes upper-body actions at 100 Hz for precise EE control and lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x to baselines and performs 2–3x closer to human-level stability, enabling delicate tasks such as carrying nearly full cups, capturing steady video during locomotion, and disturbance rejection with EE stability.",
        "keywords": [
            "Humanoid Robots",
            "Reinforcement Learning",
            "Stable Locomotion"
        ],
        "pdf_url": "https://openreview.net/pdf/33a105ae2c63a00ca048870141ecbadf58258e94.pdf",
        "reviews": [
            {
                "id": "OKeQQOwbwc",
                "forum": "Bl2VfU9NhF",
                "replyto": "Bl2VfU9NhF",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission283/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271601,
                "mdate": 1754869465645,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Bl2VfU9NhF",
                "forum": "Bl2VfU9NhF",
                "content": {
                    "title": {
                        "value": "Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control"
                    },
                    "authors": {
                        "value": [
                            "Yitang Li",
                            "Yuanhang Zhang",
                            "Wenli Xiao",
                            "Chaoyi Pan",
                            "Haoyang Weng",
                            "Guanqi He",
                            "Tairan He",
                            "Guanya Shi"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yitang_Li1",
                            "~Yuanhang_Zhang3",
                            "~Wenli_Xiao1",
                            "~Chaoyi_Pan1",
                            "~Haoyang_Weng1",
                            "~Guanqi_He1",
                            "~Tairan_He1",
                            "~Guanya_Shi1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoid Robots",
                            "Reinforcement Learning",
                            "Stable Locomotion"
                        ]
                    },
                    "TLDR": {
                        "value": "We decouple humanoid control into high-frequency arm and low-frequency leg policies, enabling stable end-effector stable control during dynamic locomotion."
                    },
                    "abstract": {
                        "value": "Can your humanoid walk up and hand you a full cup of beer—without spilling a drop? While humanoids are increasingly featured in flashy demos—dancing, delivering packages, traversing rough terrain—fine-grained control during locomotion remains a significant challenge. In particular, stabilizing a filled end-effector (EE) while walking is far from solved, due to a fundamental mismatch in task dynamics: locomotion demands slow-timescale, robust control, whereas EE stabilization requires rapid, high-precision corrections. To address this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples upper-body and lower-body control into separate agents operating at different frequencies and with distinct rewards. This temporal and objective separation mitigates policy interference mitagates objective conflict and enables coordinated whole-body behavior. SoFTA executes upper-body actions at 100 Hz for precise EE control and lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x to baselines and performs 2–3x closer to human-level stability, enabling delicate tasks such as carrying nearly full cups, capturing steady video during locomotion, and disturbance rejection with EE stability."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ecfb0d2c0ce190c0d910180dbf56b4502d3b4195.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/33a105ae2c63a00ca048870141ecbadf58258e94.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025hold,\ntitle={Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control},\nauthor={Yitang Li and Yuanhang Zhang and Wenli Xiao and Chaoyi Pan and Haoyang Weng and Guanqi He and Tairan He and Guanya Shi},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Bl2VfU9NhF}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/9101503fe969a387eb8efcb3a3e00054b5b953e7.mp4"
                    },
                    "paperhash": {
                        "value": "li|hold_my_beer_learning_gentle_humanoid_locomotion_and_endeffector_stabilization_control"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission283/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission283/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission283/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745250379316,
                "pdate": 1754680613067,
                "odate": 1758062762782,
                "mdate": 1758062812661,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission283/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission283/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ksOrtEgIC0",
        "title": "AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons",
        "abstract": "Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with visual adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, RISE-2 achieves comparable performance to policies trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning.",
        "keywords": [
            "Robotic Manipulation",
            "Scalable Data Collection",
            "Generalizable Imitation Policy"
        ],
        "pdf_url": "https://openreview.net/pdf/69e61d178c368400720b29f87a29f696d4f7d812.pdf",
        "reviews": [
            {
                "id": "QYkhOGtWwV",
                "forum": "ksOrtEgIC0",
                "replyto": "ksOrtEgIC0",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission281/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271517,
                "mdate": 1754869456170,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ksOrtEgIC0",
                "forum": "ksOrtEgIC0",
                "content": {
                    "title": {
                        "value": "AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons"
                    },
                    "authors": {
                        "value": [
                            "Hongjie Fang",
                            "Chenxi Wang",
                            "Yiming Wang",
                            "Jingjing Chen",
                            "Shangning Xia",
                            "Jun Lv",
                            "Zihao He",
                            "Xiyan Yi",
                            "Yunhan Guo",
                            "Xinyu Zhan",
                            "Lixin Yang",
                            "Weiming Wang",
                            "Cewu Lu",
                            "Hao-Shu Fang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hongjie_Fang1",
                            "~Chenxi_Wang3",
                            "~Yiming_Wang16",
                            "~Jingjing_Chen6",
                            "~Shangning_Xia1",
                            "~Jun_Lv2",
                            "~Zihao_He4",
                            "~Xiyan_Yi2",
                            "~Yunhan_Guo1",
                            "~Xinyu_Zhan1",
                            "~Lixin_Yang1",
                            "~Weiming_Wang2",
                            "~Cewu_Lu3",
                            "~Hao-Shu_Fang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robotic Manipulation",
                            "Scalable Data Collection",
                            "Generalizable Imitation Policy"
                        ]
                    },
                    "abstract": {
                        "value": "Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with visual adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, RISE-2 achieves comparable performance to policies trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/7ea198b88d62795239bdf66277a8e2ce0bf1c507.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/69e61d178c368400720b29f87a29f696d4f7d812.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfang2025airexo,\ntitle={AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons},\nauthor={Hongjie Fang and Chenxi Wang and Yiming Wang and Jingjing Chen and Shangning Xia and Jun Lv and Zihao He and Xiyan Yi and Yunhan Guo and Xinyu Zhan and Lixin Yang and Weiming Wang and Cewu Lu and Hao-Shu Fang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ksOrtEgIC0}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5861042611591e48bad5298ba4c457f5bcdc6104.mp4"
                    },
                    "paperhash": {
                        "value": "fang|airexo2_scaling_up_generalizable_robotic_imitation_learning_with_lowcost_exoskeletons"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission281/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission281/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission281/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745249125120,
                "pdate": 1754680612935,
                "odate": 1758062762678,
                "mdate": 1758062812533,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission281/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission281/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "4IuTfpWGDR",
        "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization",
        "abstract": "Robots often struggle to generalize from a single demonstration due to the lack of a transferable and interpretable spatial representation. In this work, we introduce TReF-6, a method that infers a simplified, abstracted 6DoF Task-Relevant Frame from a single trajectory. Our approach identifies an influence point purely from the trajectory geometry to define the origin for a local frame, which serves as a reference for parameterizing a Dynamic Movement Primitive (DMP). This influence point captures the task's spatial structure, extending the standard DMP formulation beyond start-goal imitation. The inferred frame is semantically grounded via a vision-language model and localized in novel scenes by Grounded-SAM, enabling functionally consistent skill generalization. We validate TReF-6 in simulation and demonstrate robustness to trajectory noise. We further deploy an end-to-end pipeline on real-world manipulation tasks, showing that TReF-6 supports one-shot imitation learning that preserves task intent across diverse object configurations.",
        "keywords": [
            "Spatial Reference Frames",
            "One-Shot Imitation Learning",
            "Dynamic Movement Primitives"
        ],
        "pdf_url": "https://openreview.net/pdf/f346d6cdaaa432e25a12aec51081069a6be3eddf.pdf",
        "reviews": [
            {
                "id": "fsAbigwtnW",
                "forum": "4IuTfpWGDR",
                "replyto": "4IuTfpWGDR",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission279/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271392,
                "mdate": 1754869465587,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "4IuTfpWGDR",
                "forum": "4IuTfpWGDR",
                "content": {
                    "title": {
                        "value": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization"
                    },
                    "authors": {
                        "value": [
                            "Yuxuan Ding",
                            "Shuangge Wang",
                            "Tesca Fitzgerald"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuxuan_Ding2",
                            "~Shuangge_Wang1",
                            "~Tesca_Fitzgerald1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Spatial Reference Frames",
                            "One-Shot Imitation Learning",
                            "Dynamic Movement Primitives"
                        ]
                    },
                    "abstract": {
                        "value": "Robots often struggle to generalize from a single demonstration due to the lack of a transferable and interpretable spatial representation. In this work, we introduce TReF-6, a method that infers a simplified, abstracted 6DoF Task-Relevant Frame from a single trajectory. Our approach identifies an influence point purely from the trajectory geometry to define the origin for a local frame, which serves as a reference for parameterizing a Dynamic Movement Primitive (DMP). This influence point captures the task's spatial structure, extending the standard DMP formulation beyond start-goal imitation. The inferred frame is semantically grounded via a vision-language model and localized in novel scenes by Grounded-SAM, enabling functionally consistent skill generalization. We validate TReF-6 in simulation and demonstrate robustness to trajectory noise. We further deploy an end-to-end pipeline on real-world manipulation tasks, showing that TReF-6 supports one-shot imitation learning that preserves task intent across diverse object configurations."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f346d6cdaaa432e25a12aec51081069a6be3eddf.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nding2025tref,\ntitle={{TR}eF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization},\nauthor={Yuxuan Ding and Shuangge Wang and Tesca Fitzgerald},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4IuTfpWGDR}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ea71dd5e215a1e93ab7d2a0432317bcc8399b639.mp4"
                    },
                    "paperhash": {
                        "value": "ding|tref6_inferring_taskrelevant_frames_from_a_single_demonstration_for_oneshot_skill_generalization"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission279/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission279/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission279/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745246868387,
                "pdate": 1754680612750,
                "odate": 1758062762563,
                "mdate": 1758062812521,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission279/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission279/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "DiyLz91PKF",
        "title": "UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation",
        "abstract": "Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks—ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, inspired by the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. Extensive experimental evaluations show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified system, enabling robust performance across a variety of real-world conditions.",
        "keywords": [
            "Tactile Object Pose Estimation",
            "Diffusion Model",
            "Precise Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/d4ef379adbfb31be27b879fba902203f43908769.pdf",
        "reviews": [
            {
                "id": "Y7HH9o1Qba",
                "forum": "DiyLz91PKF",
                "replyto": "DiyLz91PKF",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission277/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068271212,
                "mdate": 1754869465504,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "DiyLz91PKF",
                "forum": "DiyLz91PKF",
                "content": {
                    "title": {
                        "value": "UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation"
                    },
                    "authors": {
                        "value": [
                            "Mingdong Wu",
                            "Long Yang",
                            "Jin Liu",
                            "Weiyao Huang",
                            "Lehong Wu",
                            "Zelin Chen",
                            "Daolin Ma",
                            "Hao Dong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mingdong_Wu1",
                            "~Long_Yang7",
                            "~Jin_Liu23",
                            "~Weiyao_Huang1",
                            "~Lehong_Wu1",
                            "~Zelin_Chen2",
                            "~Daolin_Ma1",
                            "~Hao_Dong3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Object Pose Estimation",
                            "Diffusion Model",
                            "Precise Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks—ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, inspired by the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. Extensive experimental evaluations show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified system, enabling robust performance across a variety of real-world conditions."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2bceec51ee53c5fc40880b342daa1c4e0b3bdfad.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d4ef379adbfb31be27b879fba902203f43908769.pdf"
                    },
                    "TLDR": {
                        "value": "We propose a novel framework, trained in pure simulation, that integrates visuotactile in-hand object pose estimation, tracking, and uncertainty estimation into a unified system."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwu2025unitacpose,\ntitle={UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation},\nauthor={Mingdong Wu and Long Yang and Jin Liu and Weiyao Huang and Lehong Wu and Zelin Chen and Daolin Ma and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DiyLz91PKF}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1cafdd0d13450e83d2172f51267a6aa0ba1090d9.mp4"
                    },
                    "paperhash": {
                        "value": "wu|unitac2pose_a_unified_approach_learned_in_simulation_for_categorylevel_visuotactile_inhand_pose_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission277/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission277/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission277/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745244214758,
                "pdate": 1754680612532,
                "odate": 1758062762397,
                "mdate": 1758062812254,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission277/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "AVDCwK1dek",
        "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations",
        "abstract": "Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems.",
        "keywords": [
            "balance control",
            "bipedal standing and falling",
            "musculoskeletal system"
        ],
        "pdf_url": "https://openreview.net/pdf/4422135f58e9e559eec0175b4afa23430df8130c.pdf",
        "reviews": [
            {
                "id": "xEJzQ2WJxX",
                "forum": "AVDCwK1dek",
                "replyto": "AVDCwK1dek",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission264/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068270586,
                "mdate": 1754869465224,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "AVDCwK1dek",
                "forum": "AVDCwK1dek",
                "content": {
                    "title": {
                        "value": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations"
                    },
                    "authors": {
                        "value": [
                            "Chengtian Ma",
                            "Yunyue Wei",
                            "Chenhui Zuo",
                            "Chen Zhang",
                            "Yanan Sui"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chengtian_Ma1",
                            "~Yunyue_Wei1",
                            "~Chenhui_Zuo1",
                            "~Chen_Zhang14",
                            "~Yanan_Sui1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "balance control",
                            "bipedal standing and falling",
                            "musculoskeletal system"
                        ]
                    },
                    "abstract": {
                        "value": "Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c75baca724e3d407d2b379e8dd902076d8d5c3a6.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4422135f58e9e559eec0175b4afa23430df8130c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nma2025bipedal,\ntitle={Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations},\nauthor={Chengtian Ma and Yunyue Wei and Chenhui Zuo and Chen Zhang and Yanan Sui},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AVDCwK1dek}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2d87ee4fe535b4dc9f87a3505f22aaf7d32b17ed.mp4"
                    },
                    "paperhash": {
                        "value": "ma|bipedal_balance_control_with_wholebody_musculoskeletal_standing_and_falling_simulations"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission264/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission264/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission264/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745229813951,
                "pdate": 1754680611915,
                "odate": 1758062761967,
                "mdate": 1758062812152,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission264/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission264/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "5ySSVlJBOn",
        "title": "FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real",
        "abstract": "Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a co-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate FetchBot’s strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value.",
        "keywords": [
            "Generalizable Fetching",
            "Sim2Real",
            "Occlusion Handling"
        ],
        "pdf_url": "https://openreview.net/pdf/233ed47c2f534c8b84d1e1c2693dd26cc3bf657a.pdf",
        "reviews": [
            {
                "id": "m2l1az5prR",
                "forum": "5ySSVlJBOn",
                "replyto": "5ySSVlJBOn",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission263/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068270548,
                "mdate": 1754869456107,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "5ySSVlJBOn",
                "forum": "5ySSVlJBOn",
                "content": {
                    "title": {
                        "value": "FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real"
                    },
                    "authors": {
                        "value": [
                            "Weiheng Liu",
                            "Yuxuan Wan",
                            "Jilong Wang",
                            "Yuxuan Kuang",
                            "Xuesong Shi",
                            "Haoran Li",
                            "Dongbin Zhao",
                            "Zhizheng Zhang",
                            "He Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Weiheng_Liu2",
                            "~Yuxuan_Wan2",
                            "~Jilong_Wang1",
                            "~Yuxuan_Kuang1",
                            "~Xuesong_Shi1",
                            "~Haoran_Li7",
                            "~Dongbin_Zhao1",
                            "~Zhizheng_Zhang1",
                            "~He_Wang5"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Generalizable Fetching",
                            "Sim2Real",
                            "Occlusion Handling"
                        ]
                    },
                    "abstract": {
                        "value": "Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a co-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate FetchBot’s strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value."
                    },
                    "supplementary_material": {
                        "value": "/attachment/173481ec1e3a4777a581f2cbbc5c78c5d22b8ea5.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/233ed47c2f534c8b84d1e1c2693dd26cc3bf657a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025fetchbot,\ntitle={FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real},\nauthor={Weiheng Liu and Yuxuan Wan and Jilong Wang and Yuxuan Kuang and Xuesong Shi and Haoran Li and Dongbin Zhao and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=5ySSVlJBOn}\n}"
                    },
                    "paperhash": {
                        "value": "liu|fetchbot_learning_generalizable_object_fetching_in_cluttered_scenes_via_zeroshot_sim2real"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission263/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission263/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission263/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745227331985,
                "pdate": 1754680611754,
                "odate": 1758062761873,
                "mdate": 1758062812174,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission263/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission263/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "kto4zVmo4w",
        "title": "One View, Many Worlds: Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation",
        "abstract": "Estimating the 6D pose of arbitrary objects from a single reference image is a critical yet challenging task in robotics, especially considering the long-tail distribution of real-world instances. While category-level and model-based approaches have achieved notable progress, they remain limited in generalizing to unseen objects under one-shot settings. In this work, we propose a novel pipeline for fast and accurate one-shot 6D pose and scale estimation. Leveraging recent advances in single-view 3D generation, we first build high-fidelity textured meshes without requiring known object poses. To resolve scale ambiguity, we introduce a coarse-to-fine alignment module that estimates both object size and initial pose by matching 2D-3D features with depth information. We then generate a diversified set of plausible 3D models using text-guided generative augmentation and render them with Blender to synthesize large-scale, domain-randomized training data for pose estiamtion. This synthetic data bridges the domain gap and enables robust fine-tuning of pose estimators. Our method achieves state-of-the-art results on several 6D pose benchmarks, and we further validate its effectiveness on a newly collected in-the-wild dataset. Finally, we integrate our system with a dexterous hand, demonstrating its robustness in real-world robotic grasping tasks. All code, data, and models will be released to foster future research.",
        "keywords": [
            "Unseen Object Pose Estimation",
            "Generative Model",
            "Robot Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/6be3b357cc2fe108db5a787b80e81953afc422c6.pdf",
        "reviews": [
            {
                "id": "6SV4a2Hsvc",
                "forum": "kto4zVmo4w",
                "replyto": "kto4zVmo4w",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission262/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068270482,
                "mdate": 1754869456015,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "kto4zVmo4w",
                "forum": "kto4zVmo4w",
                "content": {
                    "title": {
                        "value": "One View, Many Worlds: Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation"
                    },
                    "authors": {
                        "value": [
                            "Zheng Geng",
                            "Nan Wang",
                            "Shaocong Xu",
                            "Chongjie Ye",
                            "Bohan Li",
                            "Zhaoxi Chen",
                            "Sida Peng",
                            "Hao Zhao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zheng_Geng1",
                            "~Nan_Wang14",
                            "~Shaocong_Xu1",
                            "~Chongjie_Ye1",
                            "~Bohan_Li6",
                            "~Zhaoxi_Chen1",
                            "~Sida_Peng1",
                            "~Hao_Zhao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Unseen Object Pose Estimation",
                            "Generative Model",
                            "Robot Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation"
                    },
                    "abstract": {
                        "value": "Estimating the 6D pose of arbitrary objects from a single reference image is a critical yet challenging task in robotics, especially considering the long-tail distribution of real-world instances. While category-level and model-based approaches have achieved notable progress, they remain limited in generalizing to unseen objects under one-shot settings. In this work, we propose a novel pipeline for fast and accurate one-shot 6D pose and scale estimation. Leveraging recent advances in single-view 3D generation, we first build high-fidelity textured meshes without requiring known object poses. To resolve scale ambiguity, we introduce a coarse-to-fine alignment module that estimates both object size and initial pose by matching 2D-3D features with depth information. We then generate a diversified set of plausible 3D models using text-guided generative augmentation and render them with Blender to synthesize large-scale, domain-randomized training data for pose estiamtion. This synthetic data bridges the domain gap and enables robust fine-tuning of pose estimators. Our method achieves state-of-the-art results on several 6D pose benchmarks, and we further validate its effectiveness on a newly collected in-the-wild dataset. Finally, we integrate our system with a dexterous hand, demonstrating its robustness in real-world robotic grasping tasks. All code, data, and models will be released to foster future research."
                    },
                    "supplementary_material": {
                        "value": "/attachment/94c17f53e4a1c93570507b2448cac1fb07ba8448.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6be3b357cc2fe108db5a787b80e81953afc422c6.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngeng2025one,\ntitle={One View, Many Worlds: Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation},\nauthor={Zheng Geng and Nan Wang and Shaocong Xu and Chongjie Ye and Bohan Li and Zhaoxi Chen and Sida Peng and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kto4zVmo4w}\n}"
                    },
                    "paperhash": {
                        "value": "geng|one_view_many_worlds_singleimage_to_3d_object_meets_generative_domain_randomization_for_oneshot_6d_pose_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission262/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission262/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission262/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745227253528,
                "pdate": 1754680611706,
                "odate": 1758062761856,
                "mdate": 1758062811925,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission262/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "AP7kM1xk2a",
        "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving",
        "abstract": "Recent breakthroughs in large language models (LLMs) have not only advanced natural language processing but also inspired their application in domains with structurally similar problems—most notably, autonomous driving motion generation. Both domains involve autoregressive sequence modeling, token-based representations, and context-aware decision making, making the transfer of LLM components a natural and increasingly common practice. However, despite promising early attempts, a systematic understanding of which LLM modules are truly transferable remains lacking. In this paper, we present a comprehensive evaluation of five key LLM modules—tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation—within the context of motion generation for autonomous driving. Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate that, when appropriately adapted, these modules can significantly improve performance for autonomous driving motion generation. In addition, we identify which techniques can be effectively transferred, analyze the potential reasons for the failure of others, and discuss the specific adaptations needed for autonomous driving scenarios. We evaluate our method on the Sim Agents task and achieve competitive results.",
        "keywords": [
            "Large Language Model",
            "Autonomous Driving",
            "Motion Generation",
            "Sim Agent"
        ],
        "pdf_url": "https://openreview.net/pdf/3cca77da6e56bb6c5e02829d2fe01e6451ff4cc5.pdf",
        "reviews": [
            {
                "id": "FH8tfVHajq",
                "forum": "AP7kM1xk2a",
                "replyto": "AP7kM1xk2a",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission259/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068270391,
                "mdate": 1754869465138,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "AP7kM1xk2a",
                "forum": "AP7kM1xk2a",
                "content": {
                    "title": {
                        "value": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving"
                    },
                    "authors": {
                        "value": [
                            "Mingyi Wang",
                            "Jingke Wang",
                            "Tengju Ye",
                            "Kaicheng Yu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mingyi_Wang1",
                            "~Jingke_Wang1",
                            "~Tengju_Ye1",
                            "~Kaicheng_Yu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Large Language Model",
                            "Autonomous Driving",
                            "Motion Generation",
                            "Sim Agent"
                        ]
                    },
                    "abstract": {
                        "value": "Recent breakthroughs in large language models (LLMs) have not only advanced natural language processing but also inspired their application in domains with structurally similar problems—most notably, autonomous driving motion generation. Both domains involve autoregressive sequence modeling, token-based representations, and context-aware decision making, making the transfer of LLM components a natural and increasingly common practice. However, despite promising early attempts, a systematic understanding of which LLM modules are truly transferable remains lacking. In this paper, we present a comprehensive evaluation of five key LLM modules—tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation—within the context of motion generation for autonomous driving. Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate that, when appropriately adapted, these modules can significantly improve performance for autonomous driving motion generation. In addition, we identify which techniques can be effectively transferred, analyze the potential reasons for the failure of others, and discuss the specific adaptations needed for autonomous driving scenarios. We evaluate our method on the Sim Agents task and achieve competitive results."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3cca77da6e56bb6c5e02829d2fe01e6451ff4cc5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025do,\ntitle={Do {LLM} Modules Generalize? A Study on Motion Generation for Autonomous Driving},\nauthor={Mingyi Wang and Jingke Wang and Tengju Ye and Kaicheng Yu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AP7kM1xk2a}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5be3a33b5fc650463a7b5d35b322c76da633a12d.mp4"
                    },
                    "paperhash": {
                        "value": "wang|do_llm_modules_generalize_a_study_on_motion_generation_for_autonomous_driving"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission259/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission259/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745224766343,
                "pdate": 1754680611488,
                "odate": 1758062761655,
                "mdate": 1758062811895,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission259/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission259/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "XoRtWWjXuC",
        "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation",
        "abstract": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment.",
        "keywords": [
            "Simulation Platform",
            "scene construction",
            "teleoperation",
            "simulation dataset"
        ],
        "pdf_url": "https://openreview.net/pdf/94769209f4d431038f0b10c60b40d24aac9bcdb3.pdf",
        "reviews": [
            {
                "id": "kC9IjYuIfy",
                "forum": "XoRtWWjXuC",
                "replyto": "XoRtWWjXuC",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission254/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068270116,
                "mdate": 1754869464904,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "XoRtWWjXuC",
                "forum": "XoRtWWjXuC",
                "content": {
                    "title": {
                        "value": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yizheng Zhang",
                            "Zhenjun Yu",
                            "JiaXin Lai",
                            "Cewu Lu",
                            "Lei Han"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yizheng_Zhang1",
                            "~Zhenjun_Yu1",
                            "~JiaXin_Lai1",
                            "~Cewu_Lu3",
                            "~Lei_Han1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Simulation Platform",
                            "scene construction",
                            "teleoperation",
                            "simulation dataset"
                        ]
                    },
                    "abstract": {
                        "value": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e726c3337f8b0aad329260a46cb66ab6394979cf.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/94769209f4d431038f0b10c60b40d24aac9bcdb3.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025agentworld,\ntitle={AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation},\nauthor={Yizheng Zhang and Zhenjun Yu and JiaXin Lai and Cewu Lu and Lei Han},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XoRtWWjXuC}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/77c3b8804164f48b4b745563c7ae3287e167b144.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|agentworld_an_interactive_simulation_platform_for_scene_construction_and_mobile_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission254/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission254/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission254/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745216627393,
                "pdate": 1754680611186,
                "odate": 1758062761475,
                "mdate": 1758062811869,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission254/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "RUSscFSEfD",
        "title": "FastUMI: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset",
        "abstract": "Real-world manipulation datasets for robotic arms remain scarce due to the high costs, rigid hardware dependencies, and complex setup procedures associated with existing data collection methods.\nWe introduce, a redesigned Universal Manipulation Interface (UMI) that addresses these challenges, enabling low-cost, scalable, and rapid deployment across heterogeneous platforms.\nFastUMI achieves this through: (i) hardware decoupling via extensive mechanical reengineering, which removes dependence on specialized robotic components while preserving a consistent visual perspective; (ii) replacement of complex visual–inertial odometry with a commercial off-the-shelf tracker, simplifying the software stack without compromising pose estimation accuracy; and (iii) the provision of an integrated ecosystem that streamlines data acquisition, automates quality control, and ensures compatibility with both standard and enhanced imitation-learning pipelines.\nTo facilitate further research, we release an open-access dataset comprising over 15,000 real-world demonstrations spanning 24 tasks constituting one of the most extensive UMI-like resources to date.\nEmpirical evaluations show that FastUMI supports rapid deployment, reduces operational overhead, and delivers robust performance across diverse manipulation scenarios, advancing scalable data-driven robotic learning.",
        "keywords": [
            "Imitation Learning",
            "Manipulation",
            "Data Collection"
        ],
        "pdf_url": "https://openreview.net/pdf/7666843449df935daf69ac9e30f7fa4d6dcc0611.pdf",
        "reviews": [
            {
                "id": "66XjmKaPBd",
                "forum": "RUSscFSEfD",
                "replyto": "RUSscFSEfD",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission249/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068269331,
                "mdate": 1754869464784,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "RUSscFSEfD",
                "forum": "RUSscFSEfD",
                "content": {
                    "title": {
                        "value": "FastUMI: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset"
                    },
                    "authors": {
                        "value": [
                            "Zhaxizhuoma",
                            "Kehui Liu",
                            "Chuyue Guan",
                            "Zhongjie Jia",
                            "Ziniu Wu",
                            "Xin Liu",
                            "Tianyu Wang",
                            "Shuai Liang",
                            "Pengan CHEN",
                            "Pingrui Zhang",
                            "Haoming Song",
                            "Delin Qu",
                            "Dong Wang",
                            "Zhigang Wang",
                            "Nieqing Cao",
                            "Yan Ding",
                            "Bin Zhao",
                            "Xuelong Li"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zhaxizhuoma1",
                            "~Kehui_Liu1",
                            "~Chuyue_Guan1",
                            "~Zhongjie_Jia1",
                            "~Ziniu_Wu3",
                            "~Xin_Liu62",
                            "~Tianyu_Wang10",
                            "~Shuai_Liang2",
                            "~Pengan_CHEN1",
                            "~Pingrui_Zhang1",
                            "~Haoming_Song1",
                            "~Delin_Qu1",
                            "~Dong_Wang1",
                            "~Zhigang_Wang3",
                            "~Nieqing_Cao1",
                            "~Yan_Ding5",
                            "~Bin_Zhao7",
                            "~Xuelong_Li2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Manipulation",
                            "Data Collection"
                        ]
                    },
                    "abstract": {
                        "value": "Real-world manipulation datasets for robotic arms remain scarce due to the high costs, rigid hardware dependencies, and complex setup procedures associated with existing data collection methods.\nWe introduce, a redesigned Universal Manipulation Interface (UMI) that addresses these challenges, enabling low-cost, scalable, and rapid deployment across heterogeneous platforms.\nFastUMI achieves this through: (i) hardware decoupling via extensive mechanical reengineering, which removes dependence on specialized robotic components while preserving a consistent visual perspective; (ii) replacement of complex visual–inertial odometry with a commercial off-the-shelf tracker, simplifying the software stack without compromising pose estimation accuracy; and (iii) the provision of an integrated ecosystem that streamlines data acquisition, automates quality control, and ensures compatibility with both standard and enhanced imitation-learning pipelines.\nTo facilitate further research, we release an open-access dataset comprising over 15,000 real-world demonstrations spanning 24 tasks constituting one of the most extensive UMI-like resources to date.\nEmpirical evaluations show that FastUMI supports rapid deployment, reduces operational overhead, and delivers robust performance across diverse manipulation scenarios, advancing scalable data-driven robotic learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/24bb7424e17d45d43d64331a6148fedaae7b5a74.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7666843449df935daf69ac9e30f7fa4d6dcc0611.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhaxizhuoma2025fastumi,\ntitle={Fast{UMI}: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset},\nauthor={Zhaxizhuoma and Kehui Liu and Chuyue Guan and Zhongjie Jia and Ziniu Wu and Xin Liu and Tianyu Wang and Shuai Liang and Pengan CHEN and Pingrui Zhang and Haoming Song and Delin Qu and Dong Wang and Zhigang Wang and Nieqing Cao and Yan Ding and Bin Zhao and Xuelong Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RUSscFSEfD}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/9e1367595308e1402211f852bcbb175e0c909979.mp4"
                    },
                    "paperhash": {
                        "value": "zhaxizhuoma|fastumi_a_scalable_and_hardwareindependent_universal_manipulation_interface_with_dataset"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission249/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission249/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission249/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745206624905,
                "pdate": 1754680610611,
                "odate": 1758062761320,
                "mdate": 1758062811674,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission249/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission249/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "FdgtV9mO6j",
        "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
        "abstract": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent’s behavior through constrained reinforcement learning. The system helps regulate the agent’s actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds.",
        "keywords": [
            "Crowd Navigation",
            "Reinforcement Learning",
            "Distribution Shift"
        ],
        "pdf_url": "https://openreview.net/pdf/b57b278736f72399064180f9198faf73a1e72fa9.pdf",
        "reviews": [
            {
                "id": "W343CkdWEK",
                "forum": "FdgtV9mO6j",
                "replyto": "FdgtV9mO6j",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission248/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068269242,
                "mdate": 1754869456002,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "FdgtV9mO6j",
                "forum": "FdgtV9mO6j",
                "content": {
                    "title": {
                        "value": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling"
                    },
                    "authors": {
                        "value": [
                            "Jianpeng Yao",
                            "Xiaopan Zhang",
                            "Yu Xia",
                            "Zejin Wang",
                            "Amit Roy-Chowdhury",
                            "Jiachen Li"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jianpeng_Yao1",
                            "~Xiaopan_Zhang1",
                            "~Yu_Xia17",
                            "~Zejin_Wang2",
                            "~Amit_Roy-Chowdhury2",
                            "~Jiachen_Li1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Crowd Navigation",
                            "Reinforcement Learning",
                            "Distribution Shift"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a reinforcement learning algorithm that maintains safe crowd navigation under distribution shifts by explicitly modeling and constraining prediction uncertainty."
                    },
                    "abstract": {
                        "value": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent’s behavior through constrained reinforcement learning. The system helps regulate the agent’s actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds."
                    },
                    "supplementary_material": {
                        "value": "/attachment/fd7d322679424cfff98b60de9eb570fd4ec3afc4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b57b278736f72399064180f9198faf73a1e72fa9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyao2025towards,\ntitle={Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling},\nauthor={Jianpeng Yao and Xiaopan Zhang and Yu Xia and Zejin Wang and Amit Roy-Chowdhury and Jiachen Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FdgtV9mO6j}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/7c3e9d63d749ae2e25e3af0883804a2f53d4105f.mp4"
                    },
                    "paperhash": {
                        "value": "yao|towards_generalizable_safety_in_crowd_navigation_via_conformal_uncertainty_handling"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission248/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission248/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission248/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745203017645,
                "pdate": 1754680610609,
                "odate": 1758062761247,
                "mdate": 1758062811625,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission248/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission248/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "PwKsCO6TAF",
        "title": "KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands",
        "abstract": "Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems, but developing dexterous manipulation skills remains challenging. While imitation learning shows promise for complex manipulation tasks, traditional approaches struggle with soft systems due to demonstration collection challenges and ineffective state representations. We present KineSoft, a framework enabling direct kinesthetic teaching of soft robotic hands by leveraging their natural compliance as a skill teaching advantage rather than only as a control challenge. KineSoft makes two key contributions: (1) an internal strain sensing array providing occlusion-free proprioceptive shape estimation, and (2) a shape-based imitation learning framework that uses proprioceptive feedback with a low-level shape-conditioned controller to ground diffusion-based policies. This enables human demonstrators to physically guide the robot while the system learns to associate proprioceptive patterns with successful manipulation strategies. We validate KineSoft through physical experiments, demonstrating superior shape estimation accuracy compared to baseline methods, precise shape-trajectory tracking, and higher task success rates compared to baseline imitation learning approaches. KineSoft's results demonstrate that embracing the inherent properties of soft robots leads to intuitive and robust dexterous manipulation capabilities.",
        "keywords": [
            "Soft Robots",
            "Proprioceptive Estimation",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/8903ba202a8d8517e032fe60112a0c9616f43c87.pdf",
        "reviews": [
            {
                "id": "HKYtHVtys9",
                "forum": "PwKsCO6TAF",
                "replyto": "PwKsCO6TAF",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission246/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068269329,
                "mdate": 1754869455942,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "PwKsCO6TAF",
                "forum": "PwKsCO6TAF",
                "content": {
                    "title": {
                        "value": "KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands"
                    },
                    "authors": {
                        "value": [
                            "Uksang Yoo",
                            "Jonathan Francis",
                            "Jean Oh",
                            "Jeffrey Ichnowski"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Uksang_Yoo1",
                            "~Jonathan_Francis1",
                            "~Jean_Oh2",
                            "~Jeffrey_Ichnowski1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Soft Robots",
                            "Proprioceptive Estimation",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose KineSoft, a framework that enables soft robot hands to perform dexterous manipulation by combining proprioceptive mesh shape sensing, kinesthetic demonstration, and a behavior cloning policy trained to track deformation trajectories."
                    },
                    "abstract": {
                        "value": "Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems, but developing dexterous manipulation skills remains challenging. While imitation learning shows promise for complex manipulation tasks, traditional approaches struggle with soft systems due to demonstration collection challenges and ineffective state representations. We present KineSoft, a framework enabling direct kinesthetic teaching of soft robotic hands by leveraging their natural compliance as a skill teaching advantage rather than only as a control challenge. KineSoft makes two key contributions: (1) an internal strain sensing array providing occlusion-free proprioceptive shape estimation, and (2) a shape-based imitation learning framework that uses proprioceptive feedback with a low-level shape-conditioned controller to ground diffusion-based policies. This enables human demonstrators to physically guide the robot while the system learns to associate proprioceptive patterns with successful manipulation strategies. We validate KineSoft through physical experiments, demonstrating superior shape estimation accuracy compared to baseline methods, precise shape-trajectory tracking, and higher task success rates compared to baseline imitation learning approaches. KineSoft's results demonstrate that embracing the inherent properties of soft robots leads to intuitive and robust dexterous manipulation capabilities."
                    },
                    "supplementary_material": {
                        "value": "/attachment/3eddc5813ac10f0629f331a7a9202390d195c7a2.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/8903ba202a8d8517e032fe60112a0c9616f43c87.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyoo2025kinesoft,\ntitle={KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands},\nauthor={Uksang Yoo and Jonathan Francis and Jean Oh and Jeffrey Ichnowski},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PwKsCO6TAF}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/9c99ce4dc24fb61cd78708abac12e7e03cb1bfca.mp4"
                    },
                    "paperhash": {
                        "value": "yoo|kinesoft_learning_proprioceptive_manipulation_policies_with_soft_robot_hands"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission246/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission246/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission246/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745198937943,
                "pdate": 1754680610345,
                "odate": 1758062761173,
                "mdate": 1758062811672,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission246/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission246/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "iq0wUf6dUy",
        "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
        "abstract": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues.\nIn this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a ``reflection'' mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://corl2025-reflectvlm.github.io.",
        "keywords": [
            "Vision-Language Models",
            "Long-Horizon Manipulation",
            "Manipulation Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/f694d1ce294341463425ccdba11d62b18992cb7d.pdf",
        "reviews": [
            {
                "id": "5VIkv9A5Qk",
                "forum": "iq0wUf6dUy",
                "replyto": "iq0wUf6dUy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission245/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268890,
                "mdate": 1754869464722,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "iq0wUf6dUy",
                "forum": "iq0wUf6dUy",
                "content": {
                    "title": {
                        "value": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yunhai Feng",
                            "Jiaming Han",
                            "Zhuoran Yang",
                            "Xiangyu Yue",
                            "Sergey Levine",
                            "Jianlan Luo"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yunhai_Feng1",
                            "~Jiaming_Han1",
                            "~Zhuoran_Yang1",
                            "~Xiangyu_Yue1",
                            "~Sergey_Levine1",
                            "~Jianlan_Luo1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language Models",
                            "Long-Horizon Manipulation",
                            "Manipulation Planning"
                        ]
                    },
                    "abstract": {
                        "value": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues.\nIn this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a ``reflection'' mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://corl2025-reflectvlm.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/01f98ff4de3ad30f1dd024c8b6de158fbbfe2c5e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f694d1ce294341463425ccdba11d62b18992cb7d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nfeng2025reflective,\ntitle={Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation},\nauthor={Yunhai Feng and Jiaming Han and Zhuoran Yang and Xiangyu Yue and Sergey Levine and Jianlan Luo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iq0wUf6dUy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b6dd6580f02d32c5bf66b9cf67784589144241e7.mp4"
                    },
                    "paperhash": {
                        "value": "feng|reflective_planning_visionlanguage_models_for_multistage_longhorizon_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission245/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission245/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission245/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745194916477,
                "pdate": 1754680610287,
                "odate": 1758062761126,
                "mdate": 1758062811390,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission245/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "VVhAhzr2WV",
        "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
        "abstract": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm—human teleoperation—remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision-modeling turned off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations.",
        "keywords": [
            "Robot Datasets",
            "Imitation Learning",
            "Data Augmentation"
        ],
        "pdf_url": "https://openreview.net/pdf/c53a6e1332fa5483cf7ce5e27ef579c9d675f0ff.pdf",
        "reviews": [
            {
                "id": "9rArnhXHaA",
                "forum": "VVhAhzr2WV",
                "replyto": "VVhAhzr2WV",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission244/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268871,
                "mdate": 1754869455779,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "VVhAhzr2WV",
                "forum": "VVhAhzr2WV",
                "content": {
                    "title": {
                        "value": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware"
                    },
                    "authors": {
                        "value": [
                            "Justin Yu",
                            "Letian Fu",
                            "Huang Huang",
                            "Karim El-Refai",
                            "Rares Andrei Ambrus",
                            "Richard Cheng",
                            "Muhammad Zubair Irshad",
                            "Ken Goldberg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Justin_Yu3",
                            "~Letian_Fu1",
                            "~Huang_Huang1",
                            "~Karim_El-Refai1",
                            "~Rares_Andrei_Ambrus1",
                            "~Richard_Cheng1",
                            "~Muhammad_Zubair_Irshad1",
                            "~Ken_Goldberg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Datasets",
                            "Imitation Learning",
                            "Data Augmentation"
                        ]
                    },
                    "abstract": {
                        "value": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm—human teleoperation—remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision-modeling turned off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations."
                    },
                    "supplementary_material": {
                        "value": "/attachment/32e454e81b44b2dabb5cbc92dda4b26694b655ed.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c53a6e1332fa5483cf7ce5e27ef579c9d675f0ff.pdf"
                    },
                    "TLDR": {
                        "value": "Real2Render2Real (R2R2R) generates robot data from a single video demo and 3D object scan, without teleop or dynamics sim, by tracking object motion, synthesizing diverse trajectories, and rendering demonstrations for policy learning."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyu2025realrenderreal,\ntitle={Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware},\nauthor={Justin Yu and Letian Fu and Huang Huang and Karim El-Refai and Rares Andrei Ambrus and Richard Cheng and Muhammad Zubair Irshad and Ken Goldberg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VVhAhzr2WV}\n}"
                    },
                    "paperhash": {
                        "value": "yu|real2render2real_scaling_robot_data_without_dynamics_simulation_or_robot_hardware"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission244/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission244/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission244/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745187745726,
                "pdate": 1754680610042,
                "odate": 1758062761083,
                "mdate": 1758062811348,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission244/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ZPJo9RJL15",
        "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion",
        "abstract": "Exploration is crucial for legged robots to learn agile locomotion behaviors capable of overcoming diverse obstacles. \nFor example, a robot may need to try different contact patterns and momentum profiles to successfully jump over an obstacle—but encouraging such diverse exploration is inherently challenging. As a result, training these behaviors often relies on additional techniques such as extensive reward engineering, expert demonstrations, or curriculum learning. However, these approaches limit generalizability, especially when prior knowledge or demonstration data is unavailable.\nIn this work, we propose using unsupervised skill discovery as a skill-level exploration strategy to significantly reduce human engineering effort. Our learning framework enables the agent to autonomously discover diverse skills to overcome complex obstacles. To dynamically regulate the degree of exploration throughout training, we introduce a bi-level optimization process that learns a parameter to balance two distinct reward signals. We demonstrate that our method enables quadrupedal robots to acquire highly agile behaviors—including crawling, climbing, leaping, and complex maneuvers such as jumping off vertical walls. Finally, we successfully deploy the learned policy on real hardware, validating its transferability to the real world.",
        "keywords": [
            "Unsupervised Reinforcement Learning",
            "Locomotion",
            "Quadruped",
            "Skill Discovery"
        ],
        "pdf_url": "https://openreview.net/pdf/8248f7a65edcb13dd45946741ccde329183aec3c.pdf",
        "reviews": [
            {
                "id": "yJ1qq8CJYR",
                "forum": "ZPJo9RJL15",
                "replyto": "ZPJo9RJL15",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission241/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268669,
                "mdate": 1754869464650,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ZPJo9RJL15",
                "forum": "ZPJo9RJL15",
                "content": {
                    "title": {
                        "value": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion"
                    },
                    "authors": {
                        "value": [
                            "Seungeun Rho",
                            "Kartik Garg",
                            "Morgan Byrd",
                            "Sehoon Ha"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Seungeun_Rho1",
                            "~Kartik_Garg1",
                            "~Morgan_Byrd1",
                            "~Sehoon_Ha2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Unsupervised Reinforcement Learning",
                            "Locomotion",
                            "Quadruped",
                            "Skill Discovery"
                        ]
                    },
                    "abstract": {
                        "value": "Exploration is crucial for legged robots to learn agile locomotion behaviors capable of overcoming diverse obstacles. \nFor example, a robot may need to try different contact patterns and momentum profiles to successfully jump over an obstacle—but encouraging such diverse exploration is inherently challenging. As a result, training these behaviors often relies on additional techniques such as extensive reward engineering, expert demonstrations, or curriculum learning. However, these approaches limit generalizability, especially when prior knowledge or demonstration data is unavailable.\nIn this work, we propose using unsupervised skill discovery as a skill-level exploration strategy to significantly reduce human engineering effort. Our learning framework enables the agent to autonomously discover diverse skills to overcome complex obstacles. To dynamically regulate the degree of exploration throughout training, we introduce a bi-level optimization process that learns a parameter to balance two distinct reward signals. We demonstrate that our method enables quadrupedal robots to acquire highly agile behaviors—including crawling, climbing, leaping, and complex maneuvers such as jumping off vertical walls. Finally, we successfully deploy the learned policy on real hardware, validating its transferability to the real world."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d2e43caf6a522c74dbe325ca966398ddf9b52faa.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/8248f7a65edcb13dd45946741ccde329183aec3c.pdf"
                    },
                    "TLDR": {
                        "value": "By applying unsupervised reinforcement learning, we trained agile locomotion behaviors without relying on curriculum or reference motions."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nrho2025unsupervised,\ntitle={Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion},\nauthor={Seungeun Rho and Kartik Garg and Morgan Byrd and Sehoon Ha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZPJo9RJL15}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b520a1477aea97d7561d239e563f20fa0065dad3.zip"
                    },
                    "paperhash": {
                        "value": "rho|unsupervised_skill_discovery_as_exploration_for_learning_agile_locomotion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission241/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission241/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission241/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745181463605,
                "pdate": 1754680609811,
                "odate": 1758062761005,
                "mdate": 1758062811305,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission241/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "BcJlmjF1vV",
        "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation",
        "abstract": "Effective policy learning for robotic manipulation requires scene representations that selectively capture task-relevant environmental features. Current approaches typically employ task-agnostic representation extraction, failing to emulate the dynamic perceptual adaptation observed in human cognition. We present HyperTASR, a hypernetwork-driven framework that modulates scene representations based on both task objectives and the execution phase. Our architecture dynamically generates representation transformation parameters conditioned on task specifications and progression state, enabling representations to evolve contextually throughout task execution. This approach maintains architectural compatibility with existing policy learning frameworks while fundamentally reconfiguring how visual features are processed. Unlike methods that simply concatenate or fuse task embeddings with task-agnostic representations, HyperTASR establishes computational separation between task-contextual and state-dependent processing paths, enhancing learning efficiency and representational quality. Comprehensive evaluations in both simulation and real-world environments demonstrate substantial performance improvements across different representation paradigms. Most notably, HyperTASR elevates success rates by over 27\\% when applied to GNFactor and achieves unprecedented single-view performance exceeding 80\\% success with 3D Diffuser Actor. Through ablation studies and attention visualization, we confirm that our approach selectively prioritizes task-relevant scene information, closely mirroring human adaptive perception during manipulation tasks.",
        "keywords": [
            "Representation learning",
            "manipulation",
            "hypernetworks"
        ],
        "pdf_url": "https://openreview.net/pdf/a03762ee97314515b8afdbc0804a0aa80d43e90c.pdf",
        "reviews": [
            {
                "id": "rZkQWRHNqQ",
                "forum": "BcJlmjF1vV",
                "replyto": "BcJlmjF1vV",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission240/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268667,
                "mdate": 1754869464471,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "BcJlmjF1vV",
                "forum": "BcJlmjF1vV",
                "content": {
                    "title": {
                        "value": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Li Sun",
                            "Jiefeng Wu",
                            "Feng Chen",
                            "Ruizhe Liu",
                            "Yanchao Yang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Li_Sun8",
                            "~Jiefeng_Wu1",
                            "~Feng_Chen16",
                            "~Ruizhe_Liu1",
                            "~Yanchao_Yang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Representation learning",
                            "manipulation",
                            "hypernetworks"
                        ]
                    },
                    "abstract": {
                        "value": "Effective policy learning for robotic manipulation requires scene representations that selectively capture task-relevant environmental features. Current approaches typically employ task-agnostic representation extraction, failing to emulate the dynamic perceptual adaptation observed in human cognition. We present HyperTASR, a hypernetwork-driven framework that modulates scene representations based on both task objectives and the execution phase. Our architecture dynamically generates representation transformation parameters conditioned on task specifications and progression state, enabling representations to evolve contextually throughout task execution. This approach maintains architectural compatibility with existing policy learning frameworks while fundamentally reconfiguring how visual features are processed. Unlike methods that simply concatenate or fuse task embeddings with task-agnostic representations, HyperTASR establishes computational separation between task-contextual and state-dependent processing paths, enhancing learning efficiency and representational quality. Comprehensive evaluations in both simulation and real-world environments demonstrate substantial performance improvements across different representation paradigms. Most notably, HyperTASR elevates success rates by over 27\\% when applied to GNFactor and achieves unprecedented single-view performance exceeding 80\\% success with 3D Diffuser Actor. Through ablation studies and attention visualization, we confirm that our approach selectively prioritizes task-relevant scene information, closely mirroring human adaptive perception during manipulation tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/388e4342cf89272542c73a2a296986d65bea7d41.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We present HyperTASR, a novel framework for task-aware scene representations in robotic manipulation that dynamically adapts perceptual processing based on both task objectives and execution progression through hypernetworks."
                    },
                    "pdf": {
                        "value": "/pdf/a03762ee97314515b8afdbc0804a0aa80d43e90c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsun2025hypertasr,\ntitle={Hyper{TASR}: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation},\nauthor={Li Sun and Jiefeng Wu and Feng Chen and Ruizhe Liu and Yanchao Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BcJlmjF1vV}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/391d5c00595d03ef2ec4d878ad458e1e6621e037.zip"
                    },
                    "paperhash": {
                        "value": "sun|hypertasr_hypernetworkdriven_taskaware_scene_representations_for_robust_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission240/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission240/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission240/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745170120013,
                "pdate": 1754680609809,
                "odate": 1758062760947,
                "mdate": 1758062811062,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission240/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "2y7TSgwqAB",
        "title": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions",
        "abstract": "We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and segmentation mask generation, particularly for stuff-type target regions with ambiguous boundaries.\nExisting methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets.\nTo overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. \nTo evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target.\nGENNAV achieved superior performance over baseline methods on standard evaluation metrics.\nFurthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance.\nIn these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments.",
        "keywords": [
            "Autonomous driving",
            "Vision and Language",
            "Semantic Understanding"
        ],
        "pdf_url": "https://openreview.net/pdf/a61b15d04da3b0ee1b91d098c4808b3f9d8560f2.pdf",
        "reviews": [
            {
                "id": "pdUC7cilz5",
                "forum": "2y7TSgwqAB",
                "replyto": "2y7TSgwqAB",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission236/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268468,
                "mdate": 1754869464435,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "2y7TSgwqAB",
                "forum": "2y7TSgwqAB",
                "content": {
                    "title": {
                        "value": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions"
                    },
                    "authors": {
                        "value": [
                            "Kei Katsumata",
                            "Yui Iioka",
                            "Naoki Hosomi",
                            "Teruhisa Misu",
                            "Kentaro Yamada",
                            "Komei Sugiura"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kei_Katsumata1",
                            "~Yui_Iioka1",
                            "~Naoki_Hosomi1",
                            "~Teruhisa_Misu2",
                            "~Kentaro_Yamada1",
                            "~Komei_Sugiura1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Autonomous driving",
                            "Vision and Language",
                            "Semantic Understanding"
                        ]
                    },
                    "abstract": {
                        "value": "We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and segmentation mask generation, particularly for stuff-type target regions with ambiguous boundaries.\nExisting methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets.\nTo overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. \nTo evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target.\nGENNAV achieved superior performance over baseline methods on standard evaluation metrics.\nFurthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance.\nIn these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments."
                    },
                    "supplementary_material": {
                        "value": "/attachment/2cc982cf010a4e5781cc4033589e3844d57cbd56.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We propose GENNAV, a polygon-based segmentation method which predicts the existence of target regions and generates segmentation masks for stuff-type target regions."
                    },
                    "pdf": {
                        "value": "/pdf/a61b15d04da3b0ee1b91d098c4808b3f9d8560f2.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkatsumata2025gennav,\ntitle={{GENNAV}: Polygon Mask Generation for Generalized Referring Navigable Regions},\nauthor={Kei Katsumata and Yui Iioka and Naoki Hosomi and Teruhisa Misu and Kentaro Yamada and Komei Sugiura},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2y7TSgwqAB}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a5760e05e92e5fcf2ddf1e915aceb2a4d87fda26.mp4"
                    },
                    "paperhash": {
                        "value": "katsumata|gennav_polygon_mask_generation_for_generalized_referring_navigable_regions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission236/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission236/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission236/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745158750378,
                "pdate": 1754680609569,
                "odate": 1758062760804,
                "mdate": 1758062811030,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission236/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "EXgckdYESp",
        "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation",
        "abstract": "RGB-based novel object pose estimation is critical for rapid deployment in robotic applications, yet zero-shot generalization remains a key challenge. In this paper, we introduce PicoPose, a novel framework designed to tackle this task using a three-stage pixel-to-pixel correspondence learning process. Firstly, PicoPose matches features from the RGB observation with those from rendered object templates, identifying the best-matched template and establishing coarse correspondences. Secondly, PicoPose smooths the correspondences by globally regressing a 2D affine transformation, including in-plane rotation, scale, and 2D translation, from the coarse correspondence map. Thirdly, PicoPose applies the affine transformation to the feature map of the best-matched template and learns correspondence offsets within local regions to achieve fine-grained correspondences. By progressively refining the correspondences, PicoPose significantly improves the accuracy of object poses computed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the seven core datasets of the BOP benchmark, demonstrating exceptional generalization to novel objects. Our code and models will be made publicly available.",
        "keywords": [
            "Novel Object Pose Estimation",
            "Robotic Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/fba0814dc176082adc8f680c9b911a513ebe8104.pdf",
        "reviews": [
            {
                "id": "ThROLIzxrI",
                "forum": "EXgckdYESp",
                "replyto": "EXgckdYESp",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission234/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268287,
                "mdate": 1754869464264,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "EXgckdYESp",
                "forum": "EXgckdYESp",
                "content": {
                    "title": {
                        "value": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation"
                    },
                    "authors": {
                        "value": [
                            "Lihua Liu",
                            "Jiehong Lin",
                            "ZhenXin Liu",
                            "Kui Jia"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Lihua_Liu1",
                            "~Jiehong_Lin1",
                            "~ZhenXin_Liu1",
                            "~Kui_Jia1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Novel Object Pose Estimation",
                            "Robotic Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "RGB-based novel object pose estimation is critical for rapid deployment in robotic applications, yet zero-shot generalization remains a key challenge. In this paper, we introduce PicoPose, a novel framework designed to tackle this task using a three-stage pixel-to-pixel correspondence learning process. Firstly, PicoPose matches features from the RGB observation with those from rendered object templates, identifying the best-matched template and establishing coarse correspondences. Secondly, PicoPose smooths the correspondences by globally regressing a 2D affine transformation, including in-plane rotation, scale, and 2D translation, from the coarse correspondence map. Thirdly, PicoPose applies the affine transformation to the feature map of the best-matched template and learns correspondence offsets within local regions to achieve fine-grained correspondences. By progressively refining the correspondences, PicoPose significantly improves the accuracy of object poses computed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the seven core datasets of the BOP benchmark, demonstrating exceptional generalization to novel objects. Our code and models will be made publicly available."
                    },
                    "supplementary_material": {
                        "value": "/attachment/32bbd7571bfd4def9fd36e5e6a2a3a758b285194.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/fba0814dc176082adc8f680c9b911a513ebe8104.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025picopose,\ntitle={PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation},\nauthor={Lihua Liu and Jiehong Lin and ZhenXin Liu and Kui Jia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EXgckdYESp}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2eb878c52982e7f6d00622371a47757e8f20329a.mp4"
                    },
                    "paperhash": {
                        "value": "liu|picopose_progressive_pixeltopixel_correspondence_learning_for_novel_object_pose_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission234/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission234/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission234/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745152405688,
                "pdate": 1754680609352,
                "odate": 1758062760745,
                "mdate": 1758062811026,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission234/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "tXY6VQlXfA",
        "title": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks",
        "abstract": "Diffusion-based planners have shown strong performance in short-horizon tasks but often fail in complex, long-horizon settings. We trace the failure to loose coupling between high-level (HL) sub-goal selection and low-level (LL) trajectory generation, which leads to incoherent plans and degraded performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that models HL sub-goals and LL trajectories jointly within a unified diffusion process. A shared classifier passes LL feedback upstream so that sub-goals self-correct while sampling proceeds. This tight HL–LL coupling improves trajectory coherence and enables scalable long-horizon diffusion planning. Experiments across maze navigation, tabletop manipulation, and household environments show that CHD consistently outperforms both flat and hierarchical diffusion baselines.",
        "keywords": [
            "Diffusion Planner",
            "Long-horizon Planning",
            "Hierarchical Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/fd629ad560f373c768455fdbed163ea57b1e9c1d.pdf",
        "reviews": [
            {
                "id": "dBtqwqo8Xi",
                "forum": "tXY6VQlXfA",
                "replyto": "tXY6VQlXfA",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission230/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068268186,
                "mdate": 1754869464332,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "tXY6VQlXfA",
                "forum": "tXY6VQlXfA",
                "content": {
                    "title": {
                        "value": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks"
                    },
                    "authors": {
                        "value": [
                            "Ce Hao",
                            "Anxing Xiao",
                            "Zhiwei Xue",
                            "Harold Soh"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ce_Hao1",
                            "~Anxing_Xiao1",
                            "~Zhiwei_Xue1",
                            "~Harold_Soh1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Diffusion Planner",
                            "Long-horizon Planning",
                            "Hierarchical Planning"
                        ]
                    },
                    "abstract": {
                        "value": "Diffusion-based planners have shown strong performance in short-horizon tasks but often fail in complex, long-horizon settings. We trace the failure to loose coupling between high-level (HL) sub-goal selection and low-level (LL) trajectory generation, which leads to incoherent plans and degraded performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that models HL sub-goals and LL trajectories jointly within a unified diffusion process. A shared classifier passes LL feedback upstream so that sub-goals self-correct while sampling proceeds. This tight HL–LL coupling improves trajectory coherence and enables scalable long-horizon diffusion planning. Experiments across maze navigation, tabletop manipulation, and household environments show that CHD consistently outperforms both flat and hierarchical diffusion baselines."
                    },
                    "supplementary_material": {
                        "value": "/attachment/a44f3becbc9abe520cee0bee2db3d7cbdbc79368.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "Coupled Hierarchical Diffusion (CHD) jointly models sub-goals and trajectories with feedback-driven correction for scalable long-horizon planning."
                    },
                    "pdf": {
                        "value": "/pdf/fd629ad560f373c768455fdbed163ea57b1e9c1d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhao2025chd,\ntitle={{CHD}: Coupled Hierarchical Diffusion for Long-Horizon Tasks},\nauthor={Ce Hao and Anxing Xiao and Zhiwei Xue and Harold Soh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=tXY6VQlXfA}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0e1f49ed613f719ae4c38b661d6d32629ce6be0c.zip"
                    },
                    "paperhash": {
                        "value": "hao|chd_coupled_hierarchical_diffusion_for_longhorizon_tasks"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission230/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission230/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission230/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745149785008,
                "pdate": 1754680609093,
                "odate": 1758062760600,
                "mdate": 1758062810814,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission230/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9FpccnRarn",
        "title": "BEVCalib: LiDAR-Camera Calibration via Geometry-Guided Bird’s-Eye View Representation",
        "abstract": "Accurate LiDAR-camera calibration is the foundation of accurate multimodal fusion environmental perception for autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCalib. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometry information from the BEV feature, we introduce a novel feature selector to choose the most important feature in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations in various datasets demonstrate that BEVCalib establishes a new state-of-the-art; improving the best open-source baseline by two orders of magnitude on KITTI, Nuscenes, and our dynamic extrinsic dataset, respectively, and outperforming the best baseline in literature by 72% on KITTI dataset, and 69% on Nuscenes dataset. All source code and checkpoints will be released.",
        "keywords": [
            "LiDAR-Camera Calibration",
            "Autonomous Driving",
            "BEV Features"
        ],
        "pdf_url": "https://openreview.net/pdf/2560dd5a27aa441c51028b43b2b23e1fd2ce4bc1.pdf",
        "reviews": [
            {
                "id": "rWOuhRU1Fw",
                "forum": "9FpccnRarn",
                "replyto": "9FpccnRarn",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission226/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267674,
                "mdate": 1754869464006,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9FpccnRarn",
                "forum": "9FpccnRarn",
                "content": {
                    "title": {
                        "value": "BEVCalib: LiDAR-Camera Calibration via Geometry-Guided Bird’s-Eye View Representation"
                    },
                    "authors": {
                        "value": [
                            "Weiduo Yuan",
                            "Jerry Li",
                            "Justin Yue",
                            "Divyank Shah",
                            "Konstantinos Karydis",
                            "Hang Qiu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Weiduo_Yuan1",
                            "~Jerry_Li5",
                            "~Justin_Yue1",
                            "~Divyank_Shah1",
                            "~Konstantinos_Karydis2",
                            "~Hang_Qiu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "LiDAR-Camera Calibration",
                            "Autonomous Driving",
                            "BEV Features"
                        ]
                    },
                    "TLDR": {
                        "value": "BEVCalib, the first model using bird's-eye view(BEV) features to perform LiDAR-camera calibration from raw data."
                    },
                    "abstract": {
                        "value": "Accurate LiDAR-camera calibration is the foundation of accurate multimodal fusion environmental perception for autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCalib. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometry information from the BEV feature, we introduce a novel feature selector to choose the most important feature in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations in various datasets demonstrate that BEVCalib establishes a new state-of-the-art; improving the best open-source baseline by two orders of magnitude on KITTI, Nuscenes, and our dynamic extrinsic dataset, respectively, and outperforming the best baseline in literature by 72% on KITTI dataset, and 69% on Nuscenes dataset. All source code and checkpoints will be released."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2560dd5a27aa441c51028b43b2b23e1fd2ce4bc1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyuan2025bevcalib,\ntitle={{BEVC}alib: Li{DAR}-Camera Calibration via Geometry-Guided Bird{\\textquoteright}s-Eye View Representation},\nauthor={Weiduo Yuan and Jerry Li and Justin Yue and Divyank Shah and Konstantinos Karydis and Hang Qiu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9FpccnRarn}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/933aad7afa3ae3d7b8ae2af859a2801b79cdd2cf.mp4"
                    },
                    "paperhash": {
                        "value": "yuan|bevcalib_lidarcamera_calibration_via_geometryguided_birdseye_view_representation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission226/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission226/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745124784790,
                "pdate": 1754680608913,
                "odate": 1758062760469,
                "mdate": 1758062810784,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission226/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "EcOGafgvuC",
        "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots",
        "abstract": "Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities.\nHowever, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions.\nTo address this limitation, we present Force-Adaptive Control via Impedance Reference Tracking (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. \nIn simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses\n(up to 200 Ns)\nand exhibits controllable compliance, achieving an 80\\% reduction in collision impulse. The policy is deployed to a physical robot, demonstrating both compliant behavior, such as initiation/cessation of movement with finger tip, and the ability to pull payloads up to 10kg. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control.",
        "keywords": [
            "Reinforcement Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/5c90275172bd3af87eea4da28fd7194148602fbb.pdf",
        "reviews": [
            {
                "id": "KXuPwixHjG",
                "forum": "EcOGafgvuC",
                "replyto": "EcOGafgvuC",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission225/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267281,
                "mdate": 1754869455765,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "EcOGafgvuC",
                "forum": "EcOGafgvuC",
                "content": {
                    "title": {
                        "value": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots"
                    },
                    "authors": {
                        "value": [
                            "Botian Xu",
                            "Haoyang Weng",
                            "Qingzhou Lu",
                            "Yang Gao",
                            "Huazhe Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Botian_Xu1",
                            "~Haoyang_Weng1",
                            "~Qingzhou_Lu1",
                            "~Yang_Gao1",
                            "~Huazhe_Xu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities.\nHowever, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions.\nTo address this limitation, we present Force-Adaptive Control via Impedance Reference Tracking (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. \nIn simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses\n(up to 200 Ns)\nand exhibits controllable compliance, achieving an 80\\% reduction in collision impulse. The policy is deployed to a physical robot, demonstrating both compliant behavior, such as initiation/cessation of movement with finger tip, and the ability to pull payloads up to 10kg. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f74133c817e210084c69b9ded9f22857c5484e21.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5c90275172bd3af87eea4da28fd7194148602fbb.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025facet,\ntitle={{FACET}: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots},\nauthor={Botian Xu and Haoyang Weng and Qingzhou Lu and Yang Gao and Huazhe Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EcOGafgvuC}\n}"
                    },
                    "paperhash": {
                        "value": "xu|facet_forceadaptive_control_via_impedance_reference_tracking_for_legged_robots"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission225/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission225/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission225/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745124692964,
                "pdate": 1754680608852,
                "odate": 1758062760426,
                "mdate": 1758062810773,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission225/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bmpDAqsJov",
        "title": "Latent Adaptive Planner for Dynamic Manipulation",
        "abstract": "This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. \n    Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. \n    We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. \n    Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.",
        "keywords": [
            "Imitation Learning",
            "Robotics",
            "Dynamic Nonprehensile Manipulation",
            "Latent Space Planning",
            "Classical Variational Bayes",
            "Video-based Skill Acquisition"
        ],
        "pdf_url": "https://openreview.net/pdf/65219ea5d3d16982f2190bce6f617009a2e47a63.pdf",
        "reviews": [
            {
                "id": "3kJuaJPFIT",
                "forum": "bmpDAqsJov",
                "replyto": "bmpDAqsJov",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission224/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267316,
                "mdate": 1754869464036,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bmpDAqsJov",
                "forum": "bmpDAqsJov",
                "content": {
                    "title": {
                        "value": "Latent Adaptive Planner for Dynamic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Donghun Noh",
                            "Deqian Kong",
                            "Minglu Zhao",
                            "Andrew Lizarraga",
                            "Jianwen Xie",
                            "Ying Nian Wu",
                            "Dennis Hong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Donghun_Noh1",
                            "~Deqian_Kong1",
                            "~Minglu_Zhao1",
                            "~Andrew_Lizarraga1",
                            "~Jianwen_Xie1",
                            "~Ying_Nian_Wu1",
                            "~Dennis_Hong2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Robotics",
                            "Dynamic Nonprehensile Manipulation",
                            "Latent Space Planning",
                            "Classical Variational Bayes",
                            "Video-based Skill Acquisition"
                        ]
                    },
                    "abstract": {
                        "value": "This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. \n    Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. \n    We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. \n    Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos."
                    },
                    "supplementary_material": {
                        "value": "/attachment/92e805c7c1e52709f5a79afc2d07b657b19727f4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/65219ea5d3d16982f2190bce6f617009a2e47a63.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nnoh2025latent,\ntitle={Latent Adaptive Planner for Dynamic Manipulation},\nauthor={Donghun Noh and Deqian Kong and Minglu Zhao and Andrew Lizarraga and Jianwen Xie and Ying Nian Wu and Dennis Hong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bmpDAqsJov}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/96a4720ac7ed12c641ed1031250e8a1b53aad5bc.mp4"
                    },
                    "paperhash": {
                        "value": "noh|latent_adaptive_planner_for_dynamic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission224/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission224/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission224/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745123785787,
                "pdate": 1754680608765,
                "odate": 1758062760333,
                "mdate": 1758062810543,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission224/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "SpFH8T7gjM",
        "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL",
        "abstract": "Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors.",
        "keywords": [
            "Real-world RL",
            "Latent Action",
            "Whole-body Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/89c1f3ecf9a55d3f6035a07fad2951dc468f392b.pdf",
        "reviews": [
            {
                "id": "b7NESaoHFE",
                "forum": "SpFH8T7gjM",
                "replyto": "SpFH8T7gjM",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission223/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267306,
                "mdate": 1754869463927,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "SpFH8T7gjM",
                "forum": "SpFH8T7gjM",
                "content": {
                    "title": {
                        "value": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL"
                    },
                    "authors": {
                        "value": [
                            "Jiaheng Hu",
                            "Peter Stone",
                            "Roberto Martín-Martín"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiaheng_Hu1",
                            "~Peter_Stone1",
                            "~Roberto_Martín-Martín1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Real-world RL",
                            "Latent Action",
                            "Whole-body Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We introduce SLAC, a method that makes real-world RL feasible for high DoF robots by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space."
                    },
                    "pdf": {
                        "value": "/pdf/89c1f3ecf9a55d3f6035a07fad2951dc468f392b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhu2025slac,\ntitle={{SLAC}: Simulation-Pretrained Latent Action Space for Whole-Body Real-World {RL}},\nauthor={Jiaheng Hu and Peter Stone and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SpFH8T7gjM}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/27fdd075cd50e5799bd8474b942a65e3cfc62a66.zip"
                    },
                    "paperhash": {
                        "value": "hu|slac_simulationpretrained_latent_action_space_for_wholebody_realworld_rl"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission223/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission223/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission223/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745121103107,
                "pdate": 1754680608721,
                "odate": 1758062760331,
                "mdate": 1758062810513,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission223/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HqqyJ9A2fy",
        "title": "Neural Robot Dynamics",
        "abstract": "Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose *NeRD* (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. *NeRD* uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned *NeRD* models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the *NeRD* simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality.",
        "keywords": [
            "Robot Model Learning; Robotics Simulation; Neural Simulation"
        ],
        "pdf_url": "https://openreview.net/pdf/d1f997956bf6fa0036ffab5131082fc593eee591.pdf",
        "reviews": [
            {
                "id": "8ZkSLsa6Y1",
                "forum": "HqqyJ9A2fy",
                "replyto": "HqqyJ9A2fy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission222/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267071,
                "mdate": 1754869463628,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HqqyJ9A2fy",
                "forum": "HqqyJ9A2fy",
                "content": {
                    "title": {
                        "value": "Neural Robot Dynamics"
                    },
                    "authors": {
                        "value": [
                            "Jie Xu",
                            "Eric Heiden",
                            "Iretiayo Akinola",
                            "Dieter Fox",
                            "Miles Macklin",
                            "Yashraj Narang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jie_Xu7",
                            "~Eric_Heiden1",
                            "~Iretiayo_Akinola1",
                            "~Dieter_Fox1",
                            "~Miles_Macklin1",
                            "~Yashraj_Narang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Model Learning; Robotics Simulation; Neural Simulation"
                        ]
                    },
                    "TLDR": {
                        "value": "We learn a robot-centric neural physics module that is genearlizable across different tasks and environment setups for a particular robot."
                    },
                    "abstract": {
                        "value": "Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose *NeRD* (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. *NeRD* uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned *NeRD* models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the *NeRD* simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d924fa3f2e0b67ceef855b7a5cc7cebabd993370.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d1f997956bf6fa0036ffab5131082fc593eee591.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025neural,\ntitle={Neural Robot Dynamics},\nauthor={Jie Xu and Eric Heiden and Iretiayo Akinola and Dieter Fox and Miles Macklin and Yashraj Narang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HqqyJ9A2fy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/52e3c7985c3e495d854b28a46dfdaf1a17863e83.zip"
                    },
                    "paperhash": {
                        "value": "xu|neural_robot_dynamics"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission222/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission222/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission222/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745106013810,
                "pdate": 1754680608644,
                "odate": 1758062760298,
                "mdate": 1758062810508,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission222/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission222/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "XrgRvBklWu",
        "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation",
        "abstract": "We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI incorporates hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap with a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. Our software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86\\%.",
        "keywords": [
            "Dexterous Manipulation",
            "Learning from Human",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/f742eb7932a161b7e7df59c4b42a3533242bf032.pdf",
        "reviews": [
            {
                "id": "LSxXC1uJfq",
                "forum": "XrgRvBklWu",
                "replyto": "XrgRvBklWu",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission220/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267040,
                "mdate": 1754869455706,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "XrgRvBklWu",
                "forum": "XrgRvBklWu",
                "content": {
                    "title": {
                        "value": "DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Mengda Xu",
                            "Han Zhang",
                            "Yifan Hou",
                            "Zhenjia Xu",
                            "Linxi Fan",
                            "Manuela Veloso",
                            "Shuran Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Mengda_Xu1",
                            "~Han_Zhang25",
                            "~Yifan_Hou2",
                            "~Zhenjia_Xu1",
                            "~Linxi_Fan2",
                            "~Manuela_Veloso1",
                            "~Shuran_Song3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Manipulation",
                            "Learning from Human",
                            "Imitation Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands."
                    },
                    "abstract": {
                        "value": "We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI incorporates hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap with a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. Our software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86\\%."
                    },
                    "supplementary_material": {
                        "value": "/attachment/351c22c96ac412c469b6d0c729d86c1785ce5583.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f742eb7932a161b7e7df59c4b42a3533242bf032.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025dexumi,\ntitle={Dex{UMI}: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation},\nauthor={Mengda Xu and Han Zhang and Yifan Hou and Zhenjia Xu and Linxi Fan and Manuela Veloso and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XrgRvBklWu}\n}"
                    },
                    "paperhash": {
                        "value": "xu|dexumi_using_human_hand_as_the_universal_manipulation_interface_for_dexterous_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission220/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission220/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission220/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745095104561,
                "pdate": 1754680608591,
                "odate": 1758062760193,
                "mdate": 1758062810280,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission220/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "a9RXjOt5bU",
        "title": "The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio",
        "abstract": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories---without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap.",
        "keywords": [
            "generative modeling",
            "real2sim",
            "sim2real",
            "multimodal learning"
        ],
        "pdf_url": "https://openreview.net/pdf/ad967f9b3db43ec92a5173c3bfdfc04038ea321e.pdf",
        "reviews": [
            {
                "id": "KYHUXvxxC9",
                "forum": "a9RXjOt5bU",
                "replyto": "a9RXjOt5bU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission219/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068267028,
                "mdate": 1754869455541,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "a9RXjOt5bU",
                "forum": "a9RXjOt5bU",
                "content": {
                    "title": {
                        "value": "The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio"
                    },
                    "authors": {
                        "value": [
                            "Renhao Wang",
                            "Haoran Geng",
                            "Tingle Li",
                            "Philipp Wu",
                            "Feishi Wang",
                            "Gopala Anumanchipalli",
                            "Trevor Darrell",
                            "Boyi Li",
                            "Pieter Abbeel",
                            "Jitendra Malik",
                            "Alexei A Efros"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Renhao_Wang1",
                            "~Haoran_Geng1",
                            "~Tingle_Li1",
                            "~Philipp_Wu1",
                            "~Feishi_Wang1",
                            "~Gopala_Anumanchipalli1",
                            "~Trevor_Darrell2",
                            "~Boyi_Li1",
                            "~Pieter_Abbeel2",
                            "~Jitendra_Malik2",
                            "~Alexei_A_Efros1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "generative modeling",
                            "real2sim",
                            "sim2real",
                            "multimodal learning"
                        ]
                    },
                    "abstract": {
                        "value": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories---without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We augment physics-based simulators with generative models capable of simulating modalities traditionally absent from simulation, enabling learning multimodal sim2real policies that transfer zero-shot to real-world tasks."
                    },
                    "pdf": {
                        "value": "/pdf/ad967f9b3db43ec92a5173c3bfdfc04038ea321e.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025the,\ntitle={The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio},\nauthor={Renhao Wang and Haoran Geng and Tingle Li and Philipp Wu and Feishi Wang and Gopala Anumanchipalli and Trevor Darrell and Boyi Li and Pieter Abbeel and Jitendra Malik and Alexei A Efros},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a9RXjOt5bU}\n}"
                    },
                    "paperhash": {
                        "value": "wang|the_sound_of_simulation_learning_multimodal_simtoreal_robot_policies_with_generative_audio"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission219/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission219/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission219/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1745094529908,
                "pdate": 1754680608463,
                "odate": 1758062760119,
                "mdate": 1758062810252,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission219/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission219/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "mjYKNIRqpy",
        "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
        "abstract": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct the navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show our framework can effectively generalize to new environments and instruction sets, paving the way for more robust and autonomous navigation framework.",
        "keywords": [
            "Vision-Language Navigation",
            "Graph Constraint"
        ],
        "pdf_url": "https://openreview.net/pdf/162837b45913fbd91cef4157e53ac31668315cd2.pdf",
        "reviews": [
            {
                "id": "ewVSesBbE3",
                "forum": "mjYKNIRqpy",
                "replyto": "mjYKNIRqpy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission214/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068266386,
                "mdate": 1754869463608,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "mjYKNIRqpy",
                "forum": "mjYKNIRqpy",
                "content": {
                    "title": {
                        "value": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation"
                    },
                    "authors": {
                        "value": [
                            "Hang Yin",
                            "Haoyu Wei",
                            "Xiuwei Xu",
                            "Wenxuan Guo",
                            "Jie Zhou",
                            "Jiwen Lu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hang_Yin5",
                            "~Haoyu_Wei3",
                            "~Xiuwei_Xu1",
                            "~Wenxuan_Guo3",
                            "~Jie_Zhou3",
                            "~Jiwen_Lu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language Navigation",
                            "Graph Constraint"
                        ]
                    },
                    "abstract": {
                        "value": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct the navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show our framework can effectively generalize to new environments and instruction sets, paving the way for more robust and autonomous navigation framework."
                    },
                    "supplementary_material": {
                        "value": "/attachment/05d8c3479971c7c2225791d0c1cb1efb795772b7.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/162837b45913fbd91cef4157e53ac31668315cd2.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyin2025gcvln,\ntitle={{GC}-{VLN}: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation},\nauthor={Hang Yin and Haoyu Wei and Xiuwei Xu and Wenxuan Guo and Jie Zhou and Jiwen Lu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=mjYKNIRqpy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0e6f768f62aebf09d78b1de98d8099b043974307.mp4"
                    },
                    "paperhash": {
                        "value": "yin|gcvln_instruction_as_graph_constraints_for_trainingfree_visionandlanguage_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission214/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission214/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission214/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745076171875,
                "pdate": 1754680607926,
                "odate": 1758062759944,
                "mdate": 1758062810244,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission214/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "TFbT7kHD89",
        "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality",
        "abstract": "Learning from demonstration has proven effective in robotics for acquiring natural behaviors, such as stylistic motions and lifelike agility, particularly when explicitly defining style-oriented reward functions is challenging. Synthesizing stylistic motions for real-world tasks usually requires balancing task performance and imitation quality. Existing methods generally depend on expert demonstrations closely aligned with task objectives. However, practical demonstrations are often incomplete or unrealistic, causing current methods to boost style at the expense of task performance. To address this issue, we propose formulating the problem as a  constrained Markov Decision Process (CMDP). Our approach integrates a style-imitation objective with constraints to maintain near-optimal task performance. We introduce an adaptively adjustable Lagrangian multiplier to guide the agent to imitate demonstrations selectively, capturing stylistic nuances without compromising task performance. We validate our approach across multiple robotic platforms and tasks, demonstrating both robust task performance and high-fidelity style learning. On ANYmal-D hardware we show a 14.5\\% drop in mechanical energy and a more agile gait pattern, showcasing real-world benefits.",
        "keywords": [
            "Constrained Markov Decision Process",
            "Imitation Learning",
            "Legged Robots"
        ],
        "pdf_url": "https://openreview.net/pdf/6ebaeb35203b73511c2a1aededb2ecf77029cf95.pdf",
        "reviews": [
            {
                "id": "nBEfa2RA3z",
                "forum": "TFbT7kHD89",
                "replyto": "TFbT7kHD89",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission212/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068266399,
                "mdate": 1754869463507,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "TFbT7kHD89",
                "forum": "TFbT7kHD89",
                "content": {
                    "title": {
                        "value": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality"
                    },
                    "authors": {
                        "value": [
                            "Kehan Wen",
                            "Chenhao Li",
                            "Junzhe He",
                            "Marco Hutter"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kehan_Wen1",
                            "~Chenhao_Li3",
                            "~Junzhe_He1",
                            "~Marco_Hutter1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Constrained Markov Decision Process",
                            "Imitation Learning",
                            "Legged Robots"
                        ]
                    },
                    "abstract": {
                        "value": "Learning from demonstration has proven effective in robotics for acquiring natural behaviors, such as stylistic motions and lifelike agility, particularly when explicitly defining style-oriented reward functions is challenging. Synthesizing stylistic motions for real-world tasks usually requires balancing task performance and imitation quality. Existing methods generally depend on expert demonstrations closely aligned with task objectives. However, practical demonstrations are often incomplete or unrealistic, causing current methods to boost style at the expense of task performance. To address this issue, we propose formulating the problem as a  constrained Markov Decision Process (CMDP). Our approach integrates a style-imitation objective with constraints to maintain near-optimal task performance. We introduce an adaptively adjustable Lagrangian multiplier to guide the agent to imitate demonstrations selectively, capturing stylistic nuances without compromising task performance. We validate our approach across multiple robotic platforms and tasks, demonstrating both robust task performance and high-fidelity style learning. On ANYmal-D hardware we show a 14.5\\% drop in mechanical energy and a more agile gait pattern, showcasing real-world benefits."
                    },
                    "supplementary_material": {
                        "value": "/attachment/5acf0b508d54cc6a987adaf55617ead20fe978d2.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A Constraint Markov Decision Process (CMDP)–based policy optimization approach for learning from imperfect demonstrations to achieve lifelike agility and energy-efficient motions while enforcing near-optimal task performance."
                    },
                    "pdf": {
                        "value": "/pdf/6ebaeb35203b73511c2a1aededb2ecf77029cf95.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwen2025constrained,\ntitle={Constrained Style Learning from Imperfect Demonstrations under Task Optimality},\nauthor={Kehan Wen and Chenhao Li and Junzhe He and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=TFbT7kHD89}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/e062c3d5e286f2651e7f90bedc0de5419177b7de.mp4"
                    },
                    "paperhash": {
                        "value": "wen|constrained_style_learning_from_imperfect_demonstrations_under_task_optimality"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission212/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission212/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission212/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745069256121,
                "pdate": 1754680607605,
                "odate": 1758062759898,
                "mdate": 1758062810000,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission212/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission212/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "7OOMC7pzaw",
        "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types",
        "abstract": "Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates.",
        "keywords": [
            "Teleoperation",
            "Dexterous",
            "Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/7f14fee90ac9484c0fd76252c85725c0083c04d5.pdf",
        "reviews": [
            {
                "id": "KTU94FYRaD",
                "forum": "7OOMC7pzaw",
                "replyto": "7OOMC7pzaw",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission211/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068266413,
                "mdate": 1754869463307,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "7OOMC7pzaw",
                "forum": "7OOMC7pzaw",
                "content": {
                    "title": {
                        "value": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types"
                    },
                    "authors": {
                        "value": [
                            "Yuhao Lin",
                            "Yi-Lin Wei",
                            "Haoran Liao",
                            "Mu Lin",
                            "Chengyi Xing",
                            "Hao Li",
                            "Dandan Zhang",
                            "Mark Cutkosky",
                            "Wei-Shi Zheng"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuhao_Lin9",
                            "~Yi-Lin_Wei1",
                            "~Haoran_Liao6",
                            "~Mu_Lin1",
                            "~Chengyi_Xing1",
                            "~Hao_Li23",
                            "~Dandan_Zhang6",
                            "~Mark_Cutkosky1",
                            "~Wei-Shi_Zheng3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Teleoperation",
                            "Dexterous",
                            "Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e79ad958e953bf87a8c9e5e17375b0931aa500a4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7f14fee90ac9484c0fd76252c85725c0083c04d5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlin2025typetele,\ntitle={TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types},\nauthor={Yuhao Lin and Yi-Lin Wei and Haoran Liao and Mu Lin and Chengyi Xing and Hao Li and Dandan Zhang and Mark Cutkosky and Wei-Shi Zheng},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7OOMC7pzaw}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/552e3c392f1817f419ef8941ad2d0e782f9d2234.mp4"
                    },
                    "paperhash": {
                        "value": "lin|typetele_releasing_dexterity_in_teleoperation_by_dexterous_manipulation_types"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission211/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission211/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission211/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745065852941,
                "pdate": 1754680607529,
                "odate": 1758062759868,
                "mdate": 1758062809964,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission211/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bOVF8Rj33i",
        "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning",
        "abstract": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback—a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning.\nOur project page is available at https://vt-refine.github.io/ .",
        "keywords": [
            "Tactile Simulation",
            "Bimanual Manipulation",
            "RL Fine-Tuning"
        ],
        "pdf_url": "https://openreview.net/pdf/919aa81cf488976df7c755bed201b10f79318082.pdf",
        "reviews": [
            {
                "id": "h3jnDachWz",
                "forum": "bOVF8Rj33i",
                "replyto": "bOVF8Rj33i",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission208/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265988,
                "mdate": 1754869463266,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bOVF8Rj33i",
                "forum": "bOVF8Rj33i",
                "content": {
                    "title": {
                        "value": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning"
                    },
                    "authors": {
                        "value": [
                            "Binghao Huang",
                            "Jie Xu",
                            "Iretiayo Akinola",
                            "Wei Yang",
                            "Balakumar Sundaralingam",
                            "Rowland O'Flaherty",
                            "Dieter Fox",
                            "Xiaolong Wang",
                            "Arsalan Mousavian",
                            "Yu-Wei Chao",
                            "Yunzhu Li"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Binghao_Huang1",
                            "~Jie_Xu7",
                            "~Iretiayo_Akinola1",
                            "~Wei_Yang2",
                            "~Balakumar_Sundaralingam1",
                            "~Rowland_O'Flaherty1",
                            "~Dieter_Fox1",
                            "~Xiaolong_Wang3",
                            "~Arsalan_Mousavian1",
                            "~Yu-Wei_Chao1",
                            "~Yunzhu_Li1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Simulation",
                            "Bimanual Manipulation",
                            "RL Fine-Tuning"
                        ]
                    },
                    "abstract": {
                        "value": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback—a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning.\nOur project page is available at https://vt-refine.github.io/ ."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/919aa81cf488976df7c755bed201b10f79318082.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhuang2025vtrefine,\ntitle={{VT}-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning},\nauthor={Binghao Huang and Jie Xu and Iretiayo Akinola and Wei Yang and Balakumar Sundaralingam and Rowland O'Flaherty and Dieter Fox and Xiaolong Wang and Arsalan Mousavian and Yu-Wei Chao and Yunzhu Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bOVF8Rj33i}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/a0a74726f8017832100f6fc733d2bfd9fd3dc95a.mp4"
                    },
                    "paperhash": {
                        "value": "huang|vtrefine_learning_bimanual_assembly_with_visuotactile_feedback_via_simulation_finetuning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission208/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission208/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission208/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745041878469,
                "pdate": 1754680607347,
                "odate": 1758062759735,
                "mdate": 1758062809947,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission208/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission208/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "sUWOSP6SUJ",
        "title": "Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning",
        "abstract": "Imitation Learning can train robots to perform complex and diverse manipulation tasks, but learned policies are brittle with observations outside of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to mitigate this, but in our evaluations with unseen embodiments and camera viewpoints they show only modest improvement. To address those challenges, we propose Adapt3R, a general-purpose 3D observation encoder which synthesizes data from calibrated RGBD cameras into a vector that can be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information, using 3D only as a medium to localize this information with respect to the end-effector. We show across 93 simulated and 6 real tasks that when trained end-to-end with a variety of IL algorithms, Adapt3R maintains these algorithms' learning capacity while enabling zero-shot transfer to novel embodiments and camera poses. For more results, visit https://adapt3r-robot.github.io.",
        "keywords": [
            "Imitation Learning",
            "3D Perception",
            "Cross-Embodiment Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/684fa6fa2e7911dc1cee8c460f6ae13920ef273f.pdf",
        "reviews": [
            {
                "id": "GjbWfycbjv",
                "forum": "sUWOSP6SUJ",
                "replyto": "sUWOSP6SUJ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission204/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265708,
                "mdate": 1754869463136,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "sUWOSP6SUJ",
                "forum": "sUWOSP6SUJ",
                "content": {
                    "title": {
                        "value": "Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning"
                    },
                    "authors": {
                        "value": [
                            "Albert Wilcox",
                            "Mohamed Ghanem",
                            "Masoud Moghani",
                            "Pierre Barroso",
                            "Benjamin Joffe",
                            "Animesh Garg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Albert_Wilcox1",
                            "~Mohamed_Ghanem2",
                            "~Masoud_Moghani1",
                            "~Pierre_Barroso1",
                            "~Benjamin_Joffe1",
                            "~Animesh_Garg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "3D Perception",
                            "Cross-Embodiment Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Imitation Learning can train robots to perform complex and diverse manipulation tasks, but learned policies are brittle with observations outside of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to mitigate this, but in our evaluations with unseen embodiments and camera viewpoints they show only modest improvement. To address those challenges, we propose Adapt3R, a general-purpose 3D observation encoder which synthesizes data from calibrated RGBD cameras into a vector that can be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information, using 3D only as a medium to localize this information with respect to the end-effector. We show across 93 simulated and 6 real tasks that when trained end-to-end with a variety of IL algorithms, Adapt3R maintains these algorithms' learning capacity while enabling zero-shot transfer to novel embodiments and camera poses. For more results, visit https://adapt3r-robot.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e196bb0d34d56de9faf8db808acd708611ae829e.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/684fa6fa2e7911dc1cee8c460f6ae13920ef273f.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwilcox2025adaptr,\ntitle={Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning},\nauthor={Albert Wilcox and Mohamed Ghanem and Masoud Moghani and Pierre Barroso and Benjamin Joffe and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sUWOSP6SUJ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/79dbdd093ce95ce94ec6509787ae60d08469b5ec.zip"
                    },
                    "paperhash": {
                        "value": "wilcox|adapt3r_adaptive_3d_scene_representation_for_domain_transfer_in_imitation_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission204/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission204/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission204/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745025264459,
                "pdate": 1754680607137,
                "odate": 1758062759481,
                "mdate": 1758062809750,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission204/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "19LSN4QnV4",
        "title": "FOMO-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection",
        "abstract": "In order to navigate complex traffic environments, self-driving vehicles\nmust recognize many semantic classes pertaining to vulnerable road users or\ntraffic control devices.  However, many safety-critical objects (e.g.,\nconstruction worker) appear infrequently in nominal traffic conditions, leading to a\nsevere shortage of training examples from driving data alone. \nRecent vision foundation models, which are trained on a large corpus of\ndata, can serve as a good source of external prior knowledge to improve\ngeneralization. We propose FOMO-3D, the first 3D detector\nto leverage vision foundation models for long-tailed 3D detection. Specifically,\nFOMO-3D exploits rich semantic and depth priors from OWLv2 and Metric3Dv2 within\na two-stage detection paradigm that first generates proposals with a\nLiDAR-based branch and a novel camera-based branch, and refines them with\nattention especially to image features from OWL. Evaluations on real-world\ndriving data show that using rich priors from vision\nfoundation models with careful multimodal fusion designs leads to large gains\nfor long-tailed 3D detection.",
        "keywords": [
            "Long-Tailed 3D Object Detection",
            "Vision Foundation Model",
            "Multimodal Fusion",
            "Autonomous Vehicles"
        ],
        "pdf_url": "https://openreview.net/pdf/90ae722f9c63423d5379d06de77659df3e262f46.pdf",
        "reviews": [
            {
                "id": "Y3U9Tirll9",
                "forum": "19LSN4QnV4",
                "replyto": "19LSN4QnV4",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission203/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265538,
                "mdate": 1754869462855,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "19LSN4QnV4",
                "forum": "19LSN4QnV4",
                "content": {
                    "title": {
                        "value": "FOMO-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection"
                    },
                    "authors": {
                        "value": [
                            "Anqi Joyce Yang",
                            "James Tu",
                            "Nikita Dvornik",
                            "Enxu Li",
                            "Raquel Urtasun"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Anqi_Joyce_Yang1",
                            "~James_Tu1",
                            "~Nikita_Dvornik1",
                            "~Enxu_Li1",
                            "~Raquel_Urtasun1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Long-Tailed 3D Object Detection",
                            "Vision Foundation Model",
                            "Multimodal Fusion",
                            "Autonomous Vehicles"
                        ]
                    },
                    "abstract": {
                        "value": "In order to navigate complex traffic environments, self-driving vehicles\nmust recognize many semantic classes pertaining to vulnerable road users or\ntraffic control devices.  However, many safety-critical objects (e.g.,\nconstruction worker) appear infrequently in nominal traffic conditions, leading to a\nsevere shortage of training examples from driving data alone. \nRecent vision foundation models, which are trained on a large corpus of\ndata, can serve as a good source of external prior knowledge to improve\ngeneralization. We propose FOMO-3D, the first 3D detector\nto leverage vision foundation models for long-tailed 3D detection. Specifically,\nFOMO-3D exploits rich semantic and depth priors from OWLv2 and Metric3Dv2 within\na two-stage detection paradigm that first generates proposals with a\nLiDAR-based branch and a novel camera-based branch, and refines them with\nattention especially to image features from OWL. Evaluations on real-world\ndriving data show that using rich priors from vision\nfoundation models with careful multimodal fusion designs leads to large gains\nfor long-tailed 3D detection."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f27ebcd89d157f0c0cae5bf8c325862fe3e4d8e0.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We propose FOMO-3D, the first to leverage vision foundation models (specifically OWLv2 for 2D object detection and Metric3Dv2 for dense depths) with novel multi-modal fusion designs to tackle long-tailed 3D object detection."
                    },
                    "pdf": {
                        "value": "/pdf/90ae722f9c63423d5379d06de77659df3e262f46.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyang2025fomod,\ntitle={{FOMO}-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection},\nauthor={Anqi Joyce Yang and James Tu and Nikita Dvornik and Enxu Li and Raquel Urtasun},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=19LSN4QnV4}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5585fd599bf1b243f373cd5e21b2692b41dd5629.zip"
                    },
                    "paperhash": {
                        "value": "yang|fomo3d_using_vision_foundation_models_for_longtailed_3d_object_detection"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission203/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission203/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission203/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745005507168,
                "pdate": 1754680607077,
                "odate": 1758062759459,
                "mdate": 1758062809638,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission203/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HMcBBIg1Th",
        "title": "Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance",
        "abstract": "Visual planning, by offering a sequence of intermediate visual subgoals to a goal-conditioned low-level policy, achieves promising performance on long-horizon manipulation tasks. To obtain the subgoals, existing methods typically resort to video generation models but suffer from model hallucination and computational cost. We present Vis2Plan, an efficient, explainable and white-box visual planning framework powered by symbolic guidance. From raw, unlabeled play data, Vis2Plan harnesses vision foundation models to automatically extract a compact set of task symbols, which allows building a high-level symbolic transition graph for multi-goal, multi-stage planning. At test time, given a desired task goal, our planner conducts planning at the symbolic level and assembles a sequence of physically consistent intermediate sub-goal images grounded by the underlying symbolic representation. Our Vis2Plan outperforms strong diffusion video generation-based visual planners by delivering 53\\% higher aggregate success rate while generating visual plans 35$\\times$ faster. The results indicate that Vis2Plan is able to generate physically consistent image goals while offering fully inspectable reasoning steps.",
        "keywords": [
            "offline imitation learning",
            "planning"
        ],
        "pdf_url": "https://openreview.net/pdf/dd9ed44b75d75077cd5877eafbccabf82ed93b31.pdf",
        "reviews": [
            {
                "id": "h5e3wNXv69",
                "forum": "HMcBBIg1Th",
                "replyto": "HMcBBIg1Th",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission202/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265527,
                "mdate": 1754869462794,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HMcBBIg1Th",
                "forum": "HMcBBIg1Th",
                "content": {
                    "title": {
                        "value": "Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance"
                    },
                    "authors": {
                        "value": [
                            "Wenyan Yang",
                            "Ahmet Tikna",
                            "Yi Zhao",
                            "Yuying Zhang",
                            "Luigi Palopoli",
                            "Marco Roveri",
                            "Joni Pajarinen"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Wenyan_Yang1",
                            "~Ahmet_Tikna1",
                            "~Yi_Zhao6",
                            "~Yuying_Zhang1",
                            "~Luigi_Palopoli2",
                            "~Marco_Roveri1",
                            "~Joni_Pajarinen2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "offline imitation learning",
                            "planning"
                        ]
                    },
                    "abstract": {
                        "value": "Visual planning, by offering a sequence of intermediate visual subgoals to a goal-conditioned low-level policy, achieves promising performance on long-horizon manipulation tasks. To obtain the subgoals, existing methods typically resort to video generation models but suffer from model hallucination and computational cost. We present Vis2Plan, an efficient, explainable and white-box visual planning framework powered by symbolic guidance. From raw, unlabeled play data, Vis2Plan harnesses vision foundation models to automatically extract a compact set of task symbols, which allows building a high-level symbolic transition graph for multi-goal, multi-stage planning. At test time, given a desired task goal, our planner conducts planning at the symbolic level and assembles a sequence of physically consistent intermediate sub-goal images grounded by the underlying symbolic representation. Our Vis2Plan outperforms strong diffusion video generation-based visual planners by delivering 53\\% higher aggregate success rate while generating visual plans 35$\\times$ faster. The results indicate that Vis2Plan is able to generate physically consistent image goals while offering fully inspectable reasoning steps."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/dd9ed44b75d75077cd5877eafbccabf82ed93b31.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyang2025extracting,\ntitle={Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance},\nauthor={Wenyan Yang and Ahmet Tikna and Yi Zhao and Yuying Zhang and Luigi Palopoli and Marco Roveri and Joni Pajarinen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HMcBBIg1Th}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8ad6ca9e447a8d606d399b5bae9e746f6e430caa.mp4"
                    },
                    "paperhash": {
                        "value": "yang|extracting_visual_plans_from_unlabeled_videos_via_symbolic_guidance"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission202/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission202/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission202/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1745004940406,
                "pdate": 1754680607015,
                "odate": 1758062759388,
                "mdate": 1758062809675,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission202/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission202/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "eLeCrM5PEO",
        "title": "Self-supervised perception for tactile skin covered dexterous hands",
        "abstract": "We present PercepSkin, a pre-trained encoder for magnetic skin sensors distributed across the fingertips, phalanges, and palm of a dexterous robot hand. \nMagnetic tactile skins offer a flexible form factor for hand-wide coverage with fast response times, in contrast to vision-based tactile sensors that are restricted to the fingertips and limited by bandwidth. Full hand tactile perception is crucial for robot dexterity. However, a lack of general-purpose models, challenges with interpreting magnetic flux and calibration have limited the adoption of these sensors.\nPercepSkin, given a history of kinematic and tactile sensing across a hand, outputs a latent tactile embedding that can be used in any downstream task. The encoder is self-supervised via self-distillation on a variety of unlabeled hand-object  interactions using an Allegro hand sensorized with Xela uSkin.\nIn experiments across several benchmark tasks, from state estimation to policy learning, we find that pretrained PercepSkin representations are both sample efficient in learning downstream tasks and improve task performance by over 41% compared to prior work and over 56% compared to end-to-end learning.",
        "keywords": [
            "Robot Perception",
            "Sensing & Vision",
            "Representation learning",
            "Foundation models",
            "Tactile sensing"
        ],
        "pdf_url": "https://openreview.net/pdf/d17cdfeeb31c2db1c18a391f62b788f7f85f8b5c.pdf",
        "reviews": [
            {
                "id": "ng3scuRCkU",
                "forum": "eLeCrM5PEO",
                "replyto": "eLeCrM5PEO",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission198/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265343,
                "mdate": 1754869462652,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "eLeCrM5PEO",
                "forum": "eLeCrM5PEO",
                "content": {
                    "title": {
                        "value": "Self-supervised perception for tactile skin covered dexterous hands"
                    },
                    "authors": {
                        "value": [
                            "Akash Sharma",
                            "Carolina Higuera",
                            "Chaithanya Krishna Bodduluri",
                            "Zixi Liu",
                            "Taosha Fan",
                            "Tess Hellebrekers",
                            "Mike Lambeta",
                            "Byron Boots",
                            "Michael Kaess",
                            "Tingfan Wu",
                            "Francois Robert Hogan",
                            "Mustafa Mukadam"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Akash_Sharma1",
                            "~Carolina_Higuera1",
                            "~Chaithanya_Krishna_Bodduluri1",
                            "~Zixi_Liu1",
                            "~Taosha_Fan1",
                            "~Tess_Hellebrekers2",
                            "~Mike_Lambeta1",
                            "~Byron_Boots1",
                            "~Michael_Kaess1",
                            "~Tingfan_Wu2",
                            "~Francois_Robert_Hogan1",
                            "~Mustafa_Mukadam1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Perception",
                            "Sensing & Vision",
                            "Representation learning",
                            "Foundation models",
                            "Tactile sensing"
                        ]
                    },
                    "abstract": {
                        "value": "We present PercepSkin, a pre-trained encoder for magnetic skin sensors distributed across the fingertips, phalanges, and palm of a dexterous robot hand. \nMagnetic tactile skins offer a flexible form factor for hand-wide coverage with fast response times, in contrast to vision-based tactile sensors that are restricted to the fingertips and limited by bandwidth. Full hand tactile perception is crucial for robot dexterity. However, a lack of general-purpose models, challenges with interpreting magnetic flux and calibration have limited the adoption of these sensors.\nPercepSkin, given a history of kinematic and tactile sensing across a hand, outputs a latent tactile embedding that can be used in any downstream task. The encoder is self-supervised via self-distillation on a variety of unlabeled hand-object  interactions using an Allegro hand sensorized with Xela uSkin.\nIn experiments across several benchmark tasks, from state estimation to policy learning, we find that pretrained PercepSkin representations are both sample efficient in learning downstream tasks and improve task performance by over 41% compared to prior work and over 56% compared to end-to-end learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b91ac81984bf9527026ce8921b9b16aa9498f86c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/d17cdfeeb31c2db1c18a391f62b788f7f85f8b5c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nsharma2025selfsupervised,\ntitle={Self-supervised perception for tactile skin covered dexterous hands},\nauthor={Akash Sharma and Carolina Higuera and Chaithanya Krishna Bodduluri and Zixi Liu and Taosha Fan and Tess Hellebrekers and Mike Lambeta and Byron Boots and Michael Kaess and Tingfan Wu and Francois Robert Hogan and Mustafa Mukadam},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=eLeCrM5PEO}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/b81b5cf83729c5cd22b3776c5949bf44ce23710b.zip"
                    },
                    "paperhash": {
                        "value": "sharma|selfsupervised_perception_for_tactile_skin_covered_dexterous_hands"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission198/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission198/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission198/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744988555230,
                "pdate": 1754680606816,
                "odate": 1758062759151,
                "mdate": 1758062809412,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission198/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "qAGHsCsA1O",
        "title": "Predictive Red Teaming: Breaking Policies Without Breaking Robots",
        "abstract": "Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2–7x.",
        "keywords": [
            "red teaming",
            "policy evaluation",
            "targeted data collection"
        ],
        "pdf_url": "https://openreview.net/pdf/71da0e290909a8fc420974519a67dd2422928bcf.pdf",
        "reviews": [
            {
                "id": "dd5tB4zMeR",
                "forum": "qAGHsCsA1O",
                "replyto": "qAGHsCsA1O",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission197/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265267,
                "mdate": 1754869462438,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "qAGHsCsA1O",
                "forum": "qAGHsCsA1O",
                "content": {
                    "title": {
                        "value": "Predictive Red Teaming: Breaking Policies Without Breaking Robots"
                    },
                    "authors": {
                        "value": [
                            "Anirudha Majumdar",
                            "Mohit Sharma",
                            "Dmitry Kalashnikov",
                            "Sumeet Singh",
                            "Pierre Sermanet",
                            "Vikas Sindhwani"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Anirudha_Majumdar1",
                            "~Mohit_Sharma1",
                            "~Dmitry_Kalashnikov1",
                            "~Sumeet_Singh3",
                            "~Pierre_Sermanet1",
                            "~Vikas_Sindhwani1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "red teaming",
                            "policy evaluation",
                            "targeted data collection"
                        ]
                    },
                    "TLDR": {
                        "value": "Discovering vulnerabilities of a visuomotor policy with respect to changes in environmental factors without hardware evaluations in off-nominal conditions."
                    },
                    "abstract": {
                        "value": "Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2–7x."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/71da0e290909a8fc420974519a67dd2422928bcf.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmajumdar2025predictive,\ntitle={Predictive Red Teaming: Breaking Policies Without Breaking Robots},\nauthor={Anirudha Majumdar and Mohit Sharma and Dmitry Kalashnikov and Sumeet Singh and Pierre Sermanet and Vikas Sindhwani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qAGHsCsA1O}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/334a7afd40dd3f1f897c47d6d4f2acfb3bdf7c9d.zip"
                    },
                    "paperhash": {
                        "value": "majumdar|predictive_red_teaming_breaking_policies_without_breaking_robots"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission197/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission197/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744975248080,
                "pdate": 1754680606757,
                "odate": 1758062759141,
                "mdate": 1758062809291,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission197/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission197/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "yOWUy97hmd",
        "title": "Learning Long-Horizon Robot Manipulation Skills via Privileged Action",
        "abstract": "Long-horizon contact-rich tasks are challenging to learn with reinforcement learning, due to ineffective exploration of high-dimensional state spaces with sparse rewards. The learning process often gets stuck in local optimum and demands task-specific reward fine-tuning for complex scenarios.  In this work, we propose a structured framework that leverages privileged actions with curriculum learning, enabling the policy to efficiently acquire long-horizon skills without relying on extensive reward engineering or reference trajectories. Specifically, we use privileged actions in simulation with a general training procedure that would be infeasible to implement in real-world scenarios. These privileges include relaxed constraints and virtual forces that enhance interaction and exploration with objects. Our results successfully achieve complex multi-stage long-horizon tasks that naturally combine non-prehensile manipulation with grasping to lift objects from non-graspable poses. We demonstrate generality by maintaining a parsimonious reward structure and showing convergence to diverse and robust behaviors across various environments. Our approach outperforms state-of-the-art methods in these tasks, converging to solutions where others fail.",
        "keywords": [
            "Reinforcement learning",
            "Grasping & Manipulation",
            "Currirulum Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/bb97ee0fb260b266229ac828e743e44b91f28f72.pdf",
        "reviews": [
            {
                "id": "CmhgVKlIM7",
                "forum": "yOWUy97hmd",
                "replyto": "yOWUy97hmd",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission196/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265200,
                "mdate": 1754869462383,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "yOWUy97hmd",
                "forum": "yOWUy97hmd",
                "content": {
                    "title": {
                        "value": "Learning Long-Horizon Robot Manipulation Skills via Privileged Action"
                    },
                    "authors": {
                        "value": [
                            "Xiaofeng Mao",
                            "Yucheng XU",
                            "Zhaole Sun",
                            "Elle Miller",
                            "Daniel Layeghi",
                            "Michael Mistry"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xiaofeng_Mao6",
                            "~Yucheng_XU1",
                            "~Zhaole_Sun1",
                            "~Elle_Miller1",
                            "~Daniel_Layeghi1",
                            "~Michael_Mistry1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement learning",
                            "Grasping & Manipulation",
                            "Currirulum Learning"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a framework leveraging privileged actions and curriculum learning to efficiently solve long-horizon, contact-rich manipulation tasks, achieving robust behaviors and outperforming state-of-the-art methods."
                    },
                    "abstract": {
                        "value": "Long-horizon contact-rich tasks are challenging to learn with reinforcement learning, due to ineffective exploration of high-dimensional state spaces with sparse rewards. The learning process often gets stuck in local optimum and demands task-specific reward fine-tuning for complex scenarios.  In this work, we propose a structured framework that leverages privileged actions with curriculum learning, enabling the policy to efficiently acquire long-horizon skills without relying on extensive reward engineering or reference trajectories. Specifically, we use privileged actions in simulation with a general training procedure that would be infeasible to implement in real-world scenarios. These privileges include relaxed constraints and virtual forces that enhance interaction and exploration with objects. Our results successfully achieve complex multi-stage long-horizon tasks that naturally combine non-prehensile manipulation with grasping to lift objects from non-graspable poses. We demonstrate generality by maintaining a parsimonious reward structure and showing convergence to diverse and robust behaviors across various environments. Our approach outperforms state-of-the-art methods in these tasks, converging to solutions where others fail."
                    },
                    "supplementary_material": {
                        "value": "/attachment/938bacb02a4c99033b1e104a1fbde92690717ecf.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/bb97ee0fb260b266229ac828e743e44b91f28f72.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nmao2025learning,\ntitle={Learning Long-Horizon Robot Manipulation Skills via Privileged Action},\nauthor={Xiaofeng Mao and Yucheng XU and Zhaole Sun and Elle Miller and Daniel Layeghi and Michael Mistry},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=yOWUy97hmd}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/7b2a6587229df21d7f392108b12c150a1ce578f9.mp4"
                    },
                    "paperhash": {
                        "value": "mao|learning_longhorizon_robot_manipulation_skills_via_privileged_action"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission196/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission196/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission196/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744960695224,
                "pdate": 1754680606678,
                "odate": 1758062759025,
                "mdate": 1758062809311,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission196/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Lv5lmtLGIQ",
        "title": "Towards Embodiment Scaling Laws in Robot Locomotion",
        "abstract": "Developing generalist agents that operate across diverse tasks, environments, and robot embodiments is a grand challenge in robotics and artificial intelligence. While substantial progress has been made in cross-task and cross-environment generalization, achieving broad generalization to novel embodiments remains elusive. In this work, we study embodiment scaling laws — the hypothesis that increasing the quantity of training embodiments improves generalization to unseen ones. To explore this, we procedurally generate a dataset of $\\sim$1,000 varied robot embodiments, spanning humanoids, quadrupeds, and hexapods, and train embodiment-specific reinforcement learning experts for legged locomotion. We then distill these experts into a single generalist policy capable of handling diverse observation and action spaces. Our large-scale study reveals that generalization performance improves with the number of training embodiments. Notably, a policy trained on the full dataset zero-shot transfers to diverse unseen embodiments in both simulation and real-world evaluations. These results provide preliminary empirical evidence for embodiment scaling laws and suggest that scaling up embodiment quantity may serve as a foundation for building generalist robot agents.",
        "keywords": [
            "Cross-Embodiment Learning",
            "Robot Locomotion",
            "Robotic Foundation Models",
            "Reinforcement Learning",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/5b34dea8018d7cc80b64d55ee51629488bfb494d.pdf",
        "reviews": [
            {
                "id": "os1EMCkh2h",
                "forum": "Lv5lmtLGIQ",
                "replyto": "Lv5lmtLGIQ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission194/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068265095,
                "mdate": 1754869462299,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Lv5lmtLGIQ",
                "forum": "Lv5lmtLGIQ",
                "content": {
                    "title": {
                        "value": "Towards Embodiment Scaling Laws in Robot Locomotion"
                    },
                    "authors": {
                        "value": [
                            "Bo Ai",
                            "Liu Dai",
                            "Nico Bohlinger",
                            "Dichen Li",
                            "Tongzhou Mu",
                            "Zhanxin Wu",
                            "K. Fay",
                            "Henrik I Christensen",
                            "Jan Peters",
                            "Hao Su"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Bo_Ai1",
                            "~Liu_Dai1",
                            "~Nico_Bohlinger1",
                            "~Dichen_Li1",
                            "~Tongzhou_Mu1",
                            "~Zhanxin_Wu1",
                            "~K._Fay1",
                            "~Henrik_I_Christensen1",
                            "~Jan_Peters3",
                            "~Hao_Su1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Cross-Embodiment Learning",
                            "Robot Locomotion",
                            "Robotic Foundation Models",
                            "Reinforcement Learning",
                            "Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Developing generalist agents that operate across diverse tasks, environments, and robot embodiments is a grand challenge in robotics and artificial intelligence. While substantial progress has been made in cross-task and cross-environment generalization, achieving broad generalization to novel embodiments remains elusive. In this work, we study embodiment scaling laws — the hypothesis that increasing the quantity of training embodiments improves generalization to unseen ones. To explore this, we procedurally generate a dataset of $\\sim$1,000 varied robot embodiments, spanning humanoids, quadrupeds, and hexapods, and train embodiment-specific reinforcement learning experts for legged locomotion. We then distill these experts into a single generalist policy capable of handling diverse observation and action spaces. Our large-scale study reveals that generalization performance improves with the number of training embodiments. Notably, a policy trained on the full dataset zero-shot transfers to diverse unseen embodiments in both simulation and real-world evaluations. These results provide preliminary empirical evidence for embodiment scaling laws and suggest that scaling up embodiment quantity may serve as a foundation for building generalist robot agents."
                    },
                    "supplementary_material": {
                        "value": "/attachment/02a91e98c88db65c5d30b829caf4b7d83b16ff26.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We conduct a large-scale study on embodiment scaling laws using robot locomotion as a testbed."
                    },
                    "pdf": {
                        "value": "/pdf/5b34dea8018d7cc80b64d55ee51629488bfb494d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nai2025towards,\ntitle={Towards Embodiment Scaling Laws in Robot Locomotion},\nauthor={Bo Ai and Liu Dai and Nico Bohlinger and Dichen Li and Tongzhou Mu and Zhanxin Wu and K. Fay and Henrik I Christensen and Jan Peters and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Lv5lmtLGIQ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/59baedb099bd1f0081efa1d0139de95908c0dd8a.mp4"
                    },
                    "paperhash": {
                        "value": "ai|towards_embodiment_scaling_laws_in_robot_locomotion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission194/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission194/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission194/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744953033826,
                "pdate": 1754680606536,
                "odate": 1758062758948,
                "mdate": 1758062809104,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission194/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "rJRFFDVTnf",
        "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation",
        "abstract": "Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for optimization-based motion planning, significantly enhancing LLMs' capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability.",
        "keywords": [
            "Affordance Grounding",
            "Few-shot Learning",
            "Vision Foundation Models"
        ],
        "pdf_url": "https://openreview.net/pdf/4f336f257903b2af9c08bb173d852590c726f7c3.pdf",
        "reviews": [
            {
                "id": "QycHufqhvO",
                "forum": "rJRFFDVTnf",
                "replyto": "rJRFFDVTnf",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission187/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068264636,
                "mdate": 1754869462241,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "rJRFFDVTnf",
                "forum": "rJRFFDVTnf",
                "content": {
                    "title": {
                        "value": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Tongxuan Tian",
                            "Xuhui Kang",
                            "Yen-Ling Kuo"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tongxuan_Tian1",
                            "~Xuhui_Kang1",
                            "~Yen-Ling_Kuo1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Affordance Grounding",
                            "Few-shot Learning",
                            "Vision Foundation Models"
                        ]
                    },
                    "TLDR": {
                        "value": "We propose a novel framework for one-shot affordance learning that leverages vision foundation models to enable effective robotic manipulation."
                    },
                    "abstract": {
                        "value": "Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for optimization-based motion planning, significantly enhancing LLMs' capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d3f56c98c183c23294d64bbf437a84f9a8b7401c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4f336f257903b2af9c08bb173d852590c726f7c3.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ntian2025oafford,\ntitle={O\\${\\textasciicircum}3\\$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation},\nauthor={Tongxuan Tian and Xuhui Kang and Yen-Ling Kuo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=rJRFFDVTnf}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/db1f5a132495d9069dacd108d5468b01a6ba535a.mp4"
                    },
                    "paperhash": {
                        "value": "tian|o^3afford_oneshot_3d_objecttoobject_affordance_grounding_for_generalizable_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission187/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission187/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission187/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744933151322,
                "pdate": 1754680606140,
                "odate": 1758062758607,
                "mdate": 1758062808938,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission187/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "xzR8rBRgPp",
        "title": "Estimating Value of Assistance for Online POMDP Robotic Agents",
        "abstract": "Robotic agents operating in dynamic, partially observable environments often benefit from teammate assistance. We address the challenge of determining when and how to assist in multi-robot systems where agents can modify the physical environment, such as moving obstacles that block perception or manipulation. For robots using online POMDP planning, evaluating assistance impacts requires computationally intensive policy evaluation, making real-time decisions difficult. We formulate Value of Assistance (VOA) for POMDP agents and develop efficient heuristics that approximate VOA without requiring complete policy evaluation. Our empirical evaluation on both a standard POMDP benchmark and a collaborative manipulation task demonstrates that our Full Information heuristic enables real-time assistance decisions while maintaining sufficient accuracy for effective helping action selection.",
        "keywords": [
            "POMDP",
            "Online Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/4d7fe6850640b1cc6c88227a4f593ed55ccd652c.pdf",
        "reviews": [
            {
                "id": "zkRcim90oH",
                "forum": "xzR8rBRgPp",
                "replyto": "xzR8rBRgPp",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission181/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068264341,
                "mdate": 1754869462167,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "xzR8rBRgPp",
                "forum": "xzR8rBRgPp",
                "content": {
                    "title": {
                        "value": "Estimating Value of Assistance for Online POMDP Robotic Agents"
                    },
                    "authors": {
                        "value": [
                            "Yuval Goshen",
                            "Sarah Keren"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuval_Goshen1",
                            "~Sarah_Keren1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "POMDP",
                            "Online Planning"
                        ]
                    },
                    "abstract": {
                        "value": "Robotic agents operating in dynamic, partially observable environments often benefit from teammate assistance. We address the challenge of determining when and how to assist in multi-robot systems where agents can modify the physical environment, such as moving obstacles that block perception or manipulation. For robots using online POMDP planning, evaluating assistance impacts requires computationally intensive policy evaluation, making real-time decisions difficult. We formulate Value of Assistance (VOA) for POMDP agents and develop efficient heuristics that approximate VOA without requiring complete policy evaluation. Our empirical evaluation on both a standard POMDP benchmark and a collaborative manipulation task demonstrates that our Full Information heuristic enables real-time assistance decisions while maintaining sufficient accuracy for effective helping action selection."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c973cfa7420cc063233e7fc5f767daf2398fa8a9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4d7fe6850640b1cc6c88227a4f593ed55ccd652c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngoshen2025estimating,\ntitle={Estimating Value of Assistance for Online {POMDP} Robotic Agents},\nauthor={Yuval Goshen and Sarah Keren},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xzR8rBRgPp}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5e249e7f5668b8c3813a28c317299a5db245b7fd.zip"
                    },
                    "paperhash": {
                        "value": "goshen|estimating_value_of_assistance_for_online_pomdp_robotic_agents"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission181/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission181/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744898920258,
                "pdate": 1754680605868,
                "odate": 1758062758413,
                "mdate": 1758062808973,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission181/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "PTJopl2uaU",
        "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation",
        "abstract": "Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability.  We identify shortcut learning—the reliance on task-irrelevant features—as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\\pi_0$ in the SIMPLER Environment.",
        "keywords": [
            "Generalist Robot Policies",
            "Shortcut Learning",
            "Large-scale Robotic Datasets"
        ],
        "pdf_url": "https://openreview.net/pdf/7c30cb1b64053b3b2bc592f104f4f3824f9cbe5b.pdf",
        "reviews": [
            {
                "id": "k29IpktQhR",
                "forum": "PTJopl2uaU",
                "replyto": "PTJopl2uaU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission179/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068264105,
                "mdate": 1754869462098,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "PTJopl2uaU",
                "forum": "PTJopl2uaU",
                "content": {
                    "title": {
                        "value": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation"
                    },
                    "authors": {
                        "value": [
                            "Youguang Xing",
                            "Xu Luo",
                            "Junlin Xie",
                            "Lianli Gao",
                            "Heng Tao Shen",
                            "Jingkuan Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Youguang_Xing1",
                            "~Xu_Luo1",
                            "~Junlin_Xie3",
                            "~Lianli_Gao1",
                            "~Heng_Tao_Shen3",
                            "~Jingkuan_Song3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Generalist Robot Policies",
                            "Shortcut Learning",
                            "Large-scale Robotic Datasets"
                        ]
                    },
                    "abstract": {
                        "value": "Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability.  We identify shortcut learning—the reliance on task-irrelevant features—as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\\pi_0$ in the SIMPLER Environment."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/7c30cb1b64053b3b2bc592f104f4f3824f9cbe5b.pdf"
                    },
                    "TLDR": {
                        "value": "Identifying shortcut learning as the key impediment to the generalization of generalist robot policies and providing a comprehensive analysis."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxing2025shortcut,\ntitle={Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation},\nauthor={Youguang Xing and Xu Luo and Junlin Xie and Lianli Gao and Heng Tao Shen and Jingkuan Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PTJopl2uaU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/7d9a1b5ce48cc054e559ef45f78e02f3f9a8c72a.zip"
                    },
                    "paperhash": {
                        "value": "xing|shortcut_learning_in_generalist_robot_policies_the_role_of_dataset_diversity_and_fragmentation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission179/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission179/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission179/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744897158870,
                "pdate": 1754680605746,
                "odate": 1758062758358,
                "mdate": 1758062808878,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission179/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "kbrb6W7bUL",
        "title": "SDS – See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration",
        "abstract": "Imagine a robot learning locomotion skills from any single video, without labels or reward engineering. We introduce SDS (\"See it. Do it. Sorted.\"), an automated pipeline for skill acquisition from unstructured video demonstrations. Using GPT-4o, SDS applies novel prompting techniques, in the form of spatio-temporal grid-based visual encoding (Gv) and structured input decomposition (SUS). These produce executable reward functions (RF) from raw input videos. The RFs are used to train PPO policies and are optimized through closed-loop evolution, using training footage and performance metrics as self-supervised signals. SDS allows quadrupeds (e.g., Unitree Go1) to learn four gaits—trot, bound, pace, and hop—achieving 100% gait matching fidelity, Dynamic Time Warping (DTW) distance in the order of 10^-6, and stable locomotion with zero failures, both in simulation and the real world. SDS generalizes to morphologically different quadrupeds (e.g., ANYmal) and outperforms prior work in data efficiency, training time, and engineering effort. Our code is open-source under: https://sdsreview.github.io/SDS_ANONYM/",
        "keywords": [
            "Skill Imitation",
            "Robot Skill Learning",
            "Quadrupedal Robot"
        ],
        "pdf_url": "https://openreview.net/pdf/a419544c3563850462e6061c1407b396e643ebf2.pdf",
        "reviews": [
            {
                "id": "WlCHIHXgW4",
                "forum": "kbrb6W7bUL",
                "replyto": "kbrb6W7bUL",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission178/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068264059,
                "mdate": 1754869462060,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "kbrb6W7bUL",
                "forum": "kbrb6W7bUL",
                "content": {
                    "title": {
                        "value": "SDS – See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration"
                    },
                    "authors": {
                        "value": [
                            "Maria Stamatopoulou",
                            "Jeffrey Li",
                            "Dimitrios Kanoulas"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Maria_Stamatopoulou1",
                            "jeffrey.li.20@alumni.ucl.ac.uk",
                            "~Dimitrios_Kanoulas1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Skill Imitation",
                            "Robot Skill Learning",
                            "Quadrupedal Robot"
                        ]
                    },
                    "TLDR": {
                        "value": "Fully automated pipeline for quadruped locomotion skill learning from a single video via GPT-4o–generated reward functions, requiring no manual tuning."
                    },
                    "abstract": {
                        "value": "Imagine a robot learning locomotion skills from any single video, without labels or reward engineering. We introduce SDS (\"See it. Do it. Sorted.\"), an automated pipeline for skill acquisition from unstructured video demonstrations. Using GPT-4o, SDS applies novel prompting techniques, in the form of spatio-temporal grid-based visual encoding (Gv) and structured input decomposition (SUS). These produce executable reward functions (RF) from raw input videos. The RFs are used to train PPO policies and are optimized through closed-loop evolution, using training footage and performance metrics as self-supervised signals. SDS allows quadrupeds (e.g., Unitree Go1) to learn four gaits—trot, bound, pace, and hop—achieving 100% gait matching fidelity, Dynamic Time Warping (DTW) distance in the order of 10^-6, and stable locomotion with zero failures, both in simulation and the real world. SDS generalizes to morphologically different quadrupeds (e.g., ANYmal) and outperforms prior work in data efficiency, training time, and engineering effort. Our code is open-source under: https://sdsreview.github.io/SDS_ANONYM/"
                    },
                    "supplementary_material": {
                        "value": "/attachment/9ceb26aaabcfbced02b3185156997aa4e96c29cc.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/a419544c3563850462e6061c1407b396e643ebf2.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nstamatopoulou2025sds,\ntitle={{SDS} {\\textendash} See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration},\nauthor={Maria Stamatopoulou and Jeffrey Li and Dimitrios Kanoulas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kbrb6W7bUL}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/27d153fee3b19ae5943df914c1c830f58b73f4bf.zip"
                    },
                    "paperhash": {
                        "value": "stamatopoulou|sds_see_it_do_it_sorted_quadruped_skill_synthesis_from_single_video_demonstration"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission178/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission178/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission178/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744896833707,
                "pdate": 1754680605683,
                "odate": 1758062758238,
                "mdate": 1758062808681,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission178/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "GIHDBntBu9",
        "title": "TrackVLA: Embodied Visual Tracking in the Wild",
        "abstract": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed.",
        "keywords": [
            "Embodied Visual Tracking",
            "Vision-Language-Action Model"
        ],
        "pdf_url": "https://openreview.net/pdf/8400d74c04bcf8ae76bf5ac7d5b368e01ca055d4.pdf",
        "reviews": [
            {
                "id": "3IvZOL1jmk",
                "forum": "GIHDBntBu9",
                "replyto": "GIHDBntBu9",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission177/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068264059,
                "mdate": 1754869461961,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "GIHDBntBu9",
                "forum": "GIHDBntBu9",
                "content": {
                    "title": {
                        "value": "TrackVLA: Embodied Visual Tracking in the Wild"
                    },
                    "authors": {
                        "value": [
                            "Shaoan Wang",
                            "Jiazhao Zhang",
                            "Minghan Li",
                            "Jiahang Liu",
                            "Anqi Li",
                            "Kui Wu",
                            "Fangwei Zhong",
                            "Junzhi Yu",
                            "Zhizheng Zhang",
                            "He Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shaoan_Wang1",
                            "~Jiazhao_Zhang2",
                            "~Minghan_Li5",
                            "~Jiahang_Liu2",
                            "~Anqi_Li7",
                            "~Kui_Wu5",
                            "~Fangwei_Zhong3",
                            "~Junzhi_Yu1",
                            "~Zhizheng_Zhang1",
                            "~He_Wang5"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Embodied Visual Tracking",
                            "Vision-Language-Action Model"
                        ]
                    },
                    "abstract": {
                        "value": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f20e503a062fa25f083fb433e71fc2028aff8b2a.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "TrackVLA is a unified Vision-Language-Action model jointly learns object recognition and trajectory planning, achieving state-of-the-art embodied visual tracking performance in dynamic environments."
                    },
                    "pdf": {
                        "value": "/pdf/8400d74c04bcf8ae76bf5ac7d5b368e01ca055d4.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025trackvla,\ntitle={Track{VLA}: Embodied Visual Tracking in the Wild},\nauthor={Shaoan Wang and Jiazhao Zhang and Minghan Li and Jiahang Liu and Anqi Li and Kui Wu and Fangwei Zhong and Junzhi Yu and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GIHDBntBu9}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d48b86697fe20b4bebfcd39b2115ad133fa9cc20.zip"
                    },
                    "paperhash": {
                        "value": "wang|trackvla_embodied_visual_tracking_in_the_wild"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission177/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission177/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission177/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744895935007,
                "pdate": 1754680605614,
                "odate": 1758062758193,
                "mdate": 1758062808653,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission177/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission177/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HT34hQcU91",
        "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation",
        "abstract": "Learning manipulation skills from human demonstration videos offers a promising path toward generalizable and interpretable robotic intelligence—particularly through the lens of *actionable affordances*.  However, transferring such knowledge remains challenging due to:  1) a lack of large-scale datasets with precise affordance annotations, and 2) insufficient exploration of affordances in diverse manipulation contexts. To address these gaps, we introduce **HOVA-500K**, a large-scale, affordance-annotated dataset comprising 500,000 images across 1,726 object categories and 675 actions. We also release a standardized benchmarking suite for multi-modal affordance reasoning. Built upon HOVA-500K, we present **GLOVER++**, a *global-to-local* affordance training framework that effectively transfers actionable affordance knowledge from human demonstrations to downstream open-vocabulary reasoning tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark and demonstrates strong generalization across diverse downstream robotic manipulation tasks. \nBy explicitly modeling actionable affordances, GLOVER++ facilitates robust transfer across scenes, modalities, and tasks. We hope that HOVA-500K and the GLOVER++ framework will serve as valuable resources for bridging the gap between human demonstrations and robotic manipulation capabilities. We will release our dataset, code and models.",
        "keywords": [
            "Actionable Affordance",
            "Affordance Transfer",
            "Vision-Language Model",
            "Human Demonstrations",
            "Robotic Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/370f43e09596759d5205d60967978536c1fd9e9c.pdf",
        "reviews": [
            {
                "id": "zMUrJpE054",
                "forum": "HT34hQcU91",
                "replyto": "HT34hQcU91",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission174/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068263901,
                "mdate": 1754869461808,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HT34hQcU91",
                "forum": "HT34hQcU91",
                "content": {
                    "title": {
                        "value": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Teli Ma",
                            "Jia Zheng",
                            "Zifan Wang",
                            "Ziyao Gao",
                            "Jiaming Zhou",
                            "Junwei Liang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Teli_Ma1",
                            "~Jia_Zheng6",
                            "~Zifan_Wang7",
                            "~Ziyao_Gao1",
                            "~Jiaming_Zhou1",
                            "~Junwei_Liang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Actionable Affordance",
                            "Affordance Transfer",
                            "Vision-Language Model",
                            "Human Demonstrations",
                            "Robotic Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Learning manipulation skills from human demonstration videos offers a promising path toward generalizable and interpretable robotic intelligence—particularly through the lens of *actionable affordances*.  However, transferring such knowledge remains challenging due to:  1) a lack of large-scale datasets with precise affordance annotations, and 2) insufficient exploration of affordances in diverse manipulation contexts. To address these gaps, we introduce **HOVA-500K**, a large-scale, affordance-annotated dataset comprising 500,000 images across 1,726 object categories and 675 actions. We also release a standardized benchmarking suite for multi-modal affordance reasoning. Built upon HOVA-500K, we present **GLOVER++**, a *global-to-local* affordance training framework that effectively transfers actionable affordance knowledge from human demonstrations to downstream open-vocabulary reasoning tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark and demonstrates strong generalization across diverse downstream robotic manipulation tasks. \nBy explicitly modeling actionable affordances, GLOVER++ facilitates robust transfer across scenes, modalities, and tasks. We hope that HOVA-500K and the GLOVER++ framework will serve as valuable resources for bridging the gap between human demonstrations and robotic manipulation capabilities. We will release our dataset, code and models."
                    },
                    "supplementary_material": {
                        "value": "/attachment/41cd1d2d4f16b80c246b2181b1c57622ebf7d45a.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/370f43e09596759d5205d60967978536c1fd9e9c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nma2025glover,\ntitle={{GLOVER}++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation},\nauthor={Teli Ma and Jia Zheng and Zifan Wang and Ziyao Gao and Jiaming Zhou and Junwei Liang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HT34hQcU91}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6055b162718412a070d1b9481ee43fc695ffe89b.mp4"
                    },
                    "paperhash": {
                        "value": "ma|glover_unleashing_the_potential_of_affordance_learning_from_human_behaviors_for_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission174/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission174/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission174/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744886198724,
                "pdate": 1754680605466,
                "odate": 1758062758080,
                "mdate": 1758062808636,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission174/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "o2w2iiMyEU",
        "title": "LaDi-WM: A Latent Diffusion-Based World Model for Predictive Manipulation",
        "abstract": "Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments. The full source code will be publicly available.",
        "keywords": [
            "Robot Learning: Model Learning",
            "Control & Dynamics",
            "Grasping & Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/108f28899d6c07ddf99f761dd641e1e1fef57a02.pdf",
        "reviews": [
            {
                "id": "eeMlRgQF5W",
                "forum": "o2w2iiMyEU",
                "replyto": "o2w2iiMyEU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission167/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068263514,
                "mdate": 1754869461752,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "o2w2iiMyEU",
                "forum": "o2w2iiMyEU",
                "content": {
                    "title": {
                        "value": "LaDi-WM: A Latent Diffusion-Based World Model for Predictive Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Yuhang Huang",
                            "Jiazhao Zhang",
                            "Shilong Zou",
                            "Xinwang Liu",
                            "Ruizhen Hu",
                            "Kai Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yuhang_Huang1",
                            "~Jiazhao_Zhang2",
                            "~Shilong_Zou1",
                            "~Xinwang_Liu1",
                            "~Ruizhen_Hu1",
                            "~Kai_Xu5"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Learning: Model Learning",
                            "Control & Dynamics",
                            "Grasping & Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments. The full source code will be publicly available."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/108f28899d6c07ddf99f761dd641e1e1fef57a02.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhuang2025ladiwm,\ntitle={LaDi-{WM}: A Latent Diffusion-Based World Model for Predictive Manipulation},\nauthor={Yuhang Huang and Jiazhao Zhang and Shilong Zou and Xinwang Liu and Ruizhen Hu and Kai Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=o2w2iiMyEU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ceac9995a7cbc2fc9ca9c4c1ab43774024f23086.zip"
                    },
                    "paperhash": {
                        "value": "huang|ladiwm_a_latent_diffusionbased_world_model_for_predictive_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission167/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission167/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission167/-/Spotlight",
                    "robot-learning.org/CoRL/2025/Conference/Submission167/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744872784112,
                "pdate": 1754680605213,
                "odate": 1758062757877,
                "mdate": 1758062808397,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission167/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission167/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "etSYDtRO0Z",
        "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training",
        "abstract": "Generative models based on flow matching offer significant potential for learning robot policies, particularly in generating high-dimensional, dexterous behaviors that are conditioned on diverse observations. In this work, we introduce ManiFlow, an advanced flow matching model specifically designed to support dexterous manipulation tasks. ManiFlow improves over flow matching both in the learning procedure and in the model architecture, resulting in better robustness and efficacy. It consistently exhibits strong generalization capabilities, outperforming existing state-of-the-art robot learning methods on a wide range of benchmarks. We also demonstrate the powerful capabilities of ManiFlow in solving complex bimanual dexterous manipulation challenges.",
        "keywords": [
            "Imitation Learning",
            "Dexterous Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/3d09c871d73f8402f2ea2414bfd9c34145075d32.pdf",
        "reviews": [
            {
                "id": "HS8ksH9NpX",
                "forum": "etSYDtRO0Z",
                "replyto": "etSYDtRO0Z",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission166/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068263318,
                "mdate": 1754869461636,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "etSYDtRO0Z",
                "forum": "etSYDtRO0Z",
                "content": {
                    "title": {
                        "value": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training"
                    },
                    "authors": {
                        "value": [
                            "Ge Yan",
                            "Jiyue Zhu",
                            "Yuquan Deng",
                            "Shiqi Yang",
                            "Ri-Zhao Qiu",
                            "Xuxin Cheng",
                            "Marius Memmel",
                            "Ranjay Krishna",
                            "Ankit Goyal",
                            "Xiaolong Wang",
                            "Dieter Fox"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ge_Yan3",
                            "~Jiyue_Zhu1",
                            "~Yuquan_Deng1",
                            "~Shiqi_Yang2",
                            "~Ri-Zhao_Qiu1",
                            "~Xuxin_Cheng2",
                            "~Marius_Memmel1",
                            "~Ranjay_Krishna1",
                            "~Ankit_Goyal1",
                            "~Xiaolong_Wang3",
                            "~Dieter_Fox1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Imitation Learning",
                            "Dexterous Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Generative models based on flow matching offer significant potential for learning robot policies, particularly in generating high-dimensional, dexterous behaviors that are conditioned on diverse observations. In this work, we introduce ManiFlow, an advanced flow matching model specifically designed to support dexterous manipulation tasks. ManiFlow improves over flow matching both in the learning procedure and in the model architecture, resulting in better robustness and efficacy. It consistently exhibits strong generalization capabilities, outperforming existing state-of-the-art robot learning methods on a wide range of benchmarks. We also demonstrate the powerful capabilities of ManiFlow in solving complex bimanual dexterous manipulation challenges."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9c191228e94c83a1641d9c49bc52c094120e479c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3d09c871d73f8402f2ea2414bfd9c34145075d32.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyan2025maniflow,\ntitle={ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training},\nauthor={Ge Yan and Jiyue Zhu and Yuquan Deng and Shiqi Yang and Ri-Zhao Qiu and Xuxin Cheng and Marius Memmel and Ranjay Krishna and Ankit Goyal and Xiaolong Wang and Dieter Fox},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=etSYDtRO0Z}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8509026d5ff0f73636eab890737b339374bfb07d.mp4"
                    },
                    "paperhash": {
                        "value": "yan|maniflow_a_general_robot_manipulation_policy_via_consistency_flow_training"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission166/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission166/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission166/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744848093754,
                "pdate": 1754680605206,
                "odate": 1758062757793,
                "mdate": 1758062808344,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission166/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "FCHl2ybdAm",
        "title": "Robot Learning from Any Images",
        "abstract": "We introduce RoLA, a framework that transforms any in‑the‑wild image into an interactive, physics‑enabled robotic environment.  Unlike previous methods, RoLA operates directly on a single image without requiring additional hardware or digital assets. Our framework democratizes robotic data generation by producing massive visuomotor robotic demonstrations within minutes from a wide range of image sources, including camera captures, robotic datasets, and Internet images. At its core, our approach combines a novel method for single-view physical scene recovery with an efficient visual blending strategy for photorealistic data collection. We demonstrate RoLA's versatility across applications like scalable robotic data generation and augmentation, robot learning from internet images, and single-image real-to-sim-to-real systems for manipulators and humanoids. Video results are available at our \\href{https://rola-2025.github.io/}{project page}.",
        "keywords": [
            "Robotic Manipulation",
            "Real-to-Sim-to-Real",
            "Robotic Data Collection"
        ],
        "pdf_url": "https://openreview.net/pdf/f6428629b74f9514e05996ea637e2602aaa5be0a.pdf",
        "reviews": [
            {
                "id": "iIV0aV7YIi",
                "forum": "FCHl2ybdAm",
                "replyto": "FCHl2ybdAm",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission163/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068263211,
                "mdate": 1754869461673,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "FCHl2ybdAm",
                "forum": "FCHl2ybdAm",
                "content": {
                    "title": {
                        "value": "Robot Learning from Any Images"
                    },
                    "authors": {
                        "value": [
                            "Siheng Zhao",
                            "Jiageng Mao",
                            "Wei Chow",
                            "Zeyu Shangguan",
                            "Tianheng Shi",
                            "Rong Xue",
                            "Yuxi Zheng",
                            "Yijia Weng",
                            "Yang You",
                            "Daniel Seita",
                            "Leonidas Guibas",
                            "Sergey Zakharov",
                            "Vitor Campagnolo Guizilini",
                            "Yue Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Siheng_Zhao1",
                            "~Jiageng_Mao1",
                            "~Wei_Chow1",
                            "~Zeyu_Shangguan1",
                            "~Tianheng_Shi1",
                            "~Rong_Xue1",
                            "~Yuxi_Zheng4",
                            "~Yijia_Weng1",
                            "~Yang_You2",
                            "~Daniel_Seita1",
                            "~Leonidas_Guibas1",
                            "~Sergey_Zakharov1",
                            "~Vitor_Campagnolo_Guizilini2",
                            "~Yue_Wang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robotic Manipulation",
                            "Real-to-Sim-to-Real",
                            "Robotic Data Collection"
                        ]
                    },
                    "abstract": {
                        "value": "We introduce RoLA, a framework that transforms any in‑the‑wild image into an interactive, physics‑enabled robotic environment.  Unlike previous methods, RoLA operates directly on a single image without requiring additional hardware or digital assets. Our framework democratizes robotic data generation by producing massive visuomotor robotic demonstrations within minutes from a wide range of image sources, including camera captures, robotic datasets, and Internet images. At its core, our approach combines a novel method for single-view physical scene recovery with an efficient visual blending strategy for photorealistic data collection. We demonstrate RoLA's versatility across applications like scalable robotic data generation and augmentation, robot learning from internet images, and single-image real-to-sim-to-real systems for manipulators and humanoids. Video results are available at our \\href{https://rola-2025.github.io/}{project page}."
                    },
                    "supplementary_material": {
                        "value": "/attachment/80bf88752f883119cd91d4d5ab830af219b5dc5f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We present a framework that transforms any single image into a robotic environment and enables scalable robotic data collection."
                    },
                    "pdf": {
                        "value": "/pdf/f6428629b74f9514e05996ea637e2602aaa5be0a.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhao2025robot,\ntitle={Robot Learning from Any Images},\nauthor={Siheng Zhao and Jiageng Mao and Wei Chow and Zeyu Shangguan and Tianheng Shi and Rong Xue and Yuxi Zheng and Yijia Weng and Yang You and Daniel Seita and Leonidas Guibas and Sergey Zakharov and Vitor Campagnolo Guizilini and Yue Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FCHl2ybdAm}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/62a2a8daf05091d22c94623e84319bb0b554f661.zip"
                    },
                    "paperhash": {
                        "value": "zhao|robot_learning_from_any_images"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission163/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission163/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission163/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744825682927,
                "pdate": 1754680605083,
                "odate": 1758062757703,
                "mdate": 1758062808332,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission163/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "4eSv0QeYlz",
        "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments",
        "abstract": "Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings.\n\nWe propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT’s static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module.\n\nWe evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. We will release the dataset, simulation environments, and trained models upon acceptance. Refer to supplementary material for videos.",
        "keywords": [
            "Motion Planning",
            "Visuo-Motor Policy",
            "Reactive Control"
        ],
        "pdf_url": "https://openreview.net/pdf/b5cd01c31176844f179bb63733d6061eec681ca7.pdf",
        "reviews": [
            {
                "id": "EUK5Hd425N",
                "forum": "4eSv0QeYlz",
                "replyto": "4eSv0QeYlz",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission145/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068262140,
                "mdate": 1754801282405,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "4eSv0QeYlz",
                "forum": "4eSv0QeYlz",
                "content": {
                    "title": {
                        "value": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments"
                    },
                    "authors": {
                        "value": [
                            "Jiahui Yang",
                            "Jason Jingzhou Liu",
                            "Yulong Li",
                            "Youssef Khaky",
                            "Kenneth Shaw",
                            "Deepak Pathak"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiahui_Yang3",
                            "~Jason_Jingzhou_Liu1",
                            "~Yulong_Li1",
                            "~Youssef_Khaky1",
                            "~Kenneth_Shaw1",
                            "~Deepak_Pathak1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Motion Planning",
                            "Visuo-Motor Policy",
                            "Reactive Control"
                        ]
                    },
                    "abstract": {
                        "value": "Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings.\n\nWe propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT’s static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module.\n\nWe evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. We will release the dataset, simulation environments, and trained models upon acceptance. Refer to supplementary material for videos."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "We introduce Deep Reactive Policy (DRP), a novel hierarchical framework capable of global planning while reacting to dynamic obstacles, directly operating on point cloud observations."
                    },
                    "pdf": {
                        "value": "/pdf/b5cd01c31176844f179bb63733d6061eec681ca7.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyang2025deep,\ntitle={Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments},\nauthor={Jiahui Yang and Jason Jingzhou Liu and Yulong Li and Youssef Khaky and Kenneth Shaw and Deepak Pathak},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4eSv0QeYlz}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/1f944cabda0cd57eb131b622f582fe13d5c20aa5.mp4"
                    },
                    "paperhash": {
                        "value": "yang|deep_reactive_policy_learning_reactive_manipulator_motion_planning_for_dynamic_environments"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission145/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission145/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission145/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744752761121,
                "pdate": 1754680604356,
                "odate": 1758062757064,
                "mdate": 1758062808147,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission145/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission145/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bU15EK0oqk",
        "title": "CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models",
        "abstract": "Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance.\nWe introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines. More information is available at https://casper-corl25.github.io/",
        "keywords": [
            "Assistive Teleoperation",
            "Mobile Manipulation",
            "Vision Language Models"
        ],
        "pdf_url": "https://openreview.net/pdf/cb55b59e1cd46fecc69a92198a3caad319b1f2ab.pdf",
        "reviews": [
            {
                "id": "fiH8MvNPgp",
                "forum": "bU15EK0oqk",
                "replyto": "bU15EK0oqk",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission143/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261977,
                "mdate": 1754801282074,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bU15EK0oqk",
                "forum": "bU15EK0oqk",
                "content": {
                    "title": {
                        "value": "CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models"
                    },
                    "authors": {
                        "value": [
                            "Huihan Liu",
                            "Rutav Shah",
                            "Shuijing Liu",
                            "Jack Pittenger",
                            "Mingyo Seo",
                            "Yuchen Cui",
                            "Yonatan Bisk",
                            "Roberto Martín-Martín",
                            "Yuke Zhu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Huihan_Liu1",
                            "~Rutav_Shah1",
                            "~Shuijing_Liu1",
                            "jackpittenger@utexas.edu",
                            "~Mingyo_Seo1",
                            "~Yuchen_Cui1",
                            "~Yonatan_Bisk1",
                            "~Roberto_Martín-Martín1",
                            "~Yuke_Zhu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Assistive Teleoperation",
                            "Mobile Manipulation",
                            "Vision Language Models"
                        ]
                    },
                    "TLDR": {
                        "value": "We develop an assistive teleoperation system that harnesses commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time human intent inference and skill execution."
                    },
                    "abstract": {
                        "value": "Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance.\nWe introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines. More information is available at https://casper-corl25.github.io/"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/cb55b59e1cd46fecc69a92198a3caad319b1f2ab.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025casper,\ntitle={{CASPER}: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models},\nauthor={Huihan Liu and Rutav Shah and Shuijing Liu and Jack Pittenger and Mingyo Seo and Yuchen Cui and Yonatan Bisk and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n and Yuke Zhu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bU15EK0oqk}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/197ed3bf0edc05be380d4fc33a475c15afff3c90.mp4"
                    },
                    "paperhash": {
                        "value": "liu|casper_inferring_diverse_intents_for_assistive_teleoperation_with_vision_language_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission143/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission143/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission143/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744749577530,
                "pdate": 1754680604157,
                "odate": 1758062756836,
                "mdate": 1758062808067,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission143/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "EgSDP6AOF1",
        "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations",
        "abstract": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts.",
        "keywords": [
            "Robot Learning",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/5b5de55fbb20fc4c12969f08dce10e989c159486.pdf",
        "reviews": [
            {
                "id": "3502xb1zBP",
                "forum": "EgSDP6AOF1",
                "replyto": "EgSDP6AOF1",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission138/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261798,
                "mdate": 1754801281464,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "EgSDP6AOF1",
                "forum": "EgSDP6AOF1",
                "content": {
                    "title": {
                        "value": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations"
                    },
                    "authors": {
                        "value": [
                            "Hanjung Kim",
                            "Jaehyun Kang",
                            "Hyolim Kang",
                            "Meedeum Cho",
                            "Seon Joo Kim",
                            "Youngwoon Lee"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hanjung_Kim1",
                            "~Jaehyun_Kang1",
                            "~Hyolim_Kang1",
                            "~Meedeum_Cho1",
                            "~Seon_Joo_Kim2",
                            "~Youngwoon_Lee1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Learning",
                            "Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f1f3bcf42a21ee4f522ed05df02f255ebd889862.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5b5de55fbb20fc4c12969f08dce10e989c159486.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkim2025uniskill,\ntitle={UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations},\nauthor={Hanjung Kim and Jaehyun Kang and Hyolim Kang and Meedeum Cho and Seon Joo Kim and Youngwoon Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EgSDP6AOF1}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/25077a5e5f67412b1d8e4ad465f169035a9edadf.zip"
                    },
                    "paperhash": {
                        "value": "kim|uniskill_imitating_human_videos_via_crossembodiment_skill_representations"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission138/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission138/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission138/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744718327919,
                "pdate": 1754680603987,
                "odate": 1758062756725,
                "mdate": 1758062808056,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission138/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission138/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "7wGYX11BJB",
        "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation",
        "abstract": "3D world models (i.e., learning-based 3D dynamics models) offer a promising approach to generalizable robotic manipulation by capturing the underlying physics of environment evolution conditioned on robot actions. However, existing 3D world models are primarily limited to single-material dynamics using a particle-based Graph Neural Network model, and often require time-consuming 3D scene reconstruction to obtain 3D particle tracks for training. In this work, we present ParticleFormer, a Transformer-based point cloud world model trained with a hybrid point cloud reconstruction loss, supervising both global and local dynamics features in multi-material, multi-object robot interactions. ParticleFormer captures fine-grained multi-object interactions between rigid, deformable, and flexible materials, trained directly from real-world robot perception data without an elaborate scene reconstruction. We demonstrate the model's effectiveness both in 3D scene forecasting tasks, and in downstream manipulation tasks using a Model Predictive Control (MPC) policy.  In addition, we extend existing dynamics learning benchmarks to include diverse multi-material, multi-object interaction scenarios. We validate our method on six simulation and three real-world experiments, where it consistently outperforms leading baselines by achieving superior dynamics prediction accuracy and less rollout error in downstream visuomotor tasks. Experimental videos are available at https://particleformer.github.io/.",
        "keywords": [
            "Learning-based Dynamics Modeling",
            "Model-based Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/8f67d19d35743853787832792dac9845bd6c9131.pdf",
        "reviews": [
            {
                "id": "4riJNB1dy9",
                "forum": "7wGYX11BJB",
                "replyto": "7wGYX11BJB",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission135/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261568,
                "mdate": 1754801281970,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "7wGYX11BJB",
                "forum": "7wGYX11BJB",
                "content": {
                    "title": {
                        "value": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Suning Huang",
                            "Qianzhong Chen",
                            "Xiaohan Zhang",
                            "Jiankai Sun",
                            "Mac Schwager"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Suning_Huang1",
                            "~Qianzhong_Chen2",
                            "~Xiaohan_Zhang7",
                            "~Jiankai_Sun6",
                            "~Mac_Schwager1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learning-based Dynamics Modeling",
                            "Model-based Planning"
                        ]
                    },
                    "abstract": {
                        "value": "3D world models (i.e., learning-based 3D dynamics models) offer a promising approach to generalizable robotic manipulation by capturing the underlying physics of environment evolution conditioned on robot actions. However, existing 3D world models are primarily limited to single-material dynamics using a particle-based Graph Neural Network model, and often require time-consuming 3D scene reconstruction to obtain 3D particle tracks for training. In this work, we present ParticleFormer, a Transformer-based point cloud world model trained with a hybrid point cloud reconstruction loss, supervising both global and local dynamics features in multi-material, multi-object robot interactions. ParticleFormer captures fine-grained multi-object interactions between rigid, deformable, and flexible materials, trained directly from real-world robot perception data without an elaborate scene reconstruction. We demonstrate the model's effectiveness both in 3D scene forecasting tasks, and in downstream manipulation tasks using a Model Predictive Control (MPC) policy.  In addition, we extend existing dynamics learning benchmarks to include diverse multi-material, multi-object interaction scenarios. We validate our method on six simulation and three real-world experiments, where it consistently outperforms leading baselines by achieving superior dynamics prediction accuracy and less rollout error in downstream visuomotor tasks. Experimental videos are available at https://particleformer.github.io/."
                    },
                    "supplementary_material": {
                        "value": "/attachment/8ff363fff37c5929276352ccb8807de09c322c71.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "ParticleFormer is a powerful dynamics modeling framework that excels at capturing complex multi-object, multi-material interactions, and enables effective downstream visuomotor control."
                    },
                    "pdf": {
                        "value": "/pdf/8f67d19d35743853787832792dac9845bd6c9131.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhuang2025particleformer,\ntitle={ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation},\nauthor={Suning Huang and Qianzhong Chen and Xiaohan Zhang and Jiankai Sun and Mac Schwager},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7wGYX11BJB}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/e5221e4810fa45c7f5e027bf25fc093b62299341.mp4"
                    },
                    "paperhash": {
                        "value": "huang|particleformer_a_3d_point_cloud_world_model_for_multiobject_multimaterial_robotic_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission135/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission135/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission135/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744689306914,
                "pdate": 1754680603840,
                "odate": 1758062756618,
                "mdate": 1758062807791,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission135/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission135/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "CgGSFtjplI",
        "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
        "abstract": "Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels and human-robot embodiment differences. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the embodiment gap without relying on wearables, teleoperation, or large-scale data collection. From the video, we extract: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. These components enable effective policy learning without any task-specific reward tuning. In the single human demo regime, Human2Sim2Robot outperforms object-aware replay by over 55% and imitation learning by over 68% on grasping, non-prehensile manipulation, and multi-step tasks. Website: https://human2sim2robot.github.io",
        "keywords": [
            "Dexterous Manipulation",
            "Reinforcement Learning",
            "Sim-to-Real"
        ],
        "pdf_url": "https://openreview.net/pdf/c3c9d5457924950dc519439de433a1f05321c14b.pdf",
        "reviews": [
            {
                "id": "TitFCC5dJU",
                "forum": "CgGSFtjplI",
                "replyto": "CgGSFtjplI",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission134/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261506,
                "mdate": 1754801280535,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "CgGSFtjplI",
                "forum": "CgGSFtjplI",
                "content": {
                    "title": {
                        "value": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration"
                    },
                    "authors": {
                        "value": [
                            "Tyler Ga Wei Lum",
                            "Olivia Y. Lee",
                            "Karen Liu",
                            "Jeannette Bohg"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Tyler_Ga_Wei_Lum1",
                            "~Olivia_Y._Lee1",
                            "~Karen_Liu1",
                            "~Jeannette_Bohg1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Manipulation",
                            "Reinforcement Learning",
                            "Sim-to-Real"
                        ]
                    },
                    "TLDR": {
                        "value": "Human2Sim2Robot trains dexterous manipulation policies from one human RGB-D video. We use object trajectories and pre-manipulation poses to guide RL in sim and thereby bridge the human-robot embodiment gap, achieving zero-shot sim-to-real transfer."
                    },
                    "abstract": {
                        "value": "Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels and human-robot embodiment differences. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the embodiment gap without relying on wearables, teleoperation, or large-scale data collection. From the video, we extract: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. These components enable effective policy learning without any task-specific reward tuning. In the single human demo regime, Human2Sim2Robot outperforms object-aware replay by over 55% and imitation learning by over 68% on grasping, non-prehensile manipulation, and multi-step tasks. Website: https://human2sim2robot.github.io"
                    },
                    "supplementary_material": {
                        "value": "/attachment/033472e8b6df3bc9107c2c67987792f1e95ec65d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c3c9d5457924950dc519439de433a1f05321c14b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlum2025crossing,\ntitle={Crossing the Human-Robot Embodiment Gap with Sim-to-Real {RL} using One Human Demonstration},\nauthor={Tyler Ga Wei Lum and Olivia Y. Lee and Karen Liu and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CgGSFtjplI}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/ba7db3fd8e8705f69c2ebe8661c9ff50c8629e8e.zip"
                    },
                    "paperhash": {
                        "value": "lum|crossing_the_humanrobot_embodiment_gap_with_simtoreal_rl_using_one_human_demonstration"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission134/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission134/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission134/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1744676632029,
                "pdate": 1754680603771,
                "odate": 1758062756492,
                "mdate": 1758062807901,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission134/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission134/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "XjjXLxfPou",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations",
        "abstract": "We introduce ReWiND, a framework for learning robot manipulation tasks solely from language instructions without per-task demonstrations. Standard reinforcement learning (RL) and imitation learning methods require expert supervision through human-designed reward functions or demonstrations for every new task. In contrast, ReWiND starts from a small demonstration dataset to learn: (1) a data-efficient, language-conditioned reward function that labels the dataset with rewards, and (2) a language-conditioned policy pre-trained with offline RL using these rewards. Given an unseen task variation, ReWiND fine-tunes the pre-trained policy using the learned reward function, requiring minimal online interaction. We show that ReWiND’s reward model generalizes effectively to unseen tasks, outperforming baselines by up to 2.4X in reward generalization and policy alignment metrics. Finally, we demonstrate that ReWiND enables sample-efficient adaptation to new tasks in both simulation and on a real bimanual manipulation platform, taking a step towards scalable, real-world robot learning.",
        "keywords": [
            "Reinforcement Learning",
            "Offline Reinforcement Learning",
            "Reward Learning",
            "Reward Modeling",
            "Language"
        ],
        "pdf_url": "https://openreview.net/pdf/dbb23418b16d43149254df98961fbd4b1ddb786d.pdf",
        "reviews": [
            {
                "id": "8JhtIUOtpr",
                "forum": "XjjXLxfPou",
                "replyto": "XjjXLxfPou",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission132/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261477,
                "mdate": 1754869455483,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "XjjXLxfPou",
                "forum": "XjjXLxfPou",
                "content": {
                    "title": {
                        "value": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations"
                    },
                    "authors": {
                        "value": [
                            "Jiahui Zhang",
                            "Yusen Luo",
                            "Abrar Anwar",
                            "Sumedh Anand Sontakke",
                            "Joseph J Lim",
                            "Jesse Thomason",
                            "Erdem Biyik",
                            "Jesse Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiahui_Zhang5",
                            "~Yusen_Luo1",
                            "~Abrar_Anwar1",
                            "~Sumedh_Anand_Sontakke1",
                            "~Joseph_J_Lim1",
                            "~Jesse_Thomason1",
                            "~Erdem_Biyik1",
                            "~Jesse_Zhang3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Reinforcement Learning",
                            "Offline Reinforcement Learning",
                            "Reward Learning",
                            "Reward Modeling",
                            "Language"
                        ]
                    },
                    "TLDR": {
                        "value": "ReWiND enables real-world robots to efficiently learn new manipulation tasks from language instructions alone by training a generalizable reward model trained on a small initial demonstration dataset."
                    },
                    "abstract": {
                        "value": "We introduce ReWiND, a framework for learning robot manipulation tasks solely from language instructions without per-task demonstrations. Standard reinforcement learning (RL) and imitation learning methods require expert supervision through human-designed reward functions or demonstrations for every new task. In contrast, ReWiND starts from a small demonstration dataset to learn: (1) a data-efficient, language-conditioned reward function that labels the dataset with rewards, and (2) a language-conditioned policy pre-trained with offline RL using these rewards. Given an unseen task variation, ReWiND fine-tunes the pre-trained policy using the learned reward function, requiring minimal online interaction. We show that ReWiND’s reward model generalizes effectively to unseen tasks, outperforming baselines by up to 2.4X in reward generalization and policy alignment metrics. Finally, we demonstrate that ReWiND enables sample-efficient adaptation to new tasks in both simulation and on a real bimanual manipulation platform, taking a step towards scalable, real-world robot learning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9dfdfda537a62c298df923f1db61392c5a21e473.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/dbb23418b16d43149254df98961fbd4b1ddb786d.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025rewind,\ntitle={ReWi{ND}: Language-Guided Rewards Teach Robot Policies without New Demonstrations},\nauthor={Jiahui Zhang and Yusen Luo and Abrar Anwar and Sumedh Anand Sontakke and Joseph J Lim and Jesse Thomason and Erdem Biyik and Jesse Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XjjXLxfPou}\n}"
                    },
                    "paperhash": {
                        "value": "zhang|rewind_languageguided_rewards_teach_robot_policies_without_new_demonstrations"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission132/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission132/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission132/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744671177255,
                "pdate": 1754680603680,
                "odate": 1758062756337,
                "mdate": 1758062807756,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission132/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission132/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Hu3NoPMAg4",
        "title": "One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies",
        "abstract": "Visuomotor policies for manipulation have demonstrated remarkable potential in modeling complex robotic behaviors, yet minor alterations in the robot’s initial configuration and unseen obstacles easily lead to out-of-distribution observations. Without extensive data collection effort, these result in catastrophic execution failures. In this work, we introduce an effective data augmentation framework that generates visually realistic fisheye image sequences and corresponding physically feasible action trajectories from real-world eye-in-hand demonstrations, captured with a portable parallel gripper with a single fisheye camera. We introduce a novel Gaussian Splatting formulation, adapted to wide FoV fisheye cameras, to reconstruct and edit the 3D scene with unseen objects. We utilize trajectory optimization to generate smooth, collision-free, view-rendering-friendly action trajectories and render visual observations from corresponding novel views. Comprehensive experiments in simulation and the real world show that our augmentation framework improves the success rate for various manipulation tasks in both the same scene and the augmented scene with obstacles requiring collision avoidance.",
        "keywords": [
            "visuomotor policies",
            "LfD",
            "manipulation",
            "3D vision",
            "gaussian splat",
            "motion planning"
        ],
        "pdf_url": "https://openreview.net/pdf/42c3ca7549ea1bd62b5543eca541a1eda66da090.pdf",
        "reviews": [
            {
                "id": "xJhyuStIGJ",
                "forum": "Hu3NoPMAg4",
                "replyto": "Hu3NoPMAg4",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission130/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261353,
                "mdate": 1754801280796,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Hu3NoPMAg4",
                "forum": "Hu3NoPMAg4",
                "content": {
                    "title": {
                        "value": "One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies"
                    },
                    "authors": {
                        "value": [
                            "Chuer Pan",
                            "Litian Liang",
                            "Dominik Bauer",
                            "Eric Cousineau",
                            "Benjamin Burchfiel",
                            "Siyuan Feng",
                            "Shuran Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chuer_Pan1",
                            "~Litian_Liang1",
                            "~Dominik_Bauer1",
                            "~Eric_Cousineau1",
                            "~Benjamin_Burchfiel1",
                            "~Siyuan_Feng5",
                            "~Shuran_Song3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "visuomotor policies",
                            "LfD",
                            "manipulation",
                            "3D vision",
                            "gaussian splat",
                            "motion planning"
                        ]
                    },
                    "abstract": {
                        "value": "Visuomotor policies for manipulation have demonstrated remarkable potential in modeling complex robotic behaviors, yet minor alterations in the robot’s initial configuration and unseen obstacles easily lead to out-of-distribution observations. Without extensive data collection effort, these result in catastrophic execution failures. In this work, we introduce an effective data augmentation framework that generates visually realistic fisheye image sequences and corresponding physically feasible action trajectories from real-world eye-in-hand demonstrations, captured with a portable parallel gripper with a single fisheye camera. We introduce a novel Gaussian Splatting formulation, adapted to wide FoV fisheye cameras, to reconstruct and edit the 3D scene with unseen objects. We utilize trajectory optimization to generate smooth, collision-free, view-rendering-friendly action trajectories and render visual observations from corresponding novel views. Comprehensive experiments in simulation and the real world show that our augmentation framework improves the success rate for various manipulation tasks in both the same scene and the augmented scene with obstacles requiring collision avoidance."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c4f93a51e33a17448ce861273cc52ed0365686e8.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/42c3ca7549ea1bd62b5543eca541a1eda66da090.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\npan2025one,\ntitle={One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies},\nauthor={Chuer Pan and Litian Liang and Dominik Bauer and Eric Cousineau and Benjamin Burchfiel and Siyuan Feng and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Hu3NoPMAg4}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f9152987bb4b6bbc6e9e2aa31136c2b04ac743f9.mp4"
                    },
                    "paperhash": {
                        "value": "pan|one_demo_is_worth_a_thousand_trajectories_actionview_augmentation_for_visuomotor_policies"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission130/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission130/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission130/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744660615677,
                "pdate": 1754680603593,
                "odate": 1758062756335,
                "mdate": 1758062807504,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission130/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "JM2vDI6DlP",
        "title": "VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision",
        "abstract": "Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes.  This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset. It further improves route completion and driving scores under closed-loop evaluation, demonstrating its effectiveness in long-horizon, interactive driving scenarios and its potential for safe and reliable real-world deployment.",
        "keywords": [
            "End-to-End Autonomous Driving",
            "Vision-Language Model"
        ],
        "pdf_url": "https://openreview.net/pdf/0fe638fb2b0c87ac2db913e95b19126693b235be.pdf",
        "reviews": [
            {
                "id": "DOXiZ1pPK7",
                "forum": "JM2vDI6DlP",
                "replyto": "JM2vDI6DlP",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission129/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261227,
                "mdate": 1754801280302,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "JM2vDI6DlP",
                "forum": "JM2vDI6DlP",
                "content": {
                    "title": {
                        "value": "VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision"
                    },
                    "authors": {
                        "value": [
                            "Yi Xu",
                            "Yuxin Hu",
                            "Zaiwei Zhang",
                            "Gregory P. Meyer",
                            "Siva Karthik Mustikovela",
                            "Siddhartha Srinivasa",
                            "Eric M. Wolff",
                            "Xin Huang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yi_Xu9",
                            "~Yuxin_Hu2",
                            "~Zaiwei_Zhang1",
                            "~Gregory_P._Meyer1",
                            "~Siva_Karthik_Mustikovela1",
                            "~Siddhartha_Srinivasa1",
                            "~Eric_M._Wolff1",
                            "~Xin_Huang8"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "End-to-End Autonomous Driving",
                            "Vision-Language Model"
                        ]
                    },
                    "abstract": {
                        "value": "Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes.  This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset. It further improves route completion and driving scores under closed-loop evaluation, demonstrating its effectiveness in long-horizon, interactive driving scenarios and its potential for safe and reliable real-world deployment."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0fe638fb2b0c87ac2db913e95b19126693b235be.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025vlmad,\ntitle={{VLM}-{AD}: End-to-End Autonomous Driving through Vision-Language Model Supervision},\nauthor={Yi Xu and Yuxin Hu and Zaiwei Zhang and Gregory P. Meyer and Siva Karthik Mustikovela and Siddhartha Srinivasa and Eric M. Wolff and Xin Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JM2vDI6DlP}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8f7c17c8e5f59418e82a594aac570e1ba292978a.mp4"
                    },
                    "paperhash": {
                        "value": "xu|vlmad_endtoend_autonomous_driving_through_visionlanguage_model_supervision"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission129/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission129/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744659108538,
                "pdate": 1754680603495,
                "odate": 1758062756334,
                "mdate": 1758062807473,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission129/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "R5Y7Sr1DIe",
        "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation",
        "abstract": "Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning.",
        "keywords": [
            "POMDP",
            "LLM",
            "Inference",
            "Probabilistic",
            "Induction",
            "Robotics",
            "Uncertainty"
        ],
        "pdf_url": "https://openreview.net/pdf/5a8c285325d27f254d513d8d71ca0699677007ff.pdf",
        "reviews": [
            {
                "id": "Qne5dv6Ai7",
                "forum": "R5Y7Sr1DIe",
                "replyto": "R5Y7Sr1DIe",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission125/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068260965,
                "mdate": 1754801279562,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "R5Y7Sr1DIe",
                "forum": "R5Y7Sr1DIe",
                "content": {
                    "title": {
                        "value": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation"
                    },
                    "authors": {
                        "value": [
                            "Aidan Curtis",
                            "Hao Tang",
                            "Thiago Veloso",
                            "Kevin Ellis",
                            "Joshua B. Tenenbaum",
                            "Tomás Lozano-Pérez",
                            "Leslie Pack Kaelbling"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Aidan_Curtis2",
                            "~Hao_Tang5",
                            "~Thiago_Veloso1",
                            "~Kevin_Ellis1",
                            "~Joshua_B._Tenenbaum1",
                            "~Tomás_Lozano-Pérez1",
                            "~Leslie_Pack_Kaelbling1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "POMDP",
                            "LLM",
                            "Inference",
                            "Probabilistic",
                            "Induction",
                            "Robotics",
                            "Uncertainty"
                        ]
                    },
                    "TLDR": {
                        "value": "We use an LLM to infer POMDP models from data"
                    },
                    "abstract": {
                        "value": "Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b81f2dbb7b2c819e00602b33436daebeeee32bc7.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5a8c285325d27f254d513d8d71ca0699677007ff.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ncurtis2025llmguided,\ntitle={{LLM}-Guided Probabilistic Program Induction for {POMDP} Model Estimation},\nauthor={Aidan Curtis and Hao Tang and Thiago Veloso and Kevin Ellis and Joshua B. Tenenbaum and Tom{\\'a}s Lozano-P{\\'e}rez and Leslie Pack Kaelbling},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=R5Y7Sr1DIe}\n}"
                    },
                    "paperhash": {
                        "value": "curtis|llmguided_probabilistic_program_induction_for_pomdp_model_estimation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission125/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission125/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission125/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744633824669,
                "pdate": 1754680603346,
                "odate": 1758062756131,
                "mdate": 1758062807454,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission125/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "MpJTyAqA0t",
        "title": "Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation",
        "abstract": "Robotic loco-manipulation tasks often involve contact-rich interactions with the environment, requiring the joint modeling of contact force and robot position. However, recent visuomotor policies often focus solely on position or force control, overlooking their integration. In this work, we propose a unified policy for legged robots that jointly models force and position control learned without reliance on force sensors. By simulating diverse combinations of active position and force commands alongside external disturbances force, we use reinforcement learning to learn a policy that estimates forces from the robot's historical states and compensates for them through position and velocity adjustments. Such a policy enables a wide range of manipulation behaviors under varying combinations of force and position inputs, including position tracking, force application, force tracking, and compliant robot behaviors. Additionally, we demonstrate that the learned policy enhances trajectory-based imitation learning pipelines by incorporating essential contact information through its force estimation module, achieving approximately ~39.5% higher success rates across four challenging contact-rich manipulation tasks compared to position-control policies. Extensive experiments on both a quadrupedal mobile manipulation platform and a humanoid validate the versatility and robustness of the proposed policy across diverse scenarios.",
        "keywords": [
            "Unified Force and Position Control",
            "Force-aware Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/37e007ee22de64d910b67b4b4a0097d9e67a4a25.pdf",
        "reviews": [
            {
                "id": "S0tTcVagkO",
                "forum": "MpJTyAqA0t",
                "replyto": "MpJTyAqA0t",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission123/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068261292,
                "mdate": 1754869455477,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "MpJTyAqA0t",
                "forum": "MpJTyAqA0t",
                "content": {
                    "title": {
                        "value": "Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Peiyuan Zhi",
                            "Peiyang Li",
                            "Jianqin Yin",
                            "Baoxiong Jia",
                            "Siyuan Huang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Peiyuan_Zhi1",
                            "~Peiyang_Li4",
                            "~Jianqin_Yin1",
                            "~Baoxiong_Jia1",
                            "~Siyuan_Huang2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Unified Force and Position Control",
                            "Force-aware Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Robotic loco-manipulation tasks often involve contact-rich interactions with the environment, requiring the joint modeling of contact force and robot position. However, recent visuomotor policies often focus solely on position or force control, overlooking their integration. In this work, we propose a unified policy for legged robots that jointly models force and position control learned without reliance on force sensors. By simulating diverse combinations of active position and force commands alongside external disturbances force, we use reinforcement learning to learn a policy that estimates forces from the robot's historical states and compensates for them through position and velocity adjustments. Such a policy enables a wide range of manipulation behaviors under varying combinations of force and position inputs, including position tracking, force application, force tracking, and compliant robot behaviors. Additionally, we demonstrate that the learned policy enhances trajectory-based imitation learning pipelines by incorporating essential contact information through its force estimation module, achieving approximately ~39.5% higher success rates across four challenging contact-rich manipulation tasks compared to position-control policies. Extensive experiments on both a quadrupedal mobile manipulation platform and a humanoid validate the versatility and robustness of the proposed policy across diverse scenarios."
                    },
                    "supplementary_material": {
                        "value": "/attachment/baeddf086027ac9ad7f3c4f140ee861390f5b594.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/37e007ee22de64d910b67b4b4a0097d9e67a4a25.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhi2025learning,\ntitle={Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation},\nauthor={Peiyuan Zhi and Peiyang Li and Jianqin Yin and Baoxiong Jia and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MpJTyAqA0t}\n}"
                    },
                    "paperhash": {
                        "value": "zhi|learning_a_unified_policy_for_position_and_force_control_in_legged_locomanipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission123/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission123/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission123/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744614321900,
                "pdate": 1754680603198,
                "odate": 1758062756087,
                "mdate": 1758062807258,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission123/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission123/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1NBdplgILy",
        "title": "Vision in Action: Learning Active Perception from Human Demonstrations",
        "abstract": "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.",
        "keywords": [
            "Active Perception",
            "Bimanual Manipulation",
            "Imitation Learning",
            "Teleoperation Systems"
        ],
        "pdf_url": "https://openreview.net/pdf/b15a0e061041751319c52c6dd47e263c8adea253.pdf",
        "reviews": [
            {
                "id": "k6O1kjB9Eg",
                "forum": "1NBdplgILy",
                "replyto": "1NBdplgILy",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission120/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068260584,
                "mdate": 1754801279482,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1NBdplgILy",
                "forum": "1NBdplgILy",
                "content": {
                    "title": {
                        "value": "Vision in Action: Learning Active Perception from Human Demonstrations"
                    },
                    "authors": {
                        "value": [
                            "Haoyu Xiong",
                            "Xiaomeng Xu",
                            "Jimmy Wu",
                            "Yifan Hou",
                            "Jeannette Bohg",
                            "Shuran Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haoyu_Xiong3",
                            "~Xiaomeng_Xu1",
                            "~Jimmy_Wu1",
                            "~Yifan_Hou2",
                            "~Jeannette_Bohg1",
                            "~Shuran_Song3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Active Perception",
                            "Bimanual Manipulation",
                            "Imitation Learning",
                            "Teleoperation Systems"
                        ]
                    },
                    "TLDR": {
                        "value": "A robotic system for learning bimanual manipulation using active perception to address tasks with visual occlusions."
                    },
                    "abstract": {
                        "value": "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b68ffcf63b31127c2836e7a68cb0c8efd8264036.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b15a0e061041751319c52c6dd47e263c8adea253.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxiong2025vision,\ntitle={Vision in Action: Learning Active Perception from Human Demonstrations},\nauthor={Haoyu Xiong and Xiaomeng Xu and Jimmy Wu and Yifan Hou and Jeannette Bohg and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1NBdplgILy}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/9e5907b928dc932ca38ca2c550a709037b645a22.zip"
                    },
                    "paperhash": {
                        "value": "xiong|vision_in_action_learning_active_perception_from_human_demonstrations"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission120/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission120/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission120/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1744593842141,
                "pdate": 1754680603022,
                "odate": 1758062755679,
                "mdate": 1758062807194,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission120/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission120/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "lRKGIidrYZ",
        "title": "Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation",
        "abstract": "Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present Point Policy, a new method for learning robot policies exclusively from offline human demonstration videos without any teleoperation data. Point Policy leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Through experiments on a diverse set of real-world tasks, we demonstrate that Point Policy significantly outperforms prior methods for policy learning from human videos, performing well not only within the training distribution but also generalizing to novel object instances and cluttered environments. Videos of the robot are best viewed at anon-point-policy.github.io.",
        "keywords": [
            "Robot Learning",
            "Imitation Learning",
            "Robot Perception",
            "Sensing & Vision"
        ],
        "pdf_url": "https://openreview.net/pdf/493f51dd011773817551b5420283928d4f647979.pdf",
        "reviews": [
            {
                "id": "KJQH2hG8uQ",
                "forum": "lRKGIidrYZ",
                "replyto": "lRKGIidrYZ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission109/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068260392,
                "mdate": 1754801279377,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "lRKGIidrYZ",
                "forum": "lRKGIidrYZ",
                "content": {
                    "title": {
                        "value": "Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Siddhant Haldar",
                            "Lerrel Pinto"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Siddhant_Haldar1",
                            "~Lerrel_Pinto1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Learning",
                            "Imitation Learning",
                            "Robot Perception",
                            "Sensing & Vision"
                        ]
                    },
                    "TLDR": {
                        "value": "We present a method that unifies robot observations and actions with key points and enables learning generalizable robot policies exclusively from human videos."
                    },
                    "abstract": {
                        "value": "Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present Point Policy, a new method for learning robot policies exclusively from offline human demonstration videos without any teleoperation data. Point Policy leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Through experiments on a diverse set of real-world tasks, we demonstrate that Point Policy significantly outperforms prior methods for policy learning from human videos, performing well not only within the training distribution but also generalizing to novel object instances and cluttered environments. Videos of the robot are best viewed at anon-point-policy.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/d03686899b2fb3f7793533ddab758ffed386d5d4.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/493f51dd011773817551b5420283928d4f647979.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhaldar2025point,\ntitle={Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation},\nauthor={Siddhant Haldar and Lerrel Pinto},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=lRKGIidrYZ}\n}"
                    },
                    "paperhash": {
                        "value": "haldar|point_policy_unifying_observations_and_actions_with_key_points_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission109/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission109/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1744415935973,
                "pdate": 1754680602651,
                "odate": 1758062754925,
                "mdate": 1758062755040,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission109/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "b8RqTaDyb3",
        "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity",
        "abstract": "Autonomous skill discovery aims to enable robots to acquire diverse be-haviors without explicit supervision. Learning such behaviors directly on physical hardware remains challenging due to safety and data efficiency constraints. Existing methods, including Quality-Diversity Actor-Critic (QDAC), require manually defined skill spaces and carefully tuned heuristics, limiting real-world applicability. We propose Unsupervised Real-world Skill Acquisition (URSA), an extension of QDAC that enables robots to autonomously discover and master diverse, high-performing skills directly in the real world. We demonstrate that URSA successfully discovers diverse locomotion skills on a Unitree A1 quadruped in both simulation and the real world. Our approach supports both heuristic-driven skill discovery and fully unsupervised settings. We also show that the learn skill repertoire can be reused for downstream tasks such as real-world damage adaptation, where URSA outperforms all baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios. Our results establish a new framework for real-world robot learning that enables continuous skill discovery with limited human intervention, representing a significant step toward more autonomous and adaptable robotic systems.",
        "keywords": [
            "Quality-Diversity",
            "Reinforcement Learning",
            "Real-World Learning",
            "Robotics"
        ],
        "pdf_url": "https://openreview.net/pdf/aa224fe3a58b9dcdde9e3d47b2a827c6922d1bbf.pdf",
        "reviews": [
            {
                "id": "M3U33cJ768",
                "forum": "b8RqTaDyb3",
                "replyto": "b8RqTaDyb3",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission100/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259924,
                "mdate": 1754801279293,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "b8RqTaDyb3",
                "forum": "b8RqTaDyb3",
                "content": {
                    "title": {
                        "value": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity"
                    },
                    "authors": {
                        "value": [
                            "Luca Grillotti",
                            "Lisa Coiffard",
                            "Oscar Pang",
                            "Maxence Faldor",
                            "Antoine Cully"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Luca_Grillotti1",
                            "~Lisa_Coiffard1",
                            "k.pang@imperial.ac.uk",
                            "~Maxence_Faldor1",
                            "~Antoine_Cully1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Quality-Diversity",
                            "Reinforcement Learning",
                            "Real-World Learning",
                            "Robotics"
                        ]
                    },
                    "TLDR": {
                        "value": "We present URSA, a method enabling robots to autonomously discover and learn their abilities in the real world without extensive human guidance."
                    },
                    "abstract": {
                        "value": "Autonomous skill discovery aims to enable robots to acquire diverse be-haviors without explicit supervision. Learning such behaviors directly on physical hardware remains challenging due to safety and data efficiency constraints. Existing methods, including Quality-Diversity Actor-Critic (QDAC), require manually defined skill spaces and carefully tuned heuristics, limiting real-world applicability. We propose Unsupervised Real-world Skill Acquisition (URSA), an extension of QDAC that enables robots to autonomously discover and master diverse, high-performing skills directly in the real world. We demonstrate that URSA successfully discovers diverse locomotion skills on a Unitree A1 quadruped in both simulation and the real world. Our approach supports both heuristic-driven skill discovery and fully unsupervised settings. We also show that the learn skill repertoire can be reused for downstream tasks such as real-world damage adaptation, where URSA outperforms all baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios. Our results establish a new framework for real-world robot learning that enables continuous skill discovery with limited human intervention, representing a significant step toward more autonomous and adaptable robotic systems."
                    },
                    "supplementary_material": {
                        "value": "/attachment/14374ae384c51cc3ba715d22ff5c58cb117d18c9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/aa224fe3a58b9dcdde9e3d47b2a827c6922d1bbf.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ngrillotti2025from,\ntitle={From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity},\nauthor={Luca Grillotti and Lisa Coiffard and Oscar Pang and Maxence Faldor and Antoine Cully},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b8RqTaDyb3}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/17f780dda840d5f9a43aad7f9c9da02efbd44eaf.zip"
                    },
                    "paperhash": {
                        "value": "grillotti|from_tabula_rasa_to_emergent_abilities_discovering_robot_skills_via_realworld_unsupervised_qualitydiversity"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission100/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission100/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission100/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744379424449,
                "pdate": 1754680602150,
                "odate": 1758062748270,
                "mdate": 1758062807082,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission100/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "SNvUSjVm6C",
        "title": "Robust Dexterous Grasping of General Objects",
        "abstract": "The ability to robustly grasp a variety of objects is essential for dexterous robots. In this paper, we present a framework for zero-shot dynamic dexterous grasping using single-view visual inputs, designed to be resilient to various disturbances. Our approach utilizes a hand-centric object shape representation based on dynamic distance vectors between finger joints and object surfaces. This representation captures the local shape around potential contact regions rather than focusing on detailed global object geometry, thereby enhancing generalization to shape variations and uncertainties. To address perception limitations, we integrate a privileged teacher policy with a mixed curriculum learning approach, allowing the student policy to effectively distill grasping capabilities and explore for adaptation to disturbances. Trained in simulation, our method achieves success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, demonstrating remarkable generalization. Quantitative and qualitative results validate the robustness of our policy against various disturbances.",
        "keywords": [
            "Dexterous Grasping; Reinforcement Learning; Robot Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/5626caad8ea3ff4de5a703e2782905aed85cb751.pdf",
        "reviews": [
            {
                "id": "KdYNh4CFFQ",
                "forum": "SNvUSjVm6C",
                "replyto": "SNvUSjVm6C",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission96/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070538638,
                "mdate": 1754801279111,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "SNvUSjVm6C",
                "forum": "SNvUSjVm6C",
                "content": {
                    "title": {
                        "value": "Robust Dexterous Grasping of General Objects"
                    },
                    "authors": {
                        "value": [
                            "Hui Zhang",
                            "Zijian wu",
                            "Linyi Huang",
                            "Sammy Christen",
                            "Jie Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hui_Zhang31",
                            "~Zijian_wu9",
                            "~Linyi_Huang3",
                            "~Sammy_Christen1",
                            "~Jie_Song1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dexterous Grasping; Reinforcement Learning; Robot Manipulation"
                        ]
                    },
                    "TLDR": {
                        "value": "An RL-based pipeline that achieves robust (94.6% success rate) dexterous grasping of various (500+) unseen objects using single-view observation, while performing adaptive motions to disturbances."
                    },
                    "abstract": {
                        "value": "The ability to robustly grasp a variety of objects is essential for dexterous robots. In this paper, we present a framework for zero-shot dynamic dexterous grasping using single-view visual inputs, designed to be resilient to various disturbances. Our approach utilizes a hand-centric object shape representation based on dynamic distance vectors between finger joints and object surfaces. This representation captures the local shape around potential contact regions rather than focusing on detailed global object geometry, thereby enhancing generalization to shape variations and uncertainties. To address perception limitations, we integrate a privileged teacher policy with a mixed curriculum learning approach, allowing the student policy to effectively distill grasping capabilities and explore for adaptation to disturbances. Trained in simulation, our method achieves success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, demonstrating remarkable generalization. Quantitative and qualitative results validate the robustness of our policy against various disturbances."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/5626caad8ea3ff4de5a703e2782905aed85cb751.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025robust,\ntitle={Robust Dexterous Grasping of General Objects},\nauthor={Hui Zhang and Zijian wu and Linyi Huang and Sammy Christen and Jie Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SNvUSjVm6C}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6e6a19d92570e75b3bdd947a6356b659e4dac955.mp4"
                    },
                    "paperhash": {
                        "value": "zhang|robust_dexterous_grasping_of_general_objects"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission96/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission96/-/Spotlight",
                    "robot-learning.org/CoRL/2025/Conference/Submission96/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744352120231,
                "pdate": 1754680602010,
                "odate": 1758062744369,
                "mdate": 1758062806922,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission96/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission96/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "8DHSyMFLbB",
        "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
        "abstract": "Learning generalizable robot manipulation policies, especially for complex multi-fingered humanoids, remains a significant challenge. Existing approaches primarily rely on extensive data collection and imitation learning, which are expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement learning (RL) offers a promising alternative, but has mostly succeeded in simpler state-based or single-hand setups. How to effectively extend this to vision-based, contact-rich bimanual manipulation tasks remains an open question. In this paper, we introduce a practical sim-to-real RL recipe that trains a humanoid robot to perform three challenging dexterous manipulation tasks: grasp-and-reach, box lift and bimanual handover. Our method features an automated real-to-sim tuning module, a generalized reward formulation based on contact and object goals, a divide-and-conquer policy distillation framework, and a hybrid object representation strategy with modality-specific augmentation. We demonstrate high success rates on unseen objects and robust, adaptive policy behaviors -- highlighting that vision-based dexterous manipulation via sim-to-real RL is not only viable, but also scalable and broadly applicable to real-world humanoid manipulation tasks.",
        "keywords": [
            "Humanoids",
            "Vision-Based Dexterous Manipulation",
            "Reinforcement Learning",
            "Sim-to-Real"
        ],
        "pdf_url": "https://openreview.net/pdf/0c4daafe5ff8207d2cbcc39f61947af4ddde0867.pdf",
        "reviews": [
            {
                "id": "Dfyeo3WEUI",
                "forum": "8DHSyMFLbB",
                "replyto": "8DHSyMFLbB",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission95/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259780,
                "mdate": 1754801278969,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "8DHSyMFLbB",
                "forum": "8DHSyMFLbB",
                "content": {
                    "title": {
                        "value": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids"
                    },
                    "authors": {
                        "value": [
                            "Toru Lin",
                            "Kartik Sachdev",
                            "Linxi Fan",
                            "Jitendra Malik",
                            "Yuke Zhu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Toru_Lin1",
                            "~Kartik_Sachdev1",
                            "~Linxi_Fan2",
                            "~Jitendra_Malik2",
                            "~Yuke_Zhu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoids",
                            "Vision-Based Dexterous Manipulation",
                            "Reinforcement Learning",
                            "Sim-to-Real"
                        ]
                    },
                    "abstract": {
                        "value": "Learning generalizable robot manipulation policies, especially for complex multi-fingered humanoids, remains a significant challenge. Existing approaches primarily rely on extensive data collection and imitation learning, which are expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement learning (RL) offers a promising alternative, but has mostly succeeded in simpler state-based or single-hand setups. How to effectively extend this to vision-based, contact-rich bimanual manipulation tasks remains an open question. In this paper, we introduce a practical sim-to-real RL recipe that trains a humanoid robot to perform three challenging dexterous manipulation tasks: grasp-and-reach, box lift and bimanual handover. Our method features an automated real-to-sim tuning module, a generalized reward formulation based on contact and object goals, a divide-and-conquer policy distillation framework, and a hybrid object representation strategy with modality-specific augmentation. We demonstrate high success rates on unseen objects and robust, adaptive policy behaviors -- highlighting that vision-based dexterous manipulation via sim-to-real RL is not only viable, but also scalable and broadly applicable to real-world humanoid manipulation tasks."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e61af1a1cea4559fc2ef4ad4ff63cb708604eaf2.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/0c4daafe5ff8207d2cbcc39f61947af4ddde0867.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nlin2025simtoreal,\ntitle={Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids},\nauthor={Toru Lin and Kartik Sachdev and Linxi Fan and Jitendra Malik and Yuke Zhu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8DHSyMFLbB}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f5727bdf6d6efbcc250740af81e664177ab46e20.zip"
                    },
                    "paperhash": {
                        "value": "lin|simtoreal_reinforcement_learning_for_visionbased_dexterous_manipulation_on_humanoids"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission95/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission95/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744350432198,
                "pdate": 1754680601952,
                "odate": 1758062744368,
                "mdate": 1758062806750,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission95/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission95/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Tx54fkQ3Cq",
        "title": "Humanoid Policy ~ Human Policy",
        "abstract": "Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection,n which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both the generalization and robustness of HAT with significantly better data collection efficiency.",
        "keywords": [
            "Robot Manipulation",
            "Humanoid",
            "Learning from Human"
        ],
        "pdf_url": "https://openreview.net/pdf/8790a25c63f01f581e1448acc4d023702c1bc9fb.pdf",
        "reviews": [
            {
                "id": "sNElThmfzE",
                "forum": "Tx54fkQ3Cq",
                "replyto": "Tx54fkQ3Cq",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission94/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259686,
                "mdate": 1754801278961,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Tx54fkQ3Cq",
                "forum": "Tx54fkQ3Cq",
                "content": {
                    "title": {
                        "value": "Humanoid Policy ~ Human Policy"
                    },
                    "authors": {
                        "value": [
                            "Ri-Zhao Qiu",
                            "Shiqi Yang",
                            "Xuxin Cheng",
                            "Chaitanya Chawla",
                            "Jialong Li",
                            "Tairan He",
                            "Ge Yan",
                            "David J. Yoon",
                            "Ryan Hoque",
                            "Lars Paulsen",
                            "Ge Yang",
                            "Jian Zhang",
                            "Sha Yi",
                            "Guanya Shi",
                            "Xiaolong Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Ri-Zhao_Qiu1",
                            "~Shiqi_Yang2",
                            "~Xuxin_Cheng2",
                            "~Chaitanya_Chawla1",
                            "~Jialong_Li3",
                            "~Tairan_He1",
                            "~Ge_Yan3",
                            "~David_J._Yoon1",
                            "~Ryan_Hoque1",
                            "~Lars_Paulsen1",
                            "~Ge_Yang1",
                            "~Jian_Zhang23",
                            "~Sha_Yi1",
                            "~Guanya_Shi1",
                            "~Xiaolong_Wang3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot Manipulation",
                            "Humanoid",
                            "Learning from Human"
                        ]
                    },
                    "abstract": {
                        "value": "Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection,n which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both the generalization and robustness of HAT with significantly better data collection efficiency."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ef243958f8beec42d56ce91d6fbed12829758fa9.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "Task-oriented egocentric human data can directly be used as a cross-embodiment learning source."
                    },
                    "pdf": {
                        "value": "/pdf/8790a25c63f01f581e1448acc4d023702c1bc9fb.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nqiu2025humanoid,\ntitle={Humanoid Policy {\\textasciitilde} Human Policy},\nauthor={Ri-Zhao Qiu and Shiqi Yang and Xuxin Cheng and Chaitanya Chawla and Jialong Li and Tairan He and Ge Yan and David J. Yoon and Ryan Hoque and Lars Paulsen and Ge Yang and Jian Zhang and Sha Yi and Guanya Shi and Xiaolong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Tx54fkQ3Cq}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/3bd4959961264cd76c4cc4504cde3661845a9a96.mp4"
                    },
                    "paperhash": {
                        "value": "qiu|humanoid_policy_human_policy"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission94/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission94/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission94/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744349405094,
                "pdate": 1754680601852,
                "odate": 1758062741688,
                "mdate": 1758062806682,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission94/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission94/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "GH9kURIRlx",
        "title": "ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation",
        "abstract": "Learning-based robotics research driven by data demands a new approach to robot hardware design—one that serves as both a platform for policy execution and a tool for embodied data collection. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for robotics and AI research. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin and zero-shot sim-to-real policy transfer. A user-friendly teleoperation interface streamlines real-world data collection from human demonstrations. With its data collection ability and anthropomorphic design, ToddlerBot is ideal for whole-body loco-manipulation research. Additionally, ToddlerBot's compact size (0.56 m, 3.4 kg) ensures safe operation in real-world environments. Reproducibility is achieved with entirely 3D-printed, open-source design and off-the-shelf components, keeping the total cost under 6,000 USD. This allows assembly and maintenance with basic technical expertise, as validated by successful independent replications of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust and scalable platform for policy learning and execution in robotics research.",
        "keywords": [
            "Humanoid Robots",
            "Mechanisms & Design",
            "Robot Modeling & Simulation"
        ],
        "pdf_url": "https://openreview.net/pdf/680b2836827942700e900154750641b6778a99ff.pdf",
        "reviews": [
            {
                "id": "99ksWOO2vG",
                "forum": "GH9kURIRlx",
                "replyto": "GH9kURIRlx",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission92/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259544,
                "mdate": 1754801278612,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "GH9kURIRlx",
                "forum": "GH9kURIRlx",
                "content": {
                    "title": {
                        "value": "ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Haochen Shi",
                            "Weizhuo Wang",
                            "Shuran Song",
                            "Karen Liu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Haochen_Shi2",
                            "~Weizhuo_Wang2",
                            "~Shuran_Song3",
                            "~Karen_Liu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoid Robots",
                            "Mechanisms & Design",
                            "Robot Modeling & Simulation"
                        ]
                    },
                    "TLDR": {
                        "value": "ToddlerBot is a low-cost, open-source humanoid robot platform designed for scalable policy learning and research in robotics and AI."
                    },
                    "abstract": {
                        "value": "Learning-based robotics research driven by data demands a new approach to robot hardware design—one that serves as both a platform for policy execution and a tool for embodied data collection. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for robotics and AI research. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin and zero-shot sim-to-real policy transfer. A user-friendly teleoperation interface streamlines real-world data collection from human demonstrations. With its data collection ability and anthropomorphic design, ToddlerBot is ideal for whole-body loco-manipulation research. Additionally, ToddlerBot's compact size (0.56 m, 3.4 kg) ensures safe operation in real-world environments. Reproducibility is achieved with entirely 3D-printed, open-source design and off-the-shelf components, keeping the total cost under 6,000 USD. This allows assembly and maintenance with basic technical expertise, as validated by successful independent replications of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust and scalable platform for policy learning and execution in robotics research."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e2e188d7f17f2d74bd9466a89acba82b18214418.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/680b2836827942700e900154750641b6778a99ff.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nshi2025toddlerbot,\ntitle={ToddlerBot: Open-Source {ML}-Compatible Humanoid Platform for Loco-Manipulation},\nauthor={Haochen Shi and Weizhuo Wang and Shuran Song and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GH9kURIRlx}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/c961b585db69b2f145977593d67c57f9980250da.zip"
                    },
                    "paperhash": {
                        "value": "shi|toddlerbot_opensource_mlcompatible_humanoid_platform_for_locomanipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission92/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission92/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission92/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744348858336,
                "pdate": 1754680601539,
                "odate": 1758062739435,
                "mdate": 1758062806614,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission92/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission92/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "htgNQHa6Ta",
        "title": "TWIST: Teleoperated Whole-Body Imitation System",
        "abstract": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills—spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement—using a single unified neural network controller.",
        "keywords": [
            "humanoid robots",
            "whole-body teleoperation",
            "learning-based control"
        ],
        "pdf_url": "https://openreview.net/pdf/18b58f4ef2723990c1891e98d7b47fe626ce8d70.pdf",
        "reviews": [
            {
                "id": "5fQq0bHdiP",
                "forum": "htgNQHa6Ta",
                "replyto": "htgNQHa6Ta",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission88/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259346,
                "mdate": 1754801278619,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "htgNQHa6Ta",
                "forum": "htgNQHa6Ta",
                "content": {
                    "title": {
                        "value": "TWIST: Teleoperated Whole-Body Imitation System"
                    },
                    "authors": {
                        "value": [
                            "Yanjie Ze",
                            "Zixuan Chen",
                            "Joao Pedro Araujo",
                            "Zi-ang Cao",
                            "Xue Bin Peng",
                            "Jiajun Wu",
                            "Karen Liu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yanjie_Ze1",
                            "~Zixuan_Chen9",
                            "~Joao_Pedro_Araujo1",
                            "~Zi-ang_Cao1",
                            "~Xue_Bin_Peng1",
                            "~Jiajun_Wu1",
                            "~Karen_Liu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "humanoid robots",
                            "whole-body teleoperation",
                            "learning-based control"
                        ]
                    },
                    "abstract": {
                        "value": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills—spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement—using a single unified neural network controller."
                    },
                    "supplementary_material": {
                        "value": "/attachment/0813e7003b552f56f47d80a86ed0359b26efc525.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A capable system for whole-body humanoid teleoperation"
                    },
                    "pdf": {
                        "value": "/pdf/18b58f4ef2723990c1891e98d7b47fe626ce8d70.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nze2025twist,\ntitle={{TWIST}: Teleoperated Whole-Body Imitation System},\nauthor={Yanjie Ze and Zixuan Chen and Joao Pedro Araujo and Zi-ang Cao and Xue Bin Peng and Jiajun Wu and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=htgNQHa6Ta}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/f90ef38f03356e24d119f27c44b8f8f4f9c31d86.zip"
                    },
                    "paperhash": {
                        "value": "ze|twist_teleoperated_wholebody_imitation_system"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission88/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission88/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission88/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744327334625,
                "pdate": 1754680601305,
                "odate": 1758062734417,
                "mdate": 1758062806480,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission88/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission88/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "b86nyIOJWq",
        "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation",
        "abstract": "Tactile-aware robot learning faces critical challenges in data collection and representation due to data scarcity and sparsity, and the absence of force feedback in existing systems. To address these limitations, we introduce a tactile robot learning system with both hardware and algorithm innovations. We present exUMI, an extensible data collection device that enhances the vanilla UMI with robust proprioception (via AR MoCap and rotary encoder), modular visuo-tactile sensing, and automated calibration, achieving 100% data usability. Building on an efficient collection of over 1 M tactile frames, we propose Tactile Prediction Pretraining (TPP), a representation learning framework through action-aware temporal tactile prediction, capturing contact dynamics and mitigates tactile sparsity. Real-world experiments show that TPP outperforms traditional tactile imitation learning. Our work bridges the gap between human tactile intuition and robot learning through co-designed hardware and algorithms, offering open-source resources to advance contact-rich manipulation research.",
        "keywords": [
            "Tactile Sensing",
            "Robot Data Collection System",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/03d8d4d3a35e47429869209e901179337767cfc9.pdf",
        "reviews": [
            {
                "id": "51q3JV6N6M",
                "forum": "b86nyIOJWq",
                "replyto": "b86nyIOJWq",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission85/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068259201,
                "mdate": 1754801278744,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "b86nyIOJWq",
                "forum": "b86nyIOJWq",
                "content": {
                    "title": {
                        "value": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation"
                    },
                    "authors": {
                        "value": [
                            "Yue Xu",
                            "Litao Wei",
                            "Pengyu An",
                            "Qingyu Zhang",
                            "Yong-Lu Li"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yue_Xu4",
                            "~Litao_Wei1",
                            "~Pengyu_An1",
                            "~Qingyu_Zhang7",
                            "~Yong-Lu_Li1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Tactile Sensing",
                            "Robot Data Collection System",
                            "Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Tactile-aware robot learning faces critical challenges in data collection and representation due to data scarcity and sparsity, and the absence of force feedback in existing systems. To address these limitations, we introduce a tactile robot learning system with both hardware and algorithm innovations. We present exUMI, an extensible data collection device that enhances the vanilla UMI with robust proprioception (via AR MoCap and rotary encoder), modular visuo-tactile sensing, and automated calibration, achieving 100% data usability. Building on an efficient collection of over 1 M tactile frames, we propose Tactile Prediction Pretraining (TPP), a representation learning framework through action-aware temporal tactile prediction, capturing contact dynamics and mitigates tactile sparsity. Real-world experiments show that TPP outperforms traditional tactile imitation learning. Our work bridges the gap between human tactile intuition and robot learning through co-designed hardware and algorithms, offering open-source resources to advance contact-rich manipulation research."
                    },
                    "supplementary_material": {
                        "value": "/attachment/95c49bb7268d70c68bca741d0aa705ea19d64717.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/03d8d4d3a35e47429869209e901179337767cfc9.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nxu2025exumi,\ntitle={ex{UMI}: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation},\nauthor={Yue Xu and Litao Wei and Pengyu An and Qingyu Zhang and Yong-Lu Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b86nyIOJWq}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5350d299d0a22ae04e357c8fcca93067663d9ce0.mp4"
                    },
                    "paperhash": {
                        "value": "xu|exumi_extensible_robot_teaching_system_with_actionaware_taskagnostic_tactile_representation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission85/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission85/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission85/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744283163952,
                "pdate": 1754680601024,
                "odate": 1758062730837,
                "mdate": 1758062806341,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission85/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "DKXx17oaUf",
        "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion",
        "abstract": "LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on the KITTI and KITTI-360 datasets demonstrate OPAL’s superiority, achieving 15.98% higher recall at  @1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and datasets will be publicly available.",
        "keywords": [
            "Place Recognition",
            "OpenStreetMap",
            "Point Cloud"
        ],
        "pdf_url": "https://openreview.net/pdf/430fd8a43e63f79b370985f8e797008e869b07ef.pdf",
        "reviews": [
            {
                "id": "bJASCg9L71",
                "forum": "DKXx17oaUf",
                "replyto": "DKXx17oaUf",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission79/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258995,
                "mdate": 1754801278086,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "DKXx17oaUf",
                "forum": "DKXx17oaUf",
                "content": {
                    "title": {
                        "value": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion"
                    },
                    "authors": {
                        "value": [
                            "Shuhao Kang",
                            "Youqi Liao",
                            "Yan Xia",
                            "Olaf Wysocki",
                            "Boris Jutzi",
                            "Daniel Cremers"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Shuhao_Kang1",
                            "~Youqi_Liao1",
                            "~Yan_Xia5",
                            "~Olaf_Wysocki1",
                            "~Boris_Jutzi1",
                            "~Daniel_Cremers1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Place Recognition",
                            "OpenStreetMap",
                            "Point Cloud"
                        ]
                    },
                    "TLDR": {
                        "value": "A novel learning-based method for point cloud-to-OpenStreetMap place recognition"
                    },
                    "abstract": {
                        "value": "LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on the KITTI and KITTI-360 datasets demonstrate OPAL’s superiority, achieving 15.98% higher recall at  @1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and datasets will be publicly available."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ad85af308b995247afcfeb0efc911049e4904070.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/430fd8a43e63f79b370985f8e797008e869b07ef.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkang2025opal,\ntitle={{OPAL}: Visibility-aware Li{DAR}-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion},\nauthor={Shuhao Kang and Youqi Liao and Yan Xia and Olaf Wysocki and Boris Jutzi and Daniel Cremers},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DKXx17oaUf}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/78ec461b5874cf397191ac97f438bc0521a6bea7.mp4"
                    },
                    "paperhash": {
                        "value": "kang|opal_visibilityaware_lidartoopenstreetmap_place_recognition_via_adaptive_radial_fusion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission79/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission79/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission79/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744252144743,
                "pdate": 1754680600766,
                "odate": 1758062719163,
                "mdate": 1758062806354,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission79/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission79/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "FwLMCbs47K",
        "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion",
        "abstract": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution.\nTo address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning.\nIn practice, to further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution.\nExtensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.",
        "keywords": [
            "Autoregressive",
            "Causal Diffusion Policy"
        ],
        "pdf_url": "https://openreview.net/pdf/187913380c8e7ddbfaa072498e94148f845e499c.pdf",
        "reviews": [
            {
                "id": "LZt2VNrx1D",
                "forum": "FwLMCbs47K",
                "replyto": "FwLMCbs47K",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission77/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070538563,
                "mdate": 1754801278158,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "FwLMCbs47K",
                "forum": "FwLMCbs47K",
                "content": {
                    "title": {
                        "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion"
                    },
                    "authors": {
                        "value": [
                            "Jiahua Ma",
                            "Yiran Qin",
                            "Yixiong Li",
                            "Xuanqi Liao",
                            "Yulan Guo",
                            "Ruimao Zhang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Jiahua_Ma2",
                            "~Yiran_Qin1",
                            "~Yixiong_Li1",
                            "~Xuanqi_Liao1",
                            "~Yulan_Guo3",
                            "~Ruimao_Zhang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Autoregressive",
                            "Causal Diffusion Policy"
                        ]
                    },
                    "abstract": {
                        "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution.\nTo address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning.\nIn practice, to further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution.\nExtensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/187913380c8e7ddbfaa072498e94148f845e499c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nma2025cdp,\ntitle={{CDP}: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion},\nauthor={Jiahua Ma and Yiran Qin and Yixiong Li and Xuanqi Liao and Yulan Guo and Ruimao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FwLMCbs47K}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/21b6dd6278c8c59c6e0d0fe5418461cda6ee68b7.mp4"
                    },
                    "paperhash": {
                        "value": "ma|cdp_towards_robust_autoregressive_visuomotor_policy_learning_via_causal_diffusion"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission77/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission77/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission77/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744206736570,
                "pdate": 1754680600697,
                "odate": 1758062714997,
                "mdate": 1758062806195,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission77/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission77/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "oRwcxFuN25",
        "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids",
        "abstract": "Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or starting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world training, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid student robot. The RTR system provides protection, schedule, reward, perturbation, failure detection, and automatic resets, enabling efficient long-term real-world training with minimal human intervention. Furthermore, we propose a novel RL pipeline that facilitates and stabilizes sim-to-real transfer by optimizing a single dynamics-encoded latent variable in the real world. We validate our method through two challenging real-world humanoid tasks: fine-tuning a walking policy for precise speed tracking and learning a humanoid swing-up task from scratch, illustrating the promising capabilities of real-world humanoid learning realized by RTR-style systems.",
        "keywords": [
            "Humanoid Robots",
            "Sim-to-Real Adaptation",
            "Real-World RL"
        ],
        "pdf_url": "https://openreview.net/pdf/eb41420ce1f2462d2b2b5f4391b9403796f0750c.pdf",
        "reviews": [
            {
                "id": "1MUl4IM9SI",
                "forum": "oRwcxFuN25",
                "replyto": "oRwcxFuN25",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission76/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258914,
                "mdate": 1754801278154,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "oRwcxFuN25",
                "forum": "oRwcxFuN25",
                "content": {
                    "title": {
                        "value": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids"
                    },
                    "authors": {
                        "value": [
                            "Kaizhe Hu",
                            "Haochen Shi",
                            "Yao He",
                            "Weizhuo Wang",
                            "Karen Liu",
                            "Shuran Song"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Kaizhe_Hu1",
                            "~Haochen_Shi2",
                            "~Yao_He1",
                            "~Weizhuo_Wang2",
                            "~Karen_Liu1",
                            "~Shuran_Song3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Humanoid Robots",
                            "Sim-to-Real Adaptation",
                            "Real-World RL"
                        ]
                    },
                    "abstract": {
                        "value": "Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or starting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world training, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid student robot. The RTR system provides protection, schedule, reward, perturbation, failure detection, and automatic resets, enabling efficient long-term real-world training with minimal human intervention. Furthermore, we propose a novel RL pipeline that facilitates and stabilizes sim-to-real transfer by optimizing a single dynamics-encoded latent variable in the real world. We validate our method through two challenging real-world humanoid tasks: fine-tuning a walking policy for precise speed tracking and learning a humanoid swing-up task from scratch, illustrating the promising capabilities of real-world humanoid learning realized by RTR-style systems."
                    },
                    "supplementary_material": {
                        "value": "/attachment/b86eeb96797275a58fda8e18c302a04979456a7f.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/eb41420ce1f2462d2b2b5f4391b9403796f0750c.pdf"
                    },
                    "TLDR": {
                        "value": "Robot-Trains-Robot uses a robot arm teacher to actively train a humanoid student, aiming for practical and highly efficient real-world humanoid adaptation and learning."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhu2025robot,\ntitle={Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids},\nauthor={Kaizhe Hu and Haochen Shi and Yao He and Weizhuo Wang and Karen Liu and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oRwcxFuN25}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/01906d94200d01f221afdf1621e71feedac8ca25.zip"
                    },
                    "paperhash": {
                        "value": "hu|robot_trains_robot_automatic_realworld_policy_adaptation_and_learning_for_humanoids"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission76/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission76/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission76/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744172913811,
                "pdate": 1754680600632,
                "odate": 1758062713198,
                "mdate": 1758062805894,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission76/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission76/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "AE299O0tph",
        "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
        "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harness VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, out performing the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs.",
        "keywords": [
            "Vision-Language Models",
            "Zero-shot UAV Navigation",
            "2D-to-3D",
            "Waypoint Prompting"
        ],
        "pdf_url": "https://openreview.net/pdf/b44230709715db67cd69181c02386cc87eae3fdb.pdf",
        "reviews": [
            {
                "id": "RJoyUPtsqL",
                "forum": "AE299O0tph",
                "replyto": "AE299O0tph",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission74/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258817,
                "mdate": 1754801277788,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "AE299O0tph",
                "forum": "AE299O0tph",
                "content": {
                    "title": {
                        "value": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation"
                    },
                    "authors": {
                        "value": [
                            "Chih Yao Hu",
                            "Yang-Sen Lin",
                            "Yuna Lee",
                            "Chih-Hai Su",
                            "Jie-Ying Lee",
                            "Shr-Ruei Tsai",
                            "Chin-Yang Lin",
                            "Kuan-Wen Chen",
                            "Tsung-Wei Ke",
                            "Yu-Lun Liu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chih_Yao_Hu1",
                            "~Yang-Sen_Lin1",
                            "~Yuna_Lee1",
                            "~Chih-Hai_Su1",
                            "~Jie-Ying_Lee1",
                            "~Shr-Ruei_Tsai1",
                            "~Chin-Yang_Lin1",
                            "~Kuan-Wen_Chen2",
                            "~Tsung-Wei_Ke2",
                            "~Yu-Lun_Liu2"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language Models",
                            "Zero-shot UAV Navigation",
                            "2D-to-3D",
                            "Waypoint Prompting"
                        ]
                    },
                    "abstract": {
                        "value": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harness VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, out performing the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs."
                    },
                    "supplementary_material": {
                        "value": "/attachment/e2b95c2db33e527d94ce0a4772a5b543324d50ff.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/b44230709715db67cd69181c02386cc87eae3fdb.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nhu2025see,\ntitle={See, Point, Fly: A Learning-Free {VLM} Framework for Universal Unmanned Aerial Navigation},\nauthor={Chih Yao Hu and Yang-Sen Lin and Yuna Lee and Chih-Hai Su and Jie-Ying Lee and Shr-Ruei Tsai and Chin-Yang Lin and Kuan-Wen Chen and Tsung-Wei Ke and Yu-Lun Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AE299O0tph}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/6c4cf3834507d0e144c0de1116ea586a9eff40af.zip"
                    },
                    "paperhash": {
                        "value": "hu|see_point_fly_a_learningfree_vlm_framework_for_universal_unmanned_aerial_navigation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission74/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission74/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission74/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744125916131,
                "pdate": 1754680600570,
                "odate": 1758062712513,
                "mdate": 1758062806101,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission74/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission74/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1D6XYy6ofW",
        "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation",
        "abstract": "Visual-textual understanding is essential for language-guided robot manipulation. Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions. However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks. We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions. This formulation allows the model to learn visual-action representations without robot action supervision. The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations. We also introduce the \\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions. This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances. Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art.",
        "keywords": [
            "Robot manipulation",
            "self-supervised representation learning"
        ],
        "pdf_url": "https://openreview.net/pdf/f13693e0bc8002f3116b9498e3c20862e129b27e.pdf",
        "reviews": [
            {
                "id": "L1AN9XRDwV",
                "forum": "1D6XYy6ofW",
                "replyto": "1D6XYy6ofW",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission72/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258777,
                "mdate": 1754801277590,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1D6XYy6ofW",
                "forum": "1D6XYy6ofW",
                "content": {
                    "title": {
                        "value": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Chaoran Zhu",
                            "Hengyi Wang",
                            "Yik Lung Pang",
                            "Changjae Oh"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Chaoran_Zhu1",
                            "~Hengyi_Wang2",
                            "~Yik_Lung_Pang1",
                            "~Changjae_Oh1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot manipulation",
                            "self-supervised representation learning"
                        ]
                    },
                    "TLDR": {
                        "value": "Self-supervised pre-training method for langauge-conditioned robot manipulation"
                    },
                    "abstract": {
                        "value": "Visual-textual understanding is essential for language-guided robot manipulation. Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions. However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks. We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions. This formulation allows the model to learn visual-action representations without robot action supervision. The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations. We also introduce the \\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions. This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances. Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c85c01ea1e7346775d68345268f15c316a5f7dc7.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f13693e0bc8002f3116b9498e3c20862e129b27e.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhu2025lavaman,\ntitle={La{VA}-Man: Learning Visual Action Representations for Robot Manipulation},\nauthor={Chaoran Zhu and Hengyi Wang and Yik Lung Pang and Changjae Oh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1D6XYy6ofW}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/d8869cb7d5e488f848114a8c014f120f712dd5ae.mp4"
                    },
                    "paperhash": {
                        "value": "zhu|lavaman_learning_visual_action_representations_for_robot_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission72/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission72/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission72/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744109421632,
                "pdate": 1754680600510,
                "odate": 1758062709320,
                "mdate": 1758062806048,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission72/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission72/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "dT45OMevL5",
        "title": "3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation",
        "abstract": "Recently, 2D vision-language-action (VLA) models have made significant strides in multi-task manipulation. However, these models struggle to reason about 3D spatial relationships from 2D image inputs. Although an increasing number of 3D approaches explicitly integrate 3D information, they encounter challenges such as limited availability of large-scale 3D datasets and loss of spatial information during input processing. Meanwhile, existing policies typically focus on the perception-to-action learning paradigm, lacking an explicit understanding of the spatial and temporal relationships between the robot and its environment. To address this, we propose 3DS-VLA, which enhances pretrained 2D vision-language models (VLMs) with comprehensive 3D awareness, enabling the prediction of robust end-effector poses.\nSpecifically, we enable a 2D vision encoder to encode both 2D images and 3D spatial observation by introducing a 2D-to-3D positional alignment mechanism. This allows 3DS-VLA to leverage the large-scale pre-trained knowledge of the VLM for effective reasoning in complex 3D robotic environments. Furthermore, to better understand the spatiotemporal relationship between 3D observations and robot behavior, we guide the model to learn the introduced sequential 3D spatial constraints, which define affordance-relevant 3D keypoints on objects, ensuring robust interactions. Experiments in simulated and real-world demonstrate that 3DS-VLA outperforms previous state-of-the-art policies and showcase its generalizable capabilities across multi-task, multi-embodiment, and diverse environmental settings.",
        "keywords": [
            "Vision-Language-Action; Robotic Manipulation; Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/2db27aa32a273716029a3a1f929f4ac3ab7d34c5.pdf",
        "reviews": [
            {
                "id": "kve6d3qdCR",
                "forum": "dT45OMevL5",
                "replyto": "dT45OMevL5",
                "content": {
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "title": {
                        "value": "Paper Decision"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission71/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754071386572,
                "mdate": 1754801277509,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "dT45OMevL5",
                "forum": "dT45OMevL5",
                "content": {
                    "title": {
                        "value": "3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation"
                    },
                    "authors": {
                        "value": [
                            "Xiaoqi Li",
                            "Liang Heng",
                            "Jiaming Liu",
                            "Yan Shen",
                            "Chenyang Gu",
                            "Zhuoyang Liu",
                            "Hao Chen",
                            "Nuowei Han",
                            "Renrui Zhang",
                            "Hao Tang",
                            "Shanghang Zhang",
                            "Hao Dong"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xiaoqi_Li3",
                            "~Liang_Heng1",
                            "~Jiaming_Liu2",
                            "~Yan_Shen3",
                            "~Chenyang_Gu1",
                            "~Zhuoyang_Liu1",
                            "~Hao_Chen38",
                            "~Nuowei_Han1",
                            "~Renrui_Zhang1",
                            "~Hao_Tang6",
                            "~Shanghang_Zhang4",
                            "~Hao_Dong3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action; Robotic Manipulation; Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "Recently, 2D vision-language-action (VLA) models have made significant strides in multi-task manipulation. However, these models struggle to reason about 3D spatial relationships from 2D image inputs. Although an increasing number of 3D approaches explicitly integrate 3D information, they encounter challenges such as limited availability of large-scale 3D datasets and loss of spatial information during input processing. Meanwhile, existing policies typically focus on the perception-to-action learning paradigm, lacking an explicit understanding of the spatial and temporal relationships between the robot and its environment. To address this, we propose 3DS-VLA, which enhances pretrained 2D vision-language models (VLMs) with comprehensive 3D awareness, enabling the prediction of robust end-effector poses.\nSpecifically, we enable a 2D vision encoder to encode both 2D images and 3D spatial observation by introducing a 2D-to-3D positional alignment mechanism. This allows 3DS-VLA to leverage the large-scale pre-trained knowledge of the VLM for effective reasoning in complex 3D robotic environments. Furthermore, to better understand the spatiotemporal relationship between 3D observations and robot behavior, we guide the model to learn the introduced sequential 3D spatial constraints, which define affordance-relevant 3D keypoints on objects, ensuring robust interactions. Experiments in simulated and real-world demonstrate that 3DS-VLA outperforms previous state-of-the-art policies and showcase its generalizable capabilities across multi-task, multi-embodiment, and diverse environmental settings."
                    },
                    "supplementary_material": {
                        "value": "/attachment/f4040f62d3771aa7682b3aa3f60d80d2ed6ca04d.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/2db27aa32a273716029a3a1f929f4ac3ab7d34c5.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nli2025dsvla,\ntitle={3{DS}-{VLA}: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation},\nauthor={Xiaoqi Li and Liang Heng and Jiaming Liu and Yan Shen and Chenyang Gu and Zhuoyang Liu and Hao Chen and Nuowei Han and Renrui Zhang and Hao Tang and Shanghang Zhang and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dT45OMevL5}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/895970794ac5f0ef4c113fd550c2697b5e20df46.zip"
                    },
                    "paperhash": {
                        "value": "li|3dsvla_a_3d_spatialaware_vision_language_action_model_for_robust_multitask_manipulation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission71/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission71/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission71/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744089253296,
                "pdate": 1754680600455,
                "odate": 1758062703322,
                "mdate": 1758062805613,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission71/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission71/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "KUSYJIlKor",
        "title": "Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments",
        "abstract": "Agile locomotion in complex 3D environments requires robust spatial awareness to safely avoid diverse obstacles such as aerial clutter, uneven terrain, and dynamic agents. Depth-based perception approaches often struggle with sensor noise, lighting variability, computational overhead from intermediate representations (e.g., elevation maps), and difficulties with non-planar obstacles, limiting performance in unstructured environments. In contrast, direct integration of LiDAR sensing into end-to-end learning for legged locomotion remains underexplored.We propose Omni-Perception, an end-to-end locomotion policy that achieves 3D spatial awareness and omnidirectional collision avoidance by directly processing raw LiDAR point clouds. At its core is PD-RiskNet (Proximal-Distal Risk-Aware Hierarchical Network), a novel perception module that interprets spatio-temporal LiDAR data for environmental risk assessment. To facilitate efficient policy learning, we develop a high-fidelity LiDAR simulation toolkit with realistic noise modeling and fast raycasting, compatible with platforms such as Isaac Gym, Genesis, and MuJoCo, enabling scalable training and effective sim-to-real transfer.Learning reactive control policies directly from raw LiDAR data enables the robot to navigate complex environments with static and dynamic obstacles more robustly than approaches relying on intermediate maps or limited sensing. We validate Omni-Perception through real-world experiments and extensive simulation, demonstrating strong omnidirectional avoidance capabilities and superior locomotion performance in highly dynamic environments.We will open-source our code and models.",
        "keywords": [
            "Locomotion",
            "Reinforcement Learning",
            "Legged robot"
        ],
        "pdf_url": "https://openreview.net/pdf/6fa104513fad0b16722f07d255a21d3f15c1c019.pdf",
        "reviews": [
            {
                "id": "GyDVUixUuC",
                "forum": "KUSYJIlKor",
                "replyto": "KUSYJIlKor",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission66/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258506,
                "mdate": 1754801273488,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "KUSYJIlKor",
                "forum": "KUSYJIlKor",
                "content": {
                    "title": {
                        "value": "Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments"
                    },
                    "authors": {
                        "value": [
                            "Zifan Wang",
                            "Teli Ma",
                            "Yufei Jia",
                            "Xun Yang",
                            "Jiaming Zhou",
                            "Wenlong OUYANG",
                            "Qiang Zhang",
                            "Junwei Liang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zifan_Wang7",
                            "~Teli_Ma1",
                            "~Yufei_Jia1",
                            "~Xun_Yang6",
                            "~Jiaming_Zhou1",
                            "~Wenlong_OUYANG2",
                            "~Qiang_Zhang10",
                            "~Junwei_Liang1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Locomotion",
                            "Reinforcement Learning",
                            "Legged robot"
                        ]
                    },
                    "abstract": {
                        "value": "Agile locomotion in complex 3D environments requires robust spatial awareness to safely avoid diverse obstacles such as aerial clutter, uneven terrain, and dynamic agents. Depth-based perception approaches often struggle with sensor noise, lighting variability, computational overhead from intermediate representations (e.g., elevation maps), and difficulties with non-planar obstacles, limiting performance in unstructured environments. In contrast, direct integration of LiDAR sensing into end-to-end learning for legged locomotion remains underexplored.We propose Omni-Perception, an end-to-end locomotion policy that achieves 3D spatial awareness and omnidirectional collision avoidance by directly processing raw LiDAR point clouds. At its core is PD-RiskNet (Proximal-Distal Risk-Aware Hierarchical Network), a novel perception module that interprets spatio-temporal LiDAR data for environmental risk assessment. To facilitate efficient policy learning, we develop a high-fidelity LiDAR simulation toolkit with realistic noise modeling and fast raycasting, compatible with platforms such as Isaac Gym, Genesis, and MuJoCo, enabling scalable training and effective sim-to-real transfer.Learning reactive control policies directly from raw LiDAR data enables the robot to navigate complex environments with static and dynamic obstacles more robustly than approaches relying on intermediate maps or limited sensing. We validate Omni-Perception through real-world experiments and extensive simulation, demonstrating strong omnidirectional avoidance capabilities and superior locomotion performance in highly dynamic environments.We will open-source our code and models."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/6fa104513fad0b16722f07d255a21d3f15c1c019.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nwang2025omniperception,\ntitle={Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments},\nauthor={Zifan Wang and Teli Ma and Yufei Jia and Xun Yang and Jiaming Zhou and Wenlong OUYANG and Qiang Zhang and Junwei Liang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KUSYJIlKor}\n}"
                    },
                    "paperhash": {
                        "value": "wang|omniperception_omnidirectional_collision_avoidance_of_legged_robots_in_dynamic_environments"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission66/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission66/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission66/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1744012397385,
                "pdate": 1754680600382,
                "odate": 1758062702194,
                "mdate": 1758062805575,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission66/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission66/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "Vo1tL9dhpk",
        "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes",
        "abstract": "Terrain elevation modeling for off-road navigation aims to accurately estimate changes in terrain geometry in real-time and quantify the corresponding uncertainties. Having precise estimations and uncertainties plays a crucial role in planning and control algorithms to explore safe and reliable maneuver strategies. However, existing approaches, such as Gaussian Processes (GPs) and neural network-based methods, often fail to meet these needs. They are either unable to perform in real-time due to high computational demands, underestimating sharp geometry changes, or harming elevation accuracy when learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a promising approach that integrates the Bayesian uncertainty estimation of GPs with the efficiency and flexibility of neural networks. Inspired by NPs, we propose an effective NP-based method that precisely estimates sharp elevation changes and quantifies the corresponding predictive uncertainty without losing elevation accuracy. Our method leverages semantic features from LiDAR and camera sensors to improve interpolation and extrapolation accuracy in unobserved regions. Also, we introduce a local ball-query attention mechanism to effectively reduce the computational complexity of global attention by 17\\% while preserving crucial local and spatial information. We evaluate our method on off-road datasets having interesting geometric features, collected from trails, deserts, and hills. Our results demonstrate superior performance over baselines and showcase the potential of neural processes for effective and expressive terrain modeling in complex off-road environments.",
        "keywords": [
            "Robot perception",
            "Terrain modeling",
            "Neural processes"
        ],
        "pdf_url": "https://openreview.net/pdf/3dca3fff962073b90c0b4777b25b5c886cc67b95.pdf",
        "reviews": [
            {
                "id": "BdWsezqSL2",
                "forum": "Vo1tL9dhpk",
                "replyto": "Vo1tL9dhpk",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission65/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258484,
                "mdate": 1754801277428,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "Vo1tL9dhpk",
                "forum": "Vo1tL9dhpk",
                "content": {
                    "title": {
                        "value": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes"
                    },
                    "authors": {
                        "value": [
                            "Sanghun Jung",
                            "Daehoon Gwak",
                            "Byron Boots",
                            "James Hays"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sanghun_Jung1",
                            "~Daehoon_Gwak1",
                            "~Byron_Boots1",
                            "~James_Hays1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robot perception",
                            "Terrain modeling",
                            "Neural processes"
                        ]
                    },
                    "abstract": {
                        "value": "Terrain elevation modeling for off-road navigation aims to accurately estimate changes in terrain geometry in real-time and quantify the corresponding uncertainties. Having precise estimations and uncertainties plays a crucial role in planning and control algorithms to explore safe and reliable maneuver strategies. However, existing approaches, such as Gaussian Processes (GPs) and neural network-based methods, often fail to meet these needs. They are either unable to perform in real-time due to high computational demands, underestimating sharp geometry changes, or harming elevation accuracy when learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a promising approach that integrates the Bayesian uncertainty estimation of GPs with the efficiency and flexibility of neural networks. Inspired by NPs, we propose an effective NP-based method that precisely estimates sharp elevation changes and quantifies the corresponding predictive uncertainty without losing elevation accuracy. Our method leverages semantic features from LiDAR and camera sensors to improve interpolation and extrapolation accuracy in unobserved regions. Also, we introduce a local ball-query attention mechanism to effectively reduce the computational complexity of global attention by 17\\% while preserving crucial local and spatial information. We evaluate our method on off-road datasets having interesting geometric features, collected from trails, deserts, and hills. Our results demonstrate superior performance over baselines and showcase the potential of neural processes for effective and expressive terrain modeling in complex off-road environments."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/3dca3fff962073b90c0b4777b25b5c886cc67b95.pdf"
                    },
                    "TLDR": {
                        "value": "We propose semantic-conditioned Neural Processes with ball-query attention for sharp elevation estimates along the associated uncertainties."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njung2025uncertaintyaware,\ntitle={Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes},\nauthor={Sanghun Jung and Daehoon Gwak and Byron Boots and James Hays},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Vo1tL9dhpk}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0773fc9d8490f7a25c7b6e7706a35a2455277ca7.mp4"
                    },
                    "paperhash": {
                        "value": "jung|uncertaintyaware_accurate_elevation_modeling_for_offroad_navigation_via_neural_processes"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission65/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission65/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission65/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743989534941,
                "pdate": 1754680600258,
                "odate": 1758062697720,
                "mdate": 1758062805596,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission65/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission65/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "hh9afiQMb2",
        "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning",
        "abstract": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation. Website and code can be found at https://submfort.github.io/fortress/.",
        "keywords": [
            "Multi-modal Reasoning in Robotics",
            "OOD Safety",
            "Fallback Synthesis"
        ],
        "pdf_url": "https://openreview.net/pdf/293fc3504eb4a1e20040472fab1aa7d6d385be29.pdf",
        "reviews": [
            {
                "id": "Bklf1E2enR",
                "forum": "hh9afiQMb2",
                "replyto": "hh9afiQMb2",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission61/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070537172,
                "mdate": 1754801273263,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "hh9afiQMb2",
                "forum": "hh9afiQMb2",
                "content": {
                    "title": {
                        "value": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning"
                    },
                    "authors": {
                        "value": [
                            "Milan Ganai",
                            "Rohan Sinha",
                            "Christopher Agia",
                            "Daniel Morton",
                            "Luigi Di Lillo",
                            "Marco Pavone"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Milan_Ganai1",
                            "~Rohan_Sinha1",
                            "~Christopher_Agia1",
                            "~Daniel_Morton1",
                            "~Luigi_Di_Lillo1",
                            "~Marco_Pavone1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Multi-modal Reasoning in Robotics",
                            "OOD Safety",
                            "Fallback Synthesis"
                        ]
                    },
                    "abstract": {
                        "value": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation. Website and code can be found at https://submfort.github.io/fortress/."
                    },
                    "supplementary_material": {
                        "value": "/attachment/0cd4f3536cbe852d956bbe884aa73a9c97ab34b3.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/293fc3504eb4a1e20040472fab1aa7d6d385be29.pdf"
                    },
                    "TLDR": {
                        "value": "An algorithm that prevents OOD Failures in open-world environments by rapidly generating semantically safe fallback plans with multi-modal reasoning."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nganai2025realtime,\ntitle={Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning},\nauthor={Milan Ganai and Rohan Sinha and Christopher Agia and Daniel Morton and Luigi Di Lillo and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=hh9afiQMb2}\n}"
                    },
                    "paperhash": {
                        "value": "ganai|realtime_outofdistribution_failure_prevention_via_multimodal_reasoning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission61/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission61/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission61/-/Spotlight_And_Camera_Ready",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision"
                ],
                "cdate": 1743961929520,
                "pdate": 1754680600192,
                "odate": 1758062696494,
                "mdate": 1758062805252,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission61/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission61/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "ZqBXnR6ppz",
        "title": "Generalist Robot Manipulation beyond Action Labeled Data",
        "abstract": "Recent advances in generalist robot manipulation leverage pre-trained Vision–Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.",
        "keywords": [
            "Vision-Language-Action Models",
            "Learning from Videos"
        ],
        "pdf_url": "https://openreview.net/pdf/f7df449e236dfb0819c39b029b9f35ca26de4a3c.pdf",
        "reviews": [
            {
                "id": "BNOn9Ihzsw",
                "forum": "ZqBXnR6ppz",
                "replyto": "ZqBXnR6ppz",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission60/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258280,
                "mdate": 1754801277344,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "ZqBXnR6ppz",
                "forum": "ZqBXnR6ppz",
                "content": {
                    "title": {
                        "value": "Generalist Robot Manipulation beyond Action Labeled Data"
                    },
                    "authors": {
                        "value": [
                            "Alexander Spiridonov",
                            "Jan-Nico Zaech",
                            "Nikolay Nikolov",
                            "Luc Van Gool",
                            "Danda Pani Paudel"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Alexander_Spiridonov1",
                            "~Jan-Nico_Zaech1",
                            "~Nikolay_Nikolov1",
                            "~Luc_Van_Gool1",
                            "~Danda_Pani_Paudel3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Vision-Language-Action Models",
                            "Learning from Videos"
                        ]
                    },
                    "abstract": {
                        "value": "Recent advances in generalist robot manipulation leverage pre-trained Vision–Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels—featuring humans and/or robots in action—enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations—improving downstream generalist robot policies—but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/f7df449e236dfb0819c39b029b9f35ca26de4a3c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nspiridonov2025generalist,\ntitle={Generalist Robot Manipulation beyond Action Labeled Data},\nauthor={Alexander Spiridonov and Jan-Nico Zaech and Nikolay Nikolov and Luc Van Gool and Danda Pani Paudel},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZqBXnR6ppz}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/3edc9eb9700a1eab3cfe229142213f747c89511c.mp4"
                    },
                    "paperhash": {
                        "value": "spiridonov|generalist_robot_manipulation_beyond_action_labeled_data"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission60/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission60/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission60/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743935736774,
                "pdate": 1754680600103,
                "odate": 1758062693442,
                "mdate": 1758062805241,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission60/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission60/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "jedBaI1fgU",
        "title": "BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions",
        "abstract": "Modeling the nuanced, multimodal nature of human driving remains a core challenge for autonomous systems, as existing methods often fail to capture the diversity of plausible behaviors in complex real-world scenarios. In this work, we introduce a novel benchmark and end-to-end planner for modeling realistic multimodality in autonomous driving decisions. \nWe propose a Gaussian Mixture Model (GMM)-based diffusion model designed to explicitly capture human-like, multimodal driving decisions in diverse contexts. Our model achieves state-of-the-art performance on current benchmarks, but reveals weaknesses in standard evaluation practices, which rely on single ground-truth trajectories or coarse closed-loop metrics while often penalizing diverse yet plausible alternatives. To address this limitation, we further develop a human-in-the-loop simulation benchmark that enables finer-grained evaluations and measures multimodal realism in challenging driving settings. Our code, models, and benchmark data will be released to promote more accurate and human-aware evaluation of autonomous driving models.",
        "keywords": [
            "Autonomous Driving",
            "Human-in-the-Loop Simulation",
            "Multi-modal Planning and Evaluation"
        ],
        "pdf_url": "https://openreview.net/pdf/8a87e7f37431a21394eac9a184d3461226abe4c7.pdf",
        "reviews": [
            {
                "id": "fKKsVRUKhI",
                "forum": "jedBaI1fgU",
                "replyto": "jedBaI1fgU",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission55/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754070538552,
                "mdate": 1754801277219,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "jedBaI1fgU",
                "forum": "jedBaI1fgU",
                "content": {
                    "title": {
                        "value": "BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions"
                    },
                    "authors": {
                        "value": [
                            "Hee Jae Kim",
                            "Zekai Yin",
                            "Lei Lai",
                            "Jason Lee",
                            "Eshed Ohn-Bar"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Hee_Jae_Kim1",
                            "~Zekai_Yin1",
                            "~Lei_Lai1",
                            "~Jason_Lee6",
                            "~Eshed_Ohn-Bar4"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Autonomous Driving",
                            "Human-in-the-Loop Simulation",
                            "Multi-modal Planning and Evaluation"
                        ]
                    },
                    "abstract": {
                        "value": "Modeling the nuanced, multimodal nature of human driving remains a core challenge for autonomous systems, as existing methods often fail to capture the diversity of plausible behaviors in complex real-world scenarios. In this work, we introduce a novel benchmark and end-to-end planner for modeling realistic multimodality in autonomous driving decisions. \nWe propose a Gaussian Mixture Model (GMM)-based diffusion model designed to explicitly capture human-like, multimodal driving decisions in diverse contexts. Our model achieves state-of-the-art performance on current benchmarks, but reveals weaknesses in standard evaluation practices, which rely on single ground-truth trajectories or coarse closed-loop metrics while often penalizing diverse yet plausible alternatives. To address this limitation, we further develop a human-in-the-loop simulation benchmark that enables finer-grained evaluations and measures multimodal realism in challenging driving settings. Our code, models, and benchmark data will be released to promote more accurate and human-aware evaluation of autonomous driving models."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/8a87e7f37431a21394eac9a184d3461226abe4c7.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkim2025branchout,\ntitle={BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions},\nauthor={Hee Jae Kim and Zekai Yin and Lei Lai and Jason Lee and Eshed Ohn-Bar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jedBaI1fgU}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/8cc028ba3fe9c22734372f0d81f28984a39ef5fb.mp4"
                    },
                    "paperhash": {
                        "value": "kim|branchout_capturing_realistic_multimodality_in_autonomous_driving_decisions"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission55/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission55/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission55/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743891155345,
                "pdate": 1754680599936,
                "odate": 1758062692562,
                "mdate": 1758062805190,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission55/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission55/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "VqmAvBkFhw",
        "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation",
        "abstract": "Humans and animals exhibit flexible locomotion strategies, such as learning to walk within minutes, and efficient adaptation to changes in morphology. In contrast, modern locomotion controllers are manually tuned for specific embodiments. In this paper, we present LocoFormer, a generalist policy that can control previously unseen legged and wheeled robots, even without precise knowledge of their kinematics. LocoFormer is able to adapt to changes in morphology and dynamics at test time. We find that two key choices enable adaptation. First, we train massive scale RL on procedurally generated robots with aggressive domain randomization. Second, in contrast to previous policies that are myopic with short context lengths, we extend context by orders of magnitude to span episode boundaries. We deploy the same LocoFormer to varied robots, and show robust control even with large disturbances such as weight and motor failures. In extreme scenarios, we see emergent adaptation across episodes, LocoFormer learns from falls in early episodes to improve control strategies in later ones. We believe this simple yet general recipe can be used to train foundation models for other robotic skills in the future. Videos at generalist-locomotion.github.io.",
        "keywords": [
            "Cross-Embodied Learning",
            "Legged Locomotion",
            "Online Adaptation"
        ],
        "pdf_url": "https://openreview.net/pdf/09f1069aefe99e01a46eef1033e2dbd007f687bd.pdf",
        "reviews": [
            {
                "id": "2zbVXK0wfw",
                "forum": "VqmAvBkFhw",
                "replyto": "VqmAvBkFhw",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission50/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068258003,
                "mdate": 1754801273265,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "VqmAvBkFhw",
                "forum": "VqmAvBkFhw",
                "content": {
                    "title": {
                        "value": "LocoFormer: Generalist Locomotion via Long-context Adaptation"
                    },
                    "authors": {
                        "value": [
                            "Min Liu",
                            "Deepak Pathak",
                            "Ananye Agarwal"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Min_Liu7",
                            "~Deepak_Pathak1",
                            "~Ananye_Agarwal1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Cross-Embodied Learning",
                            "Legged Locomotion",
                            "Online Adaptation"
                        ]
                    },
                    "abstract": {
                        "value": "Humans and animals exhibit flexible locomotion strategies, such as learning to walk within minutes, and efficient adaptation to changes in morphology. In contrast, modern locomotion controllers are manually tuned for specific embodiments. In this paper, we present LocoFormer, a generalist policy that can control previously unseen legged and wheeled robots, even without precise knowledge of their kinematics. LocoFormer is able to adapt to changes in morphology and dynamics at test time. We find that two key choices enable adaptation. First, we train massive scale RL on procedurally generated robots with aggressive domain randomization. Second, in contrast to previous policies that are myopic with short context lengths, we extend context by orders of magnitude to span episode boundaries. We deploy the same LocoFormer to varied robots, and show robust control even with large disturbances such as weight and motor failures. In extreme scenarios, we see emergent adaptation across episodes, LocoFormer learns from falls in early episodes to improve control strategies in later ones. We believe this simple yet general recipe can be used to train foundation models for other robotic skills in the future. Videos at generalist-locomotion.github.io."
                    },
                    "supplementary_material": {
                        "value": "/attachment/c5df75e80ceaa18ae6987d97d6d552077738c88a.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "LocoFormer is a generalist locomotion policy trained via large-scale RL and extended context memory, enabling it to robustly adapt to diverse, unseen legged and wheeled robots without prior knowledge."
                    },
                    "pdf": {
                        "value": "/pdf/09f1069aefe99e01a46eef1033e2dbd007f687bd.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025locoformer,\ntitle={LocoFormer: Generalist Locomotion via Long-context Adaptation},\nauthor={Min Liu and Deepak Pathak and Ananye Agarwal},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VqmAvBkFhw}\n}"
                    },
                    "paperhash": {
                        "value": "liu|locoformer_generalist_locomotion_via_longcontext_adaptation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission50/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission50/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission50/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743815918981,
                "pdate": 1754680599934,
                "odate": 1758062689566,
                "mdate": 1758062804859,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission50/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission50/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "EBcb4LemZZ",
        "title": "Co-Design of Soft Gripper with Neural Physics",
        "abstract": "For robot manipulation, both the controller and end-effector design are crucial. Compared with rigid grippers, soft grippers are more generalizable by deforming to different geometries, but designing such a gripper and finding its grasp pose remains challenging. In this paper, we propose a co-design framework that generates an optimized soft gripper’s block-wise stiffness distribution and its grasping pose, using a neural physics model trained in simulation. We adopt a uniform-pressure tendon model, then generate a diverse dataset by randomizing both gripper pose and design parameters. A neural network is trained to approximate this forward simulation, yielding a fast, differentiable surrogate. We embed that surrogate in an end-to-end optimization loop to recover the ideal stiffness configuration and best grasp pose. Finally, we 3D-print the optimized grippers of various stiffness by changing the printing infills and parameters. We demonstrate that our co-designed grippers significantly outperform baseline designs in terms of force closure and success rate.",
        "keywords": [
            "soft robot",
            "manipulation",
            "design optimization"
        ],
        "pdf_url": "https://openreview.net/pdf/4c74f0e821813aa2d5dee62e6b2ba09ffcdd7631.pdf",
        "reviews": [
            {
                "id": "Z6Km2aozgj",
                "forum": "EBcb4LemZZ",
                "replyto": "EBcb4LemZZ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission45/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068257520,
                "mdate": 1754801277125,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "EBcb4LemZZ",
                "forum": "EBcb4LemZZ",
                "content": {
                    "title": {
                        "value": "Co-Design of Soft Gripper with Neural Physics"
                    },
                    "authors": {
                        "value": [
                            "Sha Yi",
                            "Xueqian Bai",
                            "Adabhav Singh",
                            "Jianglong Ye",
                            "Michael T. Tolley",
                            "Xiaolong Wang"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sha_Yi1",
                            "~Xueqian_Bai1",
                            "~Adabhav_Singh1",
                            "~Jianglong_Ye1",
                            "~Michael_T._Tolley1",
                            "~Xiaolong_Wang3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "soft robot",
                            "manipulation",
                            "design optimization"
                        ]
                    },
                    "abstract": {
                        "value": "For robot manipulation, both the controller and end-effector design are crucial. Compared with rigid grippers, soft grippers are more generalizable by deforming to different geometries, but designing such a gripper and finding its grasp pose remains challenging. In this paper, we propose a co-design framework that generates an optimized soft gripper’s block-wise stiffness distribution and its grasping pose, using a neural physics model trained in simulation. We adopt a uniform-pressure tendon model, then generate a diverse dataset by randomizing both gripper pose and design parameters. A neural network is trained to approximate this forward simulation, yielding a fast, differentiable surrogate. We embed that surrogate in an end-to-end optimization loop to recover the ideal stiffness configuration and best grasp pose. Finally, we 3D-print the optimized grippers of various stiffness by changing the printing infills and parameters. We demonstrate that our co-designed grippers significantly outperform baseline designs in terms of force closure and success rate."
                    },
                    "supplementary_material": {
                        "value": "/attachment/ef8e99fcbd764b06ebaf226bac70b2105c3bf188.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/4c74f0e821813aa2d5dee62e6b2ba09ffcdd7631.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nyi2025codesign,\ntitle={Co-Design of Soft Gripper with Neural Physics},\nauthor={Sha Yi and Xueqian Bai and Adabhav Singh and Jianglong Ye and Michael T. Tolley and Xiaolong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EBcb4LemZZ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/926f07eb3b6b67cda151a4222727ce956a683094.zip"
                    },
                    "paperhash": {
                        "value": "yi|codesign_of_soft_gripper_with_neural_physics"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/PC_Revision",
                    "robot-learning.org/CoRL/2025/Conference/Submission45/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission45/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission45/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743792721854,
                "pdate": 1754680599752,
                "odate": 1758062687003,
                "mdate": 1758062804852,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission45/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission45/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "HAmi1X11BO",
        "title": "Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
        "abstract": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.  This is because torque signals align more closely with the decoder’s input, and the decoder is more sensitive to variations in input. Second, torque history proves to be a critical signal. We find that the most effective way to incorporate it is by summarizing the entire history into a single token, as this preserves the original input pattern of the decoder. Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings. Code, models, and datasets will be released.",
        "keywords": [
            "Torque Integration",
            "VLA Models"
        ],
        "pdf_url": "https://openreview.net/pdf/0daa40e6d51680e0d713917ea00f499035f8f68b.pdf",
        "reviews": [
            {
                "id": "2IK24fLGW4",
                "forum": "HAmi1X11BO",
                "replyto": "HAmi1X11BO",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission39/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068257496,
                "mdate": 1754801277054,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "HAmi1X11BO",
                "forum": "HAmi1X11BO",
                "content": {
                    "title": {
                        "value": "Elucidating the Design Space of Torque-aware Vision-Language-Action Models"
                    },
                    "authors": {
                        "value": [
                            "Zongzheng Zhang",
                            "Haobo Xu",
                            "Zhuo Yang",
                            "Chenghao Yue",
                            "Zehao Lin",
                            "Huan-ang Gao",
                            "Ziwei Wang",
                            "Hao Zhao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zongzheng_Zhang1",
                            "~Haobo_Xu2",
                            "~Zhuo_Yang6",
                            "~Chenghao_Yue1",
                            "~Zehao_Lin2",
                            "~Huan-ang_Gao1",
                            "~Ziwei_Wang2",
                            "~Hao_Zhao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Torque Integration",
                            "VLA Models"
                        ]
                    },
                    "abstract": {
                        "value": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.  This is because torque signals align more closely with the decoder’s input, and the decoder is more sensitive to variations in input. Second, torque history proves to be a critical signal. We find that the most effective way to incorporate it is by summarizing the entire history into a single token, as this preserves the original input pattern of the decoder. Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings. Code, models, and datasets will be released."
                    },
                    "supplementary_material": {
                        "value": "/attachment/25d0ae7e38b7f7283eb0cf8e9afcde5f3f31ad9c.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "Embedding torque history as a single decoder token and jointly predicting future torque alongside actions in pretrained vision-language-action models significantly boosts performance on contact-rich manipulation tasks."
                    },
                    "pdf": {
                        "value": "/pdf/0daa40e6d51680e0d713917ea00f499035f8f68b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025elucidating,\ntitle={Elucidating the Design Space of Torque-aware Vision-Language-Action Models},\nauthor={Zongzheng Zhang and Haobo Xu and Zhuo Yang and Chenghao Yue and Zehao Lin and Huan-ang Gao and Ziwei Wang and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HAmi1X11BO}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/2636777f3b13370ff07880a3d4b08c0e7100f205.zip"
                    },
                    "paperhash": {
                        "value": "zhang|elucidating_the_design_space_of_torqueaware_visionlanguageaction_models"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission39/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission39/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission39/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743772881381,
                "pdate": 1754680599689,
                "odate": 1758062686704,
                "mdate": 1758062804704,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission39/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission39/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "LbeMepi89R",
        "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
        "abstract": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose RoboChemist, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, $\\pi_0$) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show significant improvements in both success rate and compliance rate over state-of-the-art VLM and VLA baselines, while also demonstrating strong generalization to objects and tasks. Code, data, and models will be released.",
        "keywords": [
            "Robotic Chemistry",
            "VLA Models",
            "Visual Prompting"
        ],
        "pdf_url": "https://openreview.net/pdf/6c9b6902d3aff751cb09b1e392eaa30f97041339.pdf",
        "reviews": [
            {
                "id": "cTaeQ1ljll",
                "forum": "LbeMepi89R",
                "replyto": "LbeMepi89R",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission38/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068257272,
                "mdate": 1754801277004,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "LbeMepi89R",
                "forum": "LbeMepi89R",
                "content": {
                    "title": {
                        "value": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation"
                    },
                    "authors": {
                        "value": [
                            "Zongzheng Zhang",
                            "Chenghao Yue",
                            "Haobo Xu",
                            "Minwen Liao",
                            "Xianglin Qi",
                            "Huan-ang Gao",
                            "Ziwei Wang",
                            "Hao Zhao"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Zongzheng_Zhang1",
                            "~Chenghao_Yue1",
                            "~Haobo_Xu2",
                            "~Minwen_Liao1",
                            "~Xianglin_Qi1",
                            "~Huan-ang_Gao1",
                            "~Ziwei_Wang2",
                            "~Hao_Zhao1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Robotic Chemistry",
                            "VLA Models",
                            "Visual Prompting"
                        ]
                    },
                    "abstract": {
                        "value": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose RoboChemist, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, $\\pi_0$) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show significant improvements in both success rate and compliance rate over state-of-the-art VLM and VLA baselines, while also demonstrating strong generalization to objects and tasks. Code, data, and models will be released."
                    },
                    "supplementary_material": {
                        "value": "/attachment/9cff0bd680fd1f23494bd66d32902e4dc84f7865.zip"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "RoboChemist combines VLMs and VLA models in a close-loop framework to safely and reliably automate complex chemical experiments."
                    },
                    "pdf": {
                        "value": "/pdf/6c9b6902d3aff751cb09b1e392eaa30f97041339.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025robochemist,\ntitle={RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation},\nauthor={Zongzheng Zhang and Chenghao Yue and Haobo Xu and Minwen Liao and Xianglin Qi and Huan-ang Gao and Ziwei Wang and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LbeMepi89R}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/546761ba372861b60341869840baf4c5579905ec.zip"
                    },
                    "paperhash": {
                        "value": "zhang|robochemist_longhorizon_and_safetycompliant_robotic_chemical_experimentation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission38/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission38/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission38/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743772634717,
                "pdate": 1754680599629,
                "odate": 1758062683471,
                "mdate": 1758062804500,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission38/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission38/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "B6knAJsB9P",
        "title": "COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning",
        "abstract": "In this work, we study the problem of data retrieval for few-shot imitation learning: select data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and is prone to introducing detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task12 specific combination of multiple cues. COLLAGE follows a simple, but flexible and efficient data aggregation recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on their task relevance, measured by how well a policy trained on each subset predicts actions in the few target demonstrations. These weights are then used during policy training to perform importance sampling over the aggregated dataset, sampling data more densely or sparsely, according to their estimated relevance. This weighted aggregation strategy is general and feature-agnostic, allowing COLLAGE to combine and leverage any number of subsets selected by any retrieval heuristic or method, and to identify which subset provides the most benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches, achieving a 5.1% improvement over the best baseline in simulation across 10 tasks, and a 16.6% improvement in the real world across 6 tasks. For our real world experiments, we include data selection from the large-scale, real-world DROID dataset, significantly improving few-shot imitation policy training. More information at: https://collagecorl25.github.io/",
        "keywords": [
            "Data Retrieval",
            "Few-shot Learning",
            "Imitation Learning"
        ],
        "pdf_url": "https://openreview.net/pdf/fd36b8cbde4b76ccceee8b8484fb3fb584bd5dd1.pdf",
        "reviews": [
            {
                "id": "B48EUWa3lF",
                "forum": "B6knAJsB9P",
                "replyto": "B6knAJsB9P",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission36/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068257179,
                "mdate": 1754801276690,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "B6knAJsB9P",
                "forum": "B6knAJsB9P",
                "content": {
                    "title": {
                        "value": "COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning"
                    },
                    "authors": {
                        "value": [
                            "Sateesh Kumar",
                            "Shivin Dass",
                            "Georgios Pavlakos",
                            "Roberto Martín-Martín"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Sateesh_Kumar2",
                            "~Shivin_Dass2",
                            "~Georgios_Pavlakos1",
                            "~Roberto_Martín-Martín1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Data Retrieval",
                            "Few-shot Learning",
                            "Imitation Learning"
                        ]
                    },
                    "abstract": {
                        "value": "In this work, we study the problem of data retrieval for few-shot imitation learning: select data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and is prone to introducing detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task12 specific combination of multiple cues. COLLAGE follows a simple, but flexible and efficient data aggregation recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on their task relevance, measured by how well a policy trained on each subset predicts actions in the few target demonstrations. These weights are then used during policy training to perform importance sampling over the aggregated dataset, sampling data more densely or sparsely, according to their estimated relevance. This weighted aggregation strategy is general and feature-agnostic, allowing COLLAGE to combine and leverage any number of subsets selected by any retrieval heuristic or method, and to identify which subset provides the most benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches, achieving a 5.1% improvement over the best baseline in simulation across 10 tasks, and a 16.6% improvement in the real world across 6 tasks. For our real world experiments, we include data selection from the large-scale, real-world DROID dataset, significantly improving few-shot imitation policy training. More information at: https://collagecorl25.github.io/"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "COLLAGE improves augmented policy learning by adaptively reweighting data retrieved using multiple feature cues based on task relevance."
                    },
                    "pdf": {
                        "value": "/pdf/fd36b8cbde4b76ccceee8b8484fb3fb584bd5dd1.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nkumar2025collage,\ntitle={{COLLAGE}: Adaptive Fusion-based Retrieval for Augmented Policy Learning},\nauthor={Sateesh Kumar and Shivin Dass and Georgios Pavlakos and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=B6knAJsB9P}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/57c23d747c2a196b902d41c21c2cc1d2864b912a.mp4"
                    },
                    "paperhash": {
                        "value": "kumar|collage_adaptive_fusionbased_retrieval_for_augmented_policy_learning"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission36/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission36/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission36/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743746053572,
                "pdate": 1754680599561,
                "odate": 1758062683337,
                "mdate": 1758062804419,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission36/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission36/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "SebHZk78aS",
        "title": "GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation",
        "abstract": "We present GraspMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from a large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. \nGraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot.\nWe release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation.",
        "keywords": [
            "task-oriented grasping",
            "functional grasping",
            "robotic manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/93ba96afccb809818fbdeac6a9faabeef18c2d96.pdf",
        "reviews": [
            {
                "id": "ETvUzh8LZe",
                "forum": "SebHZk78aS",
                "replyto": "SebHZk78aS",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission34/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068257066,
                "mdate": 1754801276709,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "SebHZk78aS",
                "forum": "SebHZk78aS",
                "content": {
                    "title": {
                        "value": "GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation"
                    },
                    "authors": {
                        "value": [
                            "Abhay Deshpande",
                            "Yuquan Deng",
                            "Jordi Salvador",
                            "Arijit Ray",
                            "Winson Han",
                            "Jiafei Duan",
                            "Rose Hendrix",
                            "Yuke Zhu",
                            "Ranjay Krishna"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Abhay_Deshpande1",
                            "~Yuquan_Deng1",
                            "~Jordi_Salvador3",
                            "~Arijit_Ray1",
                            "~Winson_Han1",
                            "~Jiafei_Duan1",
                            "~Rose_Hendrix1",
                            "~Yuke_Zhu1",
                            "~Ranjay_Krishna1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "task-oriented grasping",
                            "functional grasping",
                            "robotic manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "We present GraspMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from a large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. \nGraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot.\nWe release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/93ba96afccb809818fbdeac6a9faabeef18c2d96.pdf"
                    },
                    "TLDR": {
                        "value": "We present GraspMolmo, an open-vocabulary task-oriented grasping model that predicts semantically appropriate grasps from natural language and RGB-D input, using large-scale synthetic data to achieve SOTA performance in real-world manipulation tasks."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ndeshpande2025graspmolmo,\ntitle={GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation},\nauthor={Abhay Deshpande and Yuquan Deng and Jordi Salvador and Arijit Ray and Winson Han and Jiafei Duan and Rose Hendrix and Yuke Zhu and Ranjay Krishna},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SebHZk78aS}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/0c3904f96c28adc7a774b9ecbee5de8c93c99cb6.zip"
                    },
                    "paperhash": {
                        "value": "deshpande|graspmolmo_generalizable_taskoriented_grasping_via_largescale_synthetic_data_generation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission34/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission34/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission34/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743735565628,
                "pdate": 1754680599381,
                "odate": 1758062680294,
                "mdate": 1758062804146,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission34/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission34/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "4Po2mqLjrQ",
        "title": "Motion Blender Gaussian Splatting for Dynamic Reconstruction",
        "abstract": "Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes.  However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application. To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel framework that uses motion graph as an explicit and sparse motion representation. The motion of graph links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions determining the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MB-GS achieves state-of-the-art performance on the iPhone dataset while being competitive on HyperNeRF. Additionally, we demonstrate the application potential of our method in animating novel object motions, synthesizing robot demonstrations through motion editing, and predicting robot actions through visual planning.",
        "keywords": [
            "Dynamic Reconstruction",
            "Gaussian Splatting"
        ],
        "pdf_url": "https://openreview.net/pdf/f382b3e14327fa8301c79bbf7647d757c14f37c0.pdf",
        "reviews": [
            {
                "id": "4jGUOEozKG",
                "forum": "4Po2mqLjrQ",
                "replyto": "4Po2mqLjrQ",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission32/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068256888,
                "mdate": 1754801276682,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "4Po2mqLjrQ",
                "forum": "4Po2mqLjrQ",
                "content": {
                    "title": {
                        "value": "Motion Blender Gaussian Splatting for Dynamic Reconstruction"
                    },
                    "authors": {
                        "value": [
                            "Xinyu Zhang",
                            "Haonan Chang",
                            "Yuhan Liu",
                            "Abdeslam Boularias"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Xinyu_Zhang7",
                            "~Haonan_Chang1",
                            "~Yuhan_Liu2",
                            "~Abdeslam_Boularias1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Dynamic Reconstruction",
                            "Gaussian Splatting"
                        ]
                    },
                    "abstract": {
                        "value": "Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes.  However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application. To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel framework that uses motion graph as an explicit and sparse motion representation. The motion of graph links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions determining the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MB-GS achieves state-of-the-art performance on the iPhone dataset while being competitive on HyperNeRF. Additionally, we demonstrate the application potential of our method in animating novel object motions, synthesizing robot demonstrations through motion editing, and predicting robot actions through visual planning."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "TLDR": {
                        "value": "A framework that uses graphs as an explicit and sparse motion representation for Gaussian splatting to reconstruct dynamic world, with applications in animation, robot data synthesis, and planning."
                    },
                    "supplementary_material": {
                        "value": "/attachment/8c3c0ded5408f78b5cb29d881200068b4bb38af5.zip"
                    },
                    "pdf": {
                        "value": "/pdf/f382b3e14327fa8301c79bbf7647d757c14f37c0.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nzhang2025motion,\ntitle={Motion Blender Gaussian Splatting for Dynamic Reconstruction},\nauthor={Xinyu Zhang and Haonan Chang and Yuhan Liu and Abdeslam Boularias},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4Po2mqLjrQ}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/255cccb56a81b9ee82fdd50e5ad8f34bc12a23dc.zip"
                    },
                    "paperhash": {
                        "value": "zhang|motion_blender_gaussian_splatting_for_dynamic_reconstruction"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission32/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission32/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission32/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743723677133,
                "pdate": 1754680599378,
                "odate": 1758062680291,
                "mdate": 1758062804221,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission32/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission32/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "bi8o9p6h2R",
        "title": "Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo",
        "abstract": "Sampling-based motion planning methods, while effective in high-dimensional spaces, often suffer from inefficiencies due to irregular sampling distributions, leading to suboptimal exploration of the configuration space. In this paper, we propose an approach that enhances the efficiency of these methods by utilizing low-discrepancy distributions generated through Message-Passing Monte Carlo (MPMC). MPMC leverages Graph Neural Networks (GNNs) to generate point sets that uniformly cover the space, with uniformity assessed using the the $\\mathcal{L}_p$-discrepancy measure, which quantifies the irregularity of sample distributions. By improving the uniformity of the point sets, our approach significantly reduces computational overhead and the number of samples required for solving motion planning problems. Experimental results demonstrate that our method outperforms traditional sampling techniques in terms of planning efficiency.",
        "keywords": [
            "Graph Neural Networks",
            "Discrepancy Theory",
            "Motion Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/a9adbbca2a44e253402110402a80577ea3c6204f.pdf",
        "reviews": [
            {
                "id": "vVvMlYR5TM",
                "forum": "bi8o9p6h2R",
                "replyto": "bi8o9p6h2R",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission31/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068256877,
                "mdate": 1754801276101,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "bi8o9p6h2R",
                "forum": "bi8o9p6h2R",
                "content": {
                    "title": {
                        "value": "Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo"
                    },
                    "authors": {
                        "value": [
                            "Makram Chahine",
                            "T. Konstantin Rusch",
                            "Zach J Patterson",
                            "Daniela Rus"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Makram_Chahine1",
                            "~T._Konstantin_Rusch1",
                            "~Zach_J_Patterson1",
                            "~Daniela_Rus1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Graph Neural Networks",
                            "Discrepancy Theory",
                            "Motion Planning"
                        ]
                    },
                    "TLDR": {
                        "value": "This paper introduces a new sampling method for robot motion planning, using graph neural networks to generate more uniform sample distributions, which leads to improved planning efficiency in complex environments."
                    },
                    "abstract": {
                        "value": "Sampling-based motion planning methods, while effective in high-dimensional spaces, often suffer from inefficiencies due to irregular sampling distributions, leading to suboptimal exploration of the configuration space. In this paper, we propose an approach that enhances the efficiency of these methods by utilizing low-discrepancy distributions generated through Message-Passing Monte Carlo (MPMC). MPMC leverages Graph Neural Networks (GNNs) to generate point sets that uniformly cover the space, with uniformity assessed using the the $\\mathcal{L}_p$-discrepancy measure, which quantifies the irregularity of sample distributions. By improving the uniformity of the point sets, our approach significantly reduces computational overhead and the number of samples required for solving motion planning problems. Experimental results demonstrate that our method outperforms traditional sampling techniques in terms of planning efficiency."
                    },
                    "pdf": {
                        "value": "/pdf/a9adbbca2a44e253402110402a80577ea3c6204f.pdf"
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nchahine2025improving,\ntitle={Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo},\nauthor={Makram Chahine and T. Konstantin Rusch and Zach J Patterson and Daniela Rus},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bi8o9p6h2R}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/5ab75f21fc1c3b6d32c96fb0e486209e84b344fa.zip"
                    },
                    "paperhash": {
                        "value": "chahine|improving_efficiency_of_samplingbased_motion_planning_via_messagepassing_monte_carlo"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission31/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743715751059,
                "pdate": 1754680599311,
                "odate": 1758062679944,
                "mdate": 1758062804076,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission31/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "7iaYcss56y",
        "title": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation",
        "abstract": "Learning robot manipulation from abundant human videos offers a scalable alternative to costly robot-specific data collection. However, domain gaps across visual, morphological, and physical aspects hinder direct imitation. To effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic co-training framework that leverages both human videos and a small amount of teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with either action- or visual-based mapping to map retargeted human hand poses to robot joints, followed by MixUp interpolation between paired human and robot trajectories. Our key insights are (1) retargeted human hand trajectories provide informative action labels, and (2) interpolation over the mapped data creates intermediate domains that facilitate smooth domain adaptation during co-training. Evaluations on four real-world manipulation tasks (Pick and Place, Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro, Ability) show that ImMimic improves task success rates and execution smoothness, highlighting its efficacy to bridge the domain gap for robust robot manipulation. The project website can be found at https://sites.google.com/view/immimic.",
        "keywords": [
            "Learning from Human",
            "Imitation learning",
            "Dexterous Manipulation"
        ],
        "pdf_url": "https://openreview.net/pdf/3378a04fd4ae2c707cc4f4baefadc0d70a7c6d6c.pdf",
        "reviews": [
            {
                "id": "cv7ns7jJIN",
                "forum": "7iaYcss56y",
                "replyto": "7iaYcss56y",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Oral)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission26/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068256860,
                "mdate": 1754801273088,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "7iaYcss56y",
                "forum": "7iaYcss56y",
                "content": {
                    "title": {
                        "value": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation"
                    },
                    "authors": {
                        "value": [
                            "Yangcen Liu",
                            "Woo Chul Shin",
                            "Yunhai Han",
                            "Zhenyang Chen",
                            "Harish Ravichandar",
                            "Danfei Xu"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Yangcen_Liu1",
                            "~Woo_Chul_Shin1",
                            "~Yunhai_Han1",
                            "~Zhenyang_Chen1",
                            "~Harish_Ravichandar1",
                            "~Danfei_Xu1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Learning from Human",
                            "Imitation learning",
                            "Dexterous Manipulation"
                        ]
                    },
                    "abstract": {
                        "value": "Learning robot manipulation from abundant human videos offers a scalable alternative to costly robot-specific data collection. However, domain gaps across visual, morphological, and physical aspects hinder direct imitation. To effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic co-training framework that leverages both human videos and a small amount of teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with either action- or visual-based mapping to map retargeted human hand poses to robot joints, followed by MixUp interpolation between paired human and robot trajectories. Our key insights are (1) retargeted human hand trajectories provide informative action labels, and (2) interpolation over the mapped data creates intermediate domains that facilitate smooth domain adaptation during co-training. Evaluations on four real-world manipulation tasks (Pick and Place, Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro, Ability) show that ImMimic improves task success rates and execution smoothness, highlighting its efficacy to bridge the domain gap for robust robot manipulation. The project website can be found at https://sites.google.com/view/immimic."
                    },
                    "venue": {
                        "value": "CoRL 2025 Oral"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "supplementary_material": {
                        "value": "/attachment/8bcc08b2c34e9f52ae3cebd63adfb2fa2d087c53.zip"
                    },
                    "pdf": {
                        "value": "/pdf/3378a04fd4ae2c707cc4f4baefadc0d70a7c6d6c.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\nliu2025immimic,\ntitle={ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation},\nauthor={Yangcen Liu and Woo Chul Shin and Yunhai Han and Zhenyang Chen and Harish Ravichandar and Danfei Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7iaYcss56y}\n}"
                    },
                    "paperhash": {
                        "value": "liu|immimic_crossdomain_imitation_from_human_videos_via_mapping_and_interpolation"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission26/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission26/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission26/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743692270843,
                "pdate": 1754680599247,
                "odate": 1758062676905,
                "mdate": 1758062803757,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission26/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission26/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "1otaE496Vm",
        "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
        "abstract": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. \nState-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail.\nRL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. \nWe show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. \nInfractions are penalized by terminating the episode or multiplicatively reducing route completion.\nWe find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance.\nTraining with large mini-batch sizes enables efficient scaling via distributed data parallelism. \nWe scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node.\nThe resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan.\nIt scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.",
        "keywords": [
            "Autonomous Driving",
            "Reinforcement Learning",
            "Planning"
        ],
        "pdf_url": "https://openreview.net/pdf/c40950d1ba06616e46709c0a8bcf9cc2e881ee4b.pdf",
        "reviews": [
            {
                "id": "JxnTOZD6OJ",
                "forum": "1otaE496Vm",
                "replyto": "1otaE496Vm",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission6/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068256012,
                "mdate": 1754801276066,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "1otaE496Vm",
                "forum": "1otaE496Vm",
                "content": {
                    "title": {
                        "value": "CaRL: Learning Scalable Planning Policies with Simple Rewards"
                    },
                    "authors": {
                        "value": [
                            "Bernhard Jaeger",
                            "Daniel Dauner",
                            "Jens Beißwenger",
                            "Simon Gerstenecker",
                            "Kashyap Chitta",
                            "Andreas Geiger"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Bernhard_Jaeger1",
                            "~Daniel_Dauner1",
                            "~Jens_Beißwenger1",
                            "simongerstenecker@gmail.com",
                            "~Kashyap_Chitta1",
                            "~Andreas_Geiger3"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Autonomous Driving",
                            "Reinforcement Learning",
                            "Planning"
                        ]
                    },
                    "abstract": {
                        "value": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. \nState-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail.\nRL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. \nWe show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. \nInfractions are penalized by terminating the episode or multiplicatively reducing route completion.\nWe find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance.\nTraining with large mini-batch sizes enables efficient scaling via distributed data parallelism. \nWe scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node.\nThe resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan.\nIt scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "pdf": {
                        "value": "/pdf/c40950d1ba06616e46709c0a8bcf9cc2e881ee4b.pdf"
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\njaeger2025carl,\ntitle={Ca{RL}: Learning Scalable Planning Policies with Simple Rewards},\nauthor={Bernhard Jaeger and Daniel Dauner and Jens Bei{\\ss}wenger and Simon Gerstenecker and Kashyap Chitta and Andreas Geiger},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1otaE496Vm}\n}"
                    },
                    "paperhash": {
                        "value": "jaeger|carl_learning_scalable_planning_policies_with_simple_rewards"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission6/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743576346680,
                "pdate": 1754680598625,
                "odate": 1758062667894,
                "mdate": 1758062803776,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission6/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission6/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    },
    {
        "venue": "CoRL",
        "year": "2025",
        "forum_id": "9uKL9FJBiz",
        "title": "Pseudo-Simulation for Autonomous Driving",
        "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation.",
        "keywords": [
            "Simulation",
            "Benchmarking",
            "Autonomous Driving"
        ],
        "pdf_url": "https://openreview.net/pdf/5f2a1c70c12ba9adf0d7db38aa3704bdd2672396.pdf",
        "reviews": [
            {
                "id": "LsVjjNOa5O",
                "forum": "9uKL9FJBiz",
                "replyto": "9uKL9FJBiz",
                "content": {
                    "title": {
                        "value": "Paper Decision"
                    },
                    "decision": {
                        "value": "Accept(Poster)"
                    },
                    "comment": {
                        "value": ""
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/Submission5/-/Decision"
                ],
                "parentInvitations": "robot-learning.org/CoRL/2025/Conference/-/Decision",
                "cdate": 1754068255823,
                "mdate": 1754801275935,
                "nonreaders": [],
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Program_Chairs"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            },
            {
                "id": "9uKL9FJBiz",
                "forum": "9uKL9FJBiz",
                "content": {
                    "title": {
                        "value": "Pseudo-Simulation for Autonomous Driving"
                    },
                    "authors": {
                        "value": [
                            "Wei Cao",
                            "Marcel Hallgarten",
                            "Tianyu Li",
                            "Daniel Dauner",
                            "Xunjiang Gu",
                            "Caojun Wang",
                            "Yakov Miron",
                            "Marco Aiello",
                            "Hongyang Li",
                            "Igor Gilitschenski",
                            "Boris Ivanovic",
                            "Marco Pavone",
                            "Andreas Geiger",
                            "Kashyap Chitta"
                        ]
                    },
                    "authorids": {
                        "value": [
                            "~Wei_Cao2",
                            "~Marcel_Hallgarten1",
                            "~Tianyu_Li5",
                            "~Daniel_Dauner1",
                            "~Xunjiang_Gu1",
                            "~Caojun_Wang1",
                            "~Yakov_Miron1",
                            "~Marco_Aiello2",
                            "~Hongyang_Li1",
                            "~Igor_Gilitschenski1",
                            "~Boris_Ivanovic1",
                            "~Marco_Pavone1",
                            "~Andreas_Geiger3",
                            "~Kashyap_Chitta1"
                        ]
                    },
                    "keywords": {
                        "value": [
                            "Simulation",
                            "Benchmarking",
                            "Autonomous Driving"
                        ]
                    },
                    "abstract": {
                        "value": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation."
                    },
                    "venue": {
                        "value": "CoRL 2025 Poster"
                    },
                    "venueid": {
                        "value": "robot-learning.org/CoRL/2025/Conference"
                    },
                    "supplementary_material": {
                        "value": "/attachment/8f62cec9556baca256fd283ec0c9b39a92cca704.pdf"
                    },
                    "pdf": {
                        "value": "/pdf/5f2a1c70c12ba9adf0d7db38aa3704bdd2672396.pdf"
                    },
                    "TLDR": {
                        "value": "We introduce Pseudo-Simulation, a novel AV evaluation methodology that augments real data with synthetic observations near the planned trajectory. It achieves strong correlation with closed-loop simulation while being much faster and easier to scale."
                    },
                    "_bibtex": {
                        "value": "@inproceedings{\ncao2025pseudosimulation,\ntitle={Pseudo-Simulation for Autonomous Driving},\nauthor={Wei Cao and Marcel Hallgarten and Tianyu Li and Daniel Dauner and Xunjiang Gu and Caojun Wang and Yakov Miron and Marco Aiello and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9uKL9FJBiz}\n}"
                    },
                    "spotlight": {
                        "value": "/attachment/04b5e39586072ff6a7371ce979e105eac5347709.zip"
                    },
                    "paperhash": {
                        "value": "cao|pseudosimulation_for_autonomous_driving"
                    }
                },
                "invitations": [
                    "robot-learning.org/CoRL/2025/Conference/-/Submission",
                    "robot-learning.org/CoRL/2025/Conference/-/Post_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission5/-/Full_Submission",
                    "robot-learning.org/CoRL/2025/Conference/Submission5/-/Supplementary_Material",
                    "robot-learning.org/CoRL/2025/Conference/-/Edit",
                    "robot-learning.org/CoRL/2025/Conference/Submission5/-/Spotlight_And_Camera_Ready"
                ],
                "cdate": 1743576277971,
                "pdate": 1754680598332,
                "odate": 1758062664776,
                "mdate": 1758062803778,
                "signatures": [
                    "robot-learning.org/CoRL/2025/Conference/Submission5/Authors"
                ],
                "writers": [
                    "robot-learning.org/CoRL/2025/Conference",
                    "robot-learning.org/CoRL/2025/Conference/Submission5/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "license": "CC BY 4.0"
            }
        ]
    }
]